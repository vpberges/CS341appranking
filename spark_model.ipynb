{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imported libraries\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import boto\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import langdetect\n",
    "import datetime\n",
    "%matplotlib inline  \n",
    "from sklearn.svm import SVR\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "import os\n",
    "import findspark; findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import Row\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors \n",
    "from pyspark.mllib.regression import LabeledPoint  \n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD \n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "  \"--packages com.databricks:spark-csv_2.11:1.4.0 pyspark-shell\"\n",
    ")\n",
    "\n",
    "#NEED TO ADD \"  SPARK_DRIVER_MEMORY=5G   \"  to ./conf/spark-env.sh \n",
    "\n",
    "try:\n",
    "    sc = pyspark.SparkContext()\n",
    "except Exception as e:\n",
    "    print \"SparkContext exists... Continuing on.\"\n",
    "    \n",
    "sqlCtx = pyspark.sql.SQLContext(sc)\n",
    "sc.setCheckpointDir('checkpoint/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", os.environ['AWS_ACCESS_KEY_ID'])\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", os.environ['AWS_SECRET_ACCESS_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "downloads = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true',inferSchema='true') \\\n",
    "    .load('s3n://cs341bucket1/Data/train_app_downloads.csv').drop('')\n",
    "ratings = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true',inferSchema='true') \\\n",
    "    .load('s3n://cs341bucket1/Data/train_app_rating.csv').drop('')\n",
    "usages = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true',inferSchema='true') \\\n",
    "    .load('s3n://cs341bucket1/Data/train_usage.csv').drop('')\n",
    "revenues = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true',inferSchema='true') \\\n",
    "    .load('s3n://cs341bucket1/Data/train_revenue.csv').drop('')\n",
    "output = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true',inferSchema='true') \\\n",
    "    .load('s3n://cs341bucket1/Data/train_final_downloads.csv').drop('')\n",
    "prev_downloads = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true',inferSchema='true') \\\n",
    "    .load('s3n://cs341bucket1/Data/train_cumulative_downloads_2015-02.csv').drop('')  \n",
    "release_date = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true',inferSchema='true') \\\n",
    "    .load('s3n://cs341bucket1/Data/train_release_date.csv').drop('')\n",
    "text_score = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='false',inferSchema='true') \\\n",
    "    .load('s3n://cs341bucket1/Data/sentiment.csv').drop('')\n",
    "title_score = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='false',inferSchema='true') \\\n",
    "    .load('s3n://cs341bucket1/Data/t_sentiment.csv').drop('')\n",
    "avg_score = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true',inferSchema='true') \\\n",
    "    .load('s3n://cs341bucket1/Data/avg_sent_score.csv').drop('')\n",
    "\n",
    "reviews_schema = StructType([\n",
    "    StructField(\"id\",IntegerType(),True),\n",
    "    StructField(\"name\",StringType(),True),\n",
    "    StructField(\"country\",StringType(),True),\n",
    "    StructField(\"rating\",IntegerType(),True),\n",
    "    StructField(\"date\",StringType(),True),\n",
    "    StructField(\"title\",StringType(),True),\n",
    "    StructField(\"version\",StringType(),True),\n",
    "    StructField(\"text\",StringType(),True),\n",
    "    StructField(\"reviewer\",StringType(),True)\n",
    "])\n",
    "reviews = pd.read_csv('s3://cs341bucket1/Data/train_app_review.csv')\n",
    "reviews = sqlCtx.createDataFrame(reviews,reviews_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old_dateRange = pd.date_range('03/01/2015', periods=56).format(formatter=lambda x: x.strftime('%Y-%m-%d'))\n",
    "dateRange = pd.date_range('03/01/2015', periods=56).format(formatter=lambda x: x.strftime('%m_%d_%Y'))\n",
    "for d in range(56):\n",
    "    revenues = revenues.withColumnRenamed(old_dateRange[d],dateRange[d])\n",
    "    usages = usages.withColumnRenamed(old_dateRange[d],dateRange[d])\n",
    "    downloads = downloads.withColumnRenamed(old_dateRange[d],dateRange[d])  \n",
    "output = output.withColumnRenamed(\"cumulative_downloads_2016-02\",\"cumulative_downloads_2016_02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initialization\n",
    "predictors = downloads['id','name','category','device']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate the weekly downloads\n",
    "\n",
    "sqlCtx.registerDataFrameAsTable(downloads, \"downloads\")\n",
    "sqlCtx.registerDataFrameAsTable(usages, \"usages\")\n",
    "sqlCtx.registerDataFrameAsTable(revenues, \"revenues\")\n",
    "\n",
    "def get_log_week(*args):\n",
    "    args = [x if not (x==-1) else 0 for x in list(args)]\n",
    "    nb_0 = args.count(0)\n",
    "    if nb_0 == 7:\n",
    "        return 0\n",
    "    return math.log(sum(args)/(7-nb_0))\n",
    "sqlCtx.registerFunction(\"get_log_week\", get_log_week,returnType=FloatType())\n",
    "\n",
    "predictors = sqlCtx.sql(\"SELECT id, name, category, device, \\\n",
    "           get_log_week(\"+ \",\".join(dateRange[0:7])+\") AS week_1 \\\n",
    "            ,get_log_week(\"+\",\".join(dateRange[7:14])+\") AS week_2 \\\n",
    "            ,get_log_week(\"+ \",\".join(dateRange[14:21])+\") AS week_3 \\\n",
    "           ,get_log_week(\"+\",\".join(dateRange[21:28])+\") AS week_4 \\\n",
    "           ,get_log_week(\"+\",\".join(dateRange[28:35])+\") AS week_5 \\\n",
    "           ,get_log_week(\"+\",\".join(dateRange[35:42])+\") AS week_6 \\\n",
    "           ,get_log_week(\"+\",\".join(dateRange[42:49])+\") AS week_7 \\\n",
    "           ,get_log_week(\"+\",\".join(dateRange[49:56])+\") AS week_8\\\n",
    "           ,\"+ \"+\".join(dateRange)+\" AS download_sum \\\n",
    "           from downloads\")\n",
    "sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "\n",
    "#I workaround the error by this modification. \n",
    "#I don't know why I couldn't run the code before but this workaround gives the same result.\n",
    "#predictors = sqlCtx.sql(\"SELECT \"+', '.join(predictors.columns)+\", week_1+week_2+week_3+week_4+week_5+week_6+week_7+week_8 AS download_sum FROM predictors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m1 = sqlCtx.sql(\"SELECT * FROM usages WHERE metric = 1\")\n",
    "m2 = sqlCtx.sql(\"SELECT * FROM usages WHERE metric = 2\")\n",
    "m3 = sqlCtx.sql(\"SELECT * FROM usages WHERE metric = 3\")\n",
    "m4 = sqlCtx.sql(\"SELECT * FROM usages WHERE metric = 4\")\n",
    "sqlCtx.registerDataFrameAsTable(m1,\"m1\")\n",
    "sqlCtx.registerDataFrameAsTable(m2,\"m2\")\n",
    "sqlCtx.registerDataFrameAsTable(m3,\"m3\")\n",
    "sqlCtx.registerDataFrameAsTable(m4,\"m4\")\n",
    "sqlCtx.registerDataFrameAsTable(avg_score,\"avg_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make coefficients\n",
    "\n",
    "def get_coefficients(*args):\n",
    "    #The first element of the list is the degree of the coefficient\n",
    "    args = list(args)\n",
    "    return  float(np.polyfit(range(56),np.cumsum(args[1:]),args[0])[0])\n",
    "    \n",
    "#Generate the step max and min \n",
    "def get_maxStep(maximum,*args):\n",
    "    args=list(args)\n",
    "    if (np.count_nonzero(args) == 0):\n",
    "        return 0\n",
    "    m = 0\n",
    "    for d in range(1,56):\n",
    "        if (args[d]!=0 and args[d-1]!=0):\n",
    "            c = (args[d]-args[d-1])\n",
    "            if (maximum and m < c):\n",
    "                m = c\n",
    "            if ( not maximum and m > c):\n",
    "                m = c\n",
    "    return m\n",
    "\n",
    "def get_std(*args):\n",
    "    return float(np.std(list(args)))\n",
    "\n",
    "def get_nbMissing(*args):\n",
    "    return list(args).count(-1)\n",
    "replacementValue = 0\n",
    "#Generate the daily average\n",
    "def get_dailyAvg(*inp):\n",
    "    if (np.count_nonzero(inp - replacementValue*np.ones(len(inp))) == 0):\n",
    "        return 0\n",
    "    return  (1.0*sum(inp)/np.count_nonzero(inp - replacementValue*np.ones(len(inp))))\n",
    "\n",
    "def get_usage_coefficients(*args):\n",
    "    #The first element of the list is the degree of the coefficient\n",
    "    args = list(args)\n",
    "    if -1 in args: return 0\n",
    "    return  float(np.polyfit(range(8),args[1:],args[0])[0])\n",
    "\n",
    "def get_revenue_coefficients(*args):\n",
    "    #The first element of the list is the degree of the coefficient\n",
    "    args = list(args)\n",
    "    if -1 in args: return 0\n",
    "    return  float(np.polyfit(range(56),args[1:],args[0])[0])\n",
    "\n",
    "def get_usage_max(*args):\n",
    "    #The first element of the list is the degree of the coefficient\n",
    "    args = list(args)\n",
    "    args = [c for c in args[1:] if c!=-1]\n",
    "    if len(args)==0: return 0\n",
    "    return  float(np.max(args))\n",
    "\n",
    "def get_usage_mean(*args):\n",
    "    #The first element of the list is the degree of the coefficient\n",
    "    args = list(args)\n",
    "    args = [c for c in args[1:] if c!=-1]\n",
    "    if len(args)==0: return 0\n",
    "    return  float(np.mean(args))\n",
    "\n",
    "def get_revenue_max(*args):\n",
    "    #The first element of the list is the degree of the coefficient\n",
    "    args = list(args)\n",
    "    args = [c for c in args[1:] if c!=-1]\n",
    "    if len(args)==0: return 0\n",
    "    return  float(np.max(args))\n",
    "\n",
    "def get_revenue_mean(*args):\n",
    "    #The first element of the list is the degree of the coefficient\n",
    "    args = list(args)\n",
    "    args = [c for c in args[1:] if c!=-1]\n",
    "    if len(args)==0: return 0\n",
    "    return  float(np.mean(args))\n",
    "\n",
    "sqlCtx.registerFunction(\"get_nbMissing\", get_nbMissing,returnType=IntegerType())\n",
    "sqlCtx.registerFunction(\"get_std\", get_std,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_maxStep\", get_maxStep,returnType=IntegerType())\n",
    "sqlCtx.registerFunction(\"get_coefficients\", get_coefficients,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"daily_avg\", get_dailyAvg,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_usage_coefficients\", get_usage_coefficients,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_revenue_coefficients\", get_revenue_coefficients,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_usage_max\", get_usage_max,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_usage_mean\", get_usage_mean,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_revenue_max\", get_revenue_max,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_revenue_mean\", get_revenue_mean,returnType=FloatType())\n",
    "\n",
    "temp_downloads = sqlCtx.sql(\"SELECT id,name,category, device \\\n",
    ", get_coefficients(0,\"+\",\".join(dateRange)+\") AS coef_0 \\\n",
    ",get_coefficients(1,\"+\",\".join(dateRange)+\") AS coef_1 \\\n",
    ",get_coefficients(2,\"+\",\".join(dateRange)+\") AS coef_2 \\\n",
    ",get_coefficients(3,\"+\",\".join(dateRange)+\") AS coef_3 \\\n",
    ",get_maxStep(True,\"+\",\".join(dateRange)+\") AS max_step \\\n",
    ",get_maxStep(False,\"+\",\".join(dateRange)+\") AS min_step \\\n",
    ",get_std(\"+\",\".join(dateRange)+\") AS downloads_std \\\n",
    ",get_nbMissing(\"+\",\".join(dateRange)+\") AS nb_missing \\\n",
    ",daily_avg(\" + \",\".join(dateRange[0:56]) + \") AS daily_avg \\\n",
    " FROM downloads\")\n",
    "\n",
    "temp_m1 = sqlCtx.sql(\"SELECT id, name, category, device, \\\n",
    "get_usage_coefficients(0,\"+\",\".join(m1.columns[5:13])+\") AS m1_coef_0, \\\n",
    "get_usage_coefficients(1,\"+\",\".join(m1.columns[5:13])+\") AS m1_coef_1, \\\n",
    "get_usage_coefficients(2,\"+\",\".join(m1.columns[5:13])+\") AS m1_coef_2, \\\n",
    "get_usage_max(0,\"+\",\".join(m1.columns[5:13])+\") AS m1_max, \\\n",
    "get_usage_mean(0,\"+\",\".join(m1.columns[5:13])+\") AS m1_mean FROM m1\")\n",
    "\n",
    "temp_m2 = sqlCtx.sql(\"SELECT id, name, category, device, \\\n",
    "get_usage_coefficients(0,\"+\",\".join(m2.columns[5:13])+\") AS m2_coef_0, \\\n",
    "get_usage_coefficients(1,\"+\",\".join(m2.columns[5:13])+\") AS m2_coef_1, \\\n",
    "get_usage_coefficients(2,\"+\",\".join(m2.columns[5:13])+\") AS m2_coef_2, \\\n",
    "get_usage_max(0,\"+\",\".join(m2.columns[5:13])+\") AS m2_max, \\\n",
    "get_usage_mean(0,\"+\",\".join(m2.columns[5:13])+\") AS m2_mean FROM m2\")\n",
    "\n",
    "temp_m3 = sqlCtx.sql(\"SELECT id, name, category, device, \\\n",
    "get_usage_coefficients(0,\"+\",\".join(m3.columns[5:13])+\") AS m3_coef_0, \\\n",
    "get_usage_coefficients(1,\"+\",\".join(m3.columns[5:13])+\") AS m3_coef_1, \\\n",
    "get_usage_coefficients(2,\"+\",\".join(m3.columns[5:13])+\") AS m3_coef_2, \\\n",
    "get_usage_max(0,\"+\",\".join(m3.columns[5:13])+\") AS m3_max, \\\n",
    "get_usage_mean(0,\"+\",\".join(m3.columns[5:13])+\") AS m3_mean FROM m3\")\n",
    "\n",
    "temp_m4 = sqlCtx.sql(\"SELECT id, name, category, device, \\\n",
    "get_usage_coefficients(0,\"+\",\".join(m4.columns[5:13])+\") AS m4_coef_0, \\\n",
    "get_usage_coefficients(1,\"+\",\".join(m4.columns[5:13])+\") AS m4_coef_1, \\\n",
    "get_usage_coefficients(2,\"+\",\".join(m4.columns[5:13])+\") AS m4_coef_2, \\\n",
    "get_usage_max(0,\"+\",\".join(m4.columns[5:13])+\") AS m4_max, \\\n",
    "get_usage_mean(0,\"+\",\".join(m4.columns[5:13])+\") AS m4_mean FROM m4\")\n",
    "\n",
    "temp_revenues = sqlCtx.sql(\"SELECT id, name, category, device, \\\n",
    "get_revenue_coefficients(0,\"+\",\".join(revenues.columns[4:])+\") AS rev_coef_0, \\\n",
    "get_revenue_coefficients(1,\"+\",\".join(revenues.columns[4:])+\") AS rev_coef_1, \\\n",
    "get_revenue_coefficients(2,\"+\",\".join(revenues.columns[4:])+\") AS rev_coef_2, \\\n",
    "get_revenue_max(0,\"+\",\".join(revenues.columns[4:])+\") AS rev_max, \\\n",
    "get_revenue_mean(0,\"+\",\".join(revenues.columns[4:])+\") AS rev_mean FROM revenues\")\n",
    "\n",
    "predictors = predictors.join(temp_downloads,[\"id\",\"name\",\"category\",\"device\"],how='left_outer')\n",
    "predictors = predictors.join(temp_revenues,[\"id\", \"name\", \"category\",\"device\"],how='left_outer')\n",
    "predictors = predictors.join(temp_m1,[\"id\", \"name\", \"category\",\"device\"],how='left_outer')\n",
    "predictors = predictors.join(temp_m2,[\"id\", \"name\", \"category\",\"device\"],how='left_outer')\n",
    "predictors = predictors.join(temp_m3,[\"id\", \"name\", \"category\",\"device\"],how='left_outer')\n",
    "predictors = predictors.join(temp_m4,[\"id\", \"name\", \"category\",\"device\"],how='left_outer')\n",
    "predictors = predictors.join(avg_score,[\"id\", \"name\", \"category\",\"device\"],how='left_outer')\n",
    "#predictors = predictors.join(temp_usages,[\"id\", \"name\", \"category\",\"device\"],how='left_outer')\n",
    "#predictors = predictors.join(,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# previous downloads addition\n",
    "predictors = predictors.join(prev_downloads,[\"id\",\"device\"],how='left_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Days since release generation\n",
    "def get_days(date):\n",
    "    return (datetime.datetime.strptime('03/01/2015', '%m/%d/%Y').date() \\\n",
    "            - datetime.datetime.strptime(date, '%Y-%m-%d').date()).days\n",
    "\n",
    "sqlCtx.registerDataFrameAsTable(release_date, \"release_date\")\n",
    "sqlCtx.registerFunction(\"get_days\", get_days,returnType=IntegerType())\n",
    "temp_date = sqlCtx.sql(\"SELECT id,name \\\n",
    ", get_days(release_date) AS days_since_release \\\n",
    " FROM release_date\")\n",
    "\n",
    "predictors = predictors.join(temp_date,[\"id\",\"name\"],\"left\").fillna(0, ['days_since_release'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ratings generation\n",
    "sqlCtx.registerDataFrameAsTable(ratings, \"ratings\")\n",
    "temp_ratings = sqlCtx.sql(\"SELECT id,name,category \\\n",
    ", 1.0*start1/(start1+star2+star3+star4+star5) AS star1 \\\n",
    ", 1.0*star2/(start1+star2+star3+star4+star5) AS star2 \\\n",
    ", 1.0*star3/(start1+star2+star3+star4+star5) AS star3 \\\n",
    ", 1.0*star4/(start1+star2+star3+star4+star5) AS star4 \\\n",
    ", 1.0*star5/(start1+star2+star3+star4+star5) AS star5 \\\n",
    ", (start1+star2+star3+star4+star5) AS num_ratings \\\n",
    " FROM ratings\")\n",
    "\n",
    "predictors = predictors.join(temp_ratings,[\"id\",\"name\",\"category\"],\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Categories\n",
    "list_categories = [ x.category.replace(\" \",\"_\") for x in sqlCtx.sql(\"SELECT category \\\n",
    " FROM downloads\\\n",
    " group by category \\\n",
    " \").collect()]\n",
    "for cat in list_categories:\n",
    "    sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "    predictors=sqlCtx.sql('''SELECT *, CASE WHEN (category = \"'''+cat+'''\") THEN 1 ELSE 0 END AS '''+cat+''' FROM predictors''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Device\n",
    "sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "predictors=sqlCtx.sql('''SELECT *, CASE WHEN (device = \"iphone\") THEN 1 ELSE 0 END AS iphone FROM predictors''')\n",
    "sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "predictors=sqlCtx.sql('''SELECT *, CASE WHEN (device = \"ipad\") THEN 1 ELSE 0 END AS ipad FROM predictors''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lang = ['ja','zh-cn','ko','en','other']\n",
    "def get_language(x):\n",
    "    try:\n",
    "        detected = langdetect.detect_langs(x.decode('utf8','ignore'))[0]\n",
    "        if detected.prob < 0.7:\n",
    "            return \"other\"\n",
    "        elif  detected.lang in lang:\n",
    "            return detected.lang\n",
    "        else:\n",
    "            return \"other\"\n",
    "    except:\n",
    "        return \"other\"\n",
    "sqlCtx.registerFunction(\"get_language\", get_language,returnType=StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Language of the title\n",
    "for l in lang:\n",
    "    sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "    predictors=sqlCtx.sql('''SELECT *, CASE WHEN (get_language(name) = \"'''+l+'''\") THEN 1 \\\n",
    "    ELSE 0 END AS '''+l.replace(\"-\",\"_\")+''' FROM predictors''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reviews \n",
    "#escape is used in case some asshole used - or [space] anywhere\n",
    "def escape(text):\n",
    "    return text.replace(\" \",\"_\").replace(\"-\",\"_\")\n",
    "# number of reviews\n",
    "def get_recentReviews(date):\n",
    "    return int((datetime.datetime.strptime('03/01/2015', '%m/%d/%Y').date() \\\n",
    "            - datetime.datetime.strptime(date, '%Y-%m-%d').date()).days >=0)\n",
    "\n",
    "#First step\n",
    "list_countries =['United_States', 'France', 'Japan', 'Spain', 'United_Kingdom','Saudi_Arabia', 'Germany'\\\n",
    "     , 'Hong_Kong', 'Switzerland', 'Turkey','Netherlands', 'Australia', 'Norway', 'Sweden', 'China', 'Canada'\\\n",
    "     ,'Tanzania', 'Denmark', 'South_Korea', 'Italy', 'Finland', 'Taiwan','Russia', 'Philippines', 'Slovenia'\\\n",
    "     , 'Ireland', 'Belgium', 'Mexico','Austria', 'India', 'Brazil', 'Benin', 'New_Zealand','United_Arab_Emirates'\\\n",
    "     , 'Ukraine', 'Poland', 'Israel', 'Portugal','Tunisia', 'Mali', 'Slovakia', 'Zimbabwe', 'Thailand', 'Panama'\\\n",
    "     ,'Indonesia', 'Singapore', 'Greece', 'Senegal', 'Nicaragua','Hungary', 'Czech_Republic', 'Macedonia', 'Chile'\\\n",
    "     , 'Uruguay','Malaysia', 'Algeria', 'Nepal', 'Mauritania', 'Croatia']\n",
    "\n",
    "cmd = '''review_rdd = reviews\\\n",
    ".map(lambda x : (x.id , Row(id = x.id , avg_review = x.rating \\\n",
    ", recent_review = get_recentReviews(x.date), nb_review = 1\\\n",
    ",version = set([x.version])'''\n",
    "#cmd+=\",country = set([x.country])\"\n",
    "for c in list_countries:\n",
    "    cmd+=\",\"+c+''' = int( escape(x.country) == \"'''+c+'''\")'''\n",
    "cmd+=\")))\"\n",
    "exec cmd\n",
    "\n",
    "#Group step\n",
    "cmd = '''review_rdd = review_rdd.reduceByKey(lambda x1 ,x2 : Row(\\\n",
    " avg_review = x1.avg_review + x2.avg_review\\\n",
    "   ,recent_review = x1.recent_review + x2.recent_review, nb_review = x1.nb_review + x2.nb_review'''\n",
    "for c in list_countries:\n",
    "    cmd+=\" , \"+c+\" = x1.\"+c+\" + x2.\"+c\n",
    "#cmd+=\", country = x1.country.union(x2.country)\"\n",
    "cmd+=\", version = x1.version.union(x2.version)))\"\n",
    "exec cmd\n",
    "\n",
    "# Clean the grouped rdd\n",
    "cmd = '''review_rdd = review_rdd.map(lambda (id , x) : [ id \\\n",
    ",  1.0*x.avg_review /  x.nb_review\\\n",
    "   , x.recent_review,  x.nb_review'''\n",
    "for c in list_countries:\n",
    "    cmd+=\" , 1.0* x.\"+c+\"/ x.nb_review\"\n",
    "#cmd+=\",  x.country.pop()\"\n",
    "cmd+=\",  len(x.version) - 1])\"\n",
    "exec cmd\n",
    "\n",
    "#Put back into dataframe\n",
    "grp_reviews = sqlCtx.createDataFrame(review_rdd, [\"id\",\"avg_review\"\\\n",
    "                      ,\"recent_reviews\",\"nb_reviews\"] + list_countries + [\"versions\"])\n",
    "\n",
    "#Join with predictors \n",
    "predictors = predictors.join(grp_reviews,[\"id\"],\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate DL Projection\n",
    "sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "derived_feats = sqlCtx.sql(\"SELECT id, device\\\n",
    "    ,CAST((download_sum+7*`cumulative_downloads_2015-02`) AS float ) AS dl_projection \\\n",
    "    ,CAST((1.0*num_ratings/`cumulative_downloads_2015-02`)AS float )  AS ratings_per_download \\\n",
    "    ,CAST((1.0*num_ratings/(days_since_release+60))AS float )  AS ratings_per_day \\\n",
    "    ,CAST((1.0*`cumulative_downloads_2015-02`/(days_since_release+1))AS float )  AS downloads_per_day_before \\\n",
    "    ,CAST((1.0*versions/(days_since_release+60))AS float )  AS versions_per_day \\\n",
    "    FROM predictors\")\n",
    "predictors = predictors.join(derived_feats,[\"id\",\"device\"],\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "predictors = predictors.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors.toPandas().to_csv(\"predictors_spark.csv\", sep='\\t',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### ML  pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictor_names = predictors.rdd.top(1)[0].asDict().keys()\n",
    "predictor_names.remove(\"name\")\n",
    "predictor_names.remove(\"id\")\n",
    "predictor_names.remove(\"category\")\n",
    "predictor_names.remove(\"device\")\n",
    "\n",
    "predictors_labelPoints = predictors.rdd\\\n",
    ".map(lambda x: ((x.id,x.device) ,x))\\\n",
    ".join(output.rdd.map(lambda x: ((x.id,x.device) ,x.cumulative_downloads_2016_02)))\\\n",
    ".map(lambda x : (x[0] , LabeledPoint(x[1][1], [x[1][0].asDict()[col] for col in predictor_names])))\n",
    "\n",
    "predictors_labelPoints.cache()\n",
    "predictors_labelPoints.top(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_preformance_old(labelsAndPredictions):\n",
    "    total = labelsAndPredictions.count()\n",
    "    pred_threshold = labelsAndPredictions.takeOrdered(int(0.01*total),lambda x:-x.pred)[-1].pred\n",
    "    true_threshold = labelsAndPredictions.takeOrdered(int(0.01*total),lambda x:-x.true)[-1].true\n",
    "    pred_top = labelsAndPredictions.filter(lambda x: x.pred >= pred_threshold ).map(lambda x : ((x.id,x.device),1))\n",
    "    true_top = labelsAndPredictions.filter(lambda x: x.true >= true_threshold ).map(lambda x : ((x.id,x.device),1)) \n",
    "    \n",
    "    if pred_top.count() > total*0.01 : return -1\n",
    "    \n",
    "    return 10000.0*pred_top.join(true_top).count()/total\n",
    "\n",
    "\n",
    "def get_preformance(labelsAndPredictions):\n",
    "    total = labelsAndPredictions.count()\n",
    "    pred_top = labelsAndPredictions.takeOrdered(int(0.01*total),lambda x:-x.pred).map(lambda x : ((x.id,x.device),1))\n",
    "    true_top = labelsAndPredictions.takeOrdered(int(0.01*total),lambda x:-x.true).map(lambda x : ((x.id,x.device),1))\n",
    "    \n",
    "    if pred_top.count() > total*0.01 : return -1\n",
    "    \n",
    "    return 10000.0*pred_top.join(true_top).count()/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 5\n",
    "predictors_cvSplit = predictors_labelPoints.randomSplit([1.0/K] * K)\n",
    "\n",
    "for fold in range(K):\n",
    "    test_rdd = predictors_cvSplit[fold]\n",
    "    train_rdd = sc.union([predictors_cvSplit[i]  for i in range(K) if (i!= fold)])\n",
    "    \n",
    "    model = GradientBoostedTrees.trainRegressor(train_rdd.map(lambda x:x[1])\\\n",
    "                                ,categoricalFeaturesInfo={}\\\n",
    "                                                , numIterations=500\\\n",
    "                                               ,maxDepth=1\\\n",
    "                                               ,learningRate = 0.01)\n",
    "    \n",
    "    prediction = model.predict(test_rdd.map(lambda x:x[1].features))\n",
    "    labelsAndPredictions = test_rdd.map(lambda x: x[1].label)\\\n",
    "    .zip(prediction)\\\n",
    "    .zip(test_rdd.map(lambda x : x[0]))\\\n",
    "    .map(lambda (tuple_true_pred , tuple_id) :\\\n",
    "         Row(id = tuple_id[0], device = tuple_id[1] , pred = tuple_true_pred[1],true = tuple_true_pred[0]))\n",
    "    print get_preformance(labelsAndPredictions)\n",
    "    print get_preformance_old(labelsAndPredictions)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictor_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "   \n",
    "    \n",
    "\n",
    "print list(predictors.columns.values)\n",
    "np.random.seed(1)\n",
    "kf = KFold(len(predictors), n_folds=K)\n",
    "new_top = []\n",
    "for train, test in kf:\n",
    "\n",
    "    old_mod=linear_model.LinearRegression(fit_intercept=False).fit(predictors.as_matrix()[train,4:12], output.as_matrix()[train,5])\n",
    "    y_pred =  old_mod.predict(predictors.as_matrix()[test,4:12])\n",
    "\n",
    "\n",
    "    #model\n",
    "    #mod=linear_model.LinearRegression(fit_intercept=False)\\ \n",
    "    #mod=linear_model.Lasso(alpha=100,fit_intercept=False)\\\n",
    "    #mod = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0, loss='ls')\\ \n",
    "    #mod= RandomForestRegressor(max_features = 1.0/3.0,n_estimators = 100)\\\n",
    "    #mod= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100)\\\n",
    "    \n",
    "    #mod = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01,max_depth=2, random_state=0, loss='ls')\\\n",
    "    #.fit(predictors.as_matrix()[train,4:], output.as_matrix()[train,5]) \n",
    "\n",
    "    #y_pred =  mod.predict(predictors.as_matrix()[test,4:])\n",
    "    new_top.append(metric(y_pred,output.as_matrix()[test,5]))\n",
    "\n",
    "print 1.0*sum(new_top)/len(new_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictor_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LabeledPoint(1,[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use something like this in the classification step\n",
    "def even(x): return x % 2 == 0\n",
    "def odd(x): return not even(x)\n",
    "rdd = sc.parallelize(range(20))\n",
    "\n",
    "rdd_odd, rdd_even = (rdd.filter(f) for f in (odd, even))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "predictors_df = predictors\n",
    "predictors_pd = pd.read_csv('predictors_spark.csv',sep='\\t').drop('Unnamed: 0', 1).fillna(0)\n",
    "\n",
    "\n",
    "numerical_predictors = list(predictors_pd.columns.values)\n",
    "numerical_predictors.remove(\"id\");numerical_predictors.remove(\"device\");\n",
    "numerical_predictors.remove(\"category\");numerical_predictors.remove(\"name\");\n",
    "\n",
    "predictors_id = predictors[[\"id\",\"device\"]].as_matrix()\n",
    "\n",
    "predictors_matrix = predictors_pd[numerical_predictors].as_matrix()\n",
    "\n",
    "output_pd = pd.merge(predictors,output.toPandas(),on = [\"id\",\"device\"],how=\"left\")\\\n",
    "[[\"id\",\"device\", \"cumulative_downloads_2016_02\"]]\n",
    "output_matrix = output_pd[\"cumulative_downloads_2016_02\"].as_matrix()\n",
    "\n",
    "predictors_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is the metric we use to determine our performance\n",
    "def metric(y_pred,y_test,percent=1):\n",
    "    top = int(len(y_pred)/100.0*percent)\n",
    "    return (len(set([i[0] for i in sorted(enumerate(y_pred), key=lambda x:x[1],reverse=True)][0:top])\n",
    "       .intersection([i[0] for i in sorted(enumerate(y_test), key=lambda x:x[1],reverse=True)][0:top])\n",
    "               ))/(percent/100.0)/len(y_pred)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "K = 5\n",
    "print list(predictors.columns.values)\n",
    "np.random.seed(1)\n",
    "kf = KFold(len(predictors), n_folds=K)\n",
    "new_top = []\n",
    "for train, test in kf:\n",
    "\n",
    "    #old_mod=linear_model.LinearRegression(fit_intercept=False).fit(predictors.as_matrix()[train,4:12].astype(float), output['cumulative_downloads_2016-02'].as_matrix()[train])\n",
    "    #y_pred =  old_mod.predict(predictors.as_matrix()[test,4:12])\n",
    "\n",
    "\n",
    "    #model\n",
    "    #mod=linear_model.LinearRegression(fit_intercept=False)\\ \n",
    "    #mod=linear_model.Lasso(alpha=100,fit_intercept=False)\\\n",
    "    #mod = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0, loss='ls')\\ \n",
    "    #mod= RandomForestRegressor(max_features = 1.0/3.0,n_estimators = 100)\\\n",
    "    #mod= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100)\\\n",
    "    \n",
    "    mod = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01,max_depth=2, random_state=0, loss='ls')\\\n",
    "    .fit(predictors_matrix[train].astype(float), output_matrix[train]) \n",
    "\n",
    "    y_pred =  mod.predict(predictors_matrix[test].astype(float))\n",
    "    err = (metric(y_pred,output_matrix[test]))\n",
    "    new_top.append(err)\n",
    "    print err\n",
    "\n",
    "print 1.0*sum(new_top)/len(new_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old_predictors = pd.read_csv('predictors.csv').drop('Unnamed: 0', 1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#old_list = list(old_predictors.columns.values)\n",
    "(predictors.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors_pd.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(predictors.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
