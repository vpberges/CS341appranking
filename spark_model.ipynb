{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imported libraries\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import langdetect\n",
    "import datetime\n",
    "%matplotlib inline  \n",
    "from sklearn.svm import SVR\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "import findspark; findspark.init()\n",
    "import pyspark\n",
    "import os\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "  \"--packages com.databricks:spark-csv_2.11:1.4.0 pyspark-shell\"\n",
    ")\n",
    "\n",
    "#try:\n",
    "sc = pyspark.SparkContext()\n",
    "#except Exception as e:\n",
    "#    print \"SparkContext exists... Continuing on.\"\n",
    "    \n",
    "sqlCtx = pyspark.sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "downloads = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true',inferSchema='true') \\\n",
    "    .load('train_app_downloads.csv').drop('')\n",
    "ratings = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true',inferSchema='true') \\\n",
    "    .load('train_app_rating.csv').drop('')\n",
    "usages = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true',inferSchema='true') \\\n",
    "    .load('train_usage.csv').drop('')\n",
    "revenues = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true',inferSchema='true') \\\n",
    "    .load('train_revenue.csv').drop('')\n",
    "output = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true',inferSchema='true') \\\n",
    "    .load('train_final_downloads.csv').drop('')\n",
    "prev_downloads = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true',inferSchema='true') \\\n",
    "    .load('train_cumulative_downloads_2015-02.csv').drop('')  \n",
    "release_date = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true',inferSchema='true') \\\n",
    "    .load('train_release_date.csv').drop('')\n",
    "text_score = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='false',inferSchema='true') \\\n",
    "    .load('sentiment.csv').drop('')\n",
    "title_score = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='false',inferSchema='true') \\\n",
    "    .load('t_sentiment.csv').drop('')\n",
    "avg_score = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true',inferSchema='true') \\\n",
    "    .load('avg_sent_score.csv').drop('')\n",
    "    \n",
    "    \n",
    "reviews = pd.read_csv('train_app_review.csv')\n",
    "reviews_schema = StructType([\n",
    "    StructField(\"id\",IntegerType(),True),\n",
    "    StructField(\"name\",StringType(),True),\n",
    "    StructField(\"country\",StringType(),True),\n",
    "    StructField(\"rating\",IntegerType(),True),\n",
    "    StructField(\"date\",StringType(),True),\n",
    "    StructField(\"title\",StringType(),True),\n",
    "    StructField(\"version\",StringType(),True),\n",
    "    StructField(\"text\",StringType(),True),\n",
    "    StructField(\"reviewer\",StringType(),True)\n",
    "    \n",
    "])\n",
    "reviews = sqlCtx.createDataFrame(reviews,reviews_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(avg_sent_score=0.327267919153, id=281704574, device=u'iphone', category=u'Social Networking', name=u'AIM: Chat, Free Text, Photo Share, Voice Message')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_score.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old_dateRange = pd.date_range('2015-03-01', periods=56).format(formatter=lambda x: x.strftime('%Y-%m-%d'))\n",
    "dateRange = pd.date_range('2015-03-01', periods=56).format(formatter=lambda x: x.strftime('%Y_%m_%d'))\n",
    "for d in range(56):\n",
    "    revenues = revenues.withColumnRenamed(old_dateRange[d],dateRange[d])\n",
    "    usages = usages.withColumnRenamed(old_dateRange[d],dateRange[d])\n",
    "    downloads = downloads.withColumnRenamed(old_dateRange[d],dateRange[d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initialization\n",
    "predictors = downloads['id','name','category','device']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate the weekly downloads\n",
    "\n",
    "sqlCtx.registerDataFrameAsTable(downloads, \"downloads\")\n",
    "sqlCtx.registerDataFrameAsTable(usages, \"usages\")\n",
    "sqlCtx.registerDataFrameAsTable(revenues, \"revenues\")\n",
    "\n",
    "predictors = sqlCtx.sql(\"SELECT id, name, category, device, \"+\\\n",
    "           \"+\".join(dateRange[0:7])+\" AS week_1, \"+\\\n",
    "           \"+\".join(dateRange[7:14])+\" AS week_2, \"+\\\n",
    "           \"+\".join(dateRange[14:21])+\" AS week_3, \"+\\\n",
    "           \"+\".join(dateRange[21:28])+\" AS week_4, \"+\\\n",
    "           \"+\".join(dateRange[28:35])+\" AS week_5, \"+\\\n",
    "           \"+\".join(dateRange[35:42])+\" AS week_6, \"+\\\n",
    "           \"+\".join(dateRange[42:49])+\" AS week_7, \"+\\\n",
    "           \"+\".join(dateRange[49:56])+\" AS week_8 \"+\\\n",
    "           #\"+\".join(dateRange)+\" AS download_sum \\\n",
    "           \"from downloads\")\n",
    "sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "\n",
    "#I workaround the error by this modification. \n",
    "#I don't know why I couldn't run the code before but this workaround gives the same result.\n",
    "predictors = sqlCtx.sql(\"SELECT \"+', '.join(predictors.columns)+\", week_1+week_2+week_3+week_4+week_5+week_6+week_7+week_8 AS download_sum FROM predictors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m1 = sqlCtx.sql(\"SELECT * FROM usages WHERE metric = 1\")\n",
    "m2 = sqlCtx.sql(\"SELECT * FROM usages WHERE metric = 2\")\n",
    "m3 = sqlCtx.sql(\"SELECT * FROM usages WHERE metric = 3\")\n",
    "m4 = sqlCtx.sql(\"SELECT * FROM usages WHERE metric = 4\")\n",
    "sqlCtx.registerDataFrameAsTable(m1,\"m1\")\n",
    "sqlCtx.registerDataFrameAsTable(m2,\"m2\")\n",
    "sqlCtx.registerDataFrameAsTable(m3,\"m3\")\n",
    "sqlCtx.registerDataFrameAsTable(m4,\"m4\")\n",
    "sqlCtx.registerDataFrameAsTable(avg_score,\"avg_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make coefficients\n",
    "\n",
    "def get_coefficients(*args):\n",
    "    #The first element of the list is the degree of the coefficient\n",
    "    args = list(args)\n",
    "    return  float(np.polyfit(range(56),np.cumsum(args[1:]),args[0])[0])\n",
    "    \n",
    "#Generate the step max and min \n",
    "def get_maxStep(maximum,*args):\n",
    "    args=list(args)\n",
    "    if (np.count_nonzero(args) == 0):\n",
    "        return 0\n",
    "    m = 0\n",
    "    for d in range(1,56):\n",
    "        if (args[d]!=0 and args[d-1]!=0):\n",
    "            c = (args[d]-args[d-1])\n",
    "            if (maximum and m < c):\n",
    "                m = c\n",
    "            if ( not maximum and m > c):\n",
    "                m = c\n",
    "    return m\n",
    "\n",
    "def get_std(*args):\n",
    "    return float(np.std(list(args)))\n",
    "\n",
    "def get_nbMissing(*args):\n",
    "    return list(args).count(-1)\n",
    "replacementValue = 0\n",
    "#Generate the daily average\n",
    "def get_dailyAvg(*inp):\n",
    "    if (np.count_nonzero(inp - replacementValue*np.ones(len(inp))) == 0):\n",
    "        return 0\n",
    "    return  (1.0*sum(inp)/np.count_nonzero(inp - replacementValue*np.ones(len(inp))))\n",
    "\n",
    "def get_usage_coefficients(*args):\n",
    "    #The first element of the list is the degree of the coefficient\n",
    "    args = list(args)\n",
    "    if -1 in args: return 0\n",
    "    return  float(np.polyfit(range(8),args[1:],args[0])[0])\n",
    "\n",
    "def get_revenue_coefficients(*args):\n",
    "    #The first element of the list is the degree of the coefficient\n",
    "    args = list(args)\n",
    "    if -1 in args: return 0\n",
    "    return  float(np.polyfit(range(56),args[1:],args[0])[0])\n",
    "\n",
    "def get_usage_max(*args):\n",
    "    #The first element of the list is the degree of the coefficient\n",
    "    args = list(args)\n",
    "    return  float(args[1:].max())\n",
    "\n",
    "def get_usage_mean(*args):\n",
    "    #The first element of the list is the degree of the coefficient\n",
    "    args = list(args)\n",
    "    if -1 in args: return 0\n",
    "    return  float(args[1:].mean())\n",
    "\n",
    "def get_revenue_max(*args):\n",
    "    #The first element of the list is the degree of the coefficient\n",
    "    args = list(args)\n",
    "    if -1 in args: return 0\n",
    "    return  float(args[1:].max())\n",
    "\n",
    "def get_revenue_mean(*args):\n",
    "    #The first element of the list is the degree of the coefficient\n",
    "    args = list(args)\n",
    "    if -1 in args: return 0\n",
    "    return  float(args[1:].max())\n",
    "\n",
    "sqlCtx.registerFunction(\"get_nbMissing\", get_nbMissing,returnType=IntegerType())\n",
    "sqlCtx.registerFunction(\"get_std\", get_std,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_maxStep\", get_maxStep,returnType=IntegerType())\n",
    "sqlCtx.registerFunction(\"get_coefficients\", get_coefficients,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"daily_avg\", get_dailyAvg,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_usage_coefficients\", get_usage_coefficients,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_revenue_coefficients\", get_revenue_coefficients,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_usage_max\", get_usage_max,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_usage_mean\", get_usage_mean,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_revenue_max\", get_revenue_max,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_revenue_mean\", get_revenue_mean,returnType=FloatType())\n",
    "\n",
    "temp_downloads = sqlCtx.sql(\"SELECT id,name,category, device \\\n",
    ", get_coefficients(0,\"+\",\".join(dateRange)+\") AS coef_0 \\\n",
    ",get_coefficients(1,\"+\",\".join(dateRange)+\") AS coef_1 \\\n",
    ",get_coefficients(2,\"+\",\".join(dateRange)+\") AS coef_2 \\\n",
    ",get_coefficients(3,\"+\",\".join(dateRange)+\") AS coef_3 \\\n",
    ",get_maxStep(True,\"+\",\".join(dateRange)+\") AS max_step \\\n",
    ",get_maxStep(False,\"+\",\".join(dateRange)+\") AS min_step \\\n",
    ",get_std(\"+\",\".join(dateRange)+\") AS downloads_std \\\n",
    ",get_nbMissing(\"+\",\".join(dateRange)+\") AS nb_missing \\\n",
    ",daily_avg(\" + \",\".join(dateRange[0:56]) + \") AS daily_avg \\\n",
    " FROM downloads\")\n",
    "\n",
    "temp_m1 = sqlCtx.sql(\"SELECT id, name, category, device, \\\n",
    "get_usage_coefficients(0,\"+\",\".join(m1.columns[5:13])+\") AS m1_coef_0, \\\n",
    "get_usage_coefficients(1,\"+\",\".join(m1.columns[5:13])+\") AS m1_coef_1, \\\n",
    "get_usage_coefficients(2,\"+\",\".join(m1.columns[5:13])+\") AS m1_coef_2, \\\n",
    "get_usage_max(0,\"+\",\".join(m1.columns[5:13])+\") AS m1_max, \\\n",
    "get_usage_mean(0,\"+\",\".join(m1.columns[5:13])+\") AS m1_mean FROM m1\")\n",
    "\n",
    "temp_m2 = sqlCtx.sql(\"SELECT id, name, category, device, \\\n",
    "get_usage_coefficients(0,\"+\",\".join(m2.columns[5:13])+\") AS m2_coef_0, \\\n",
    "get_usage_coefficients(1,\"+\",\".join(m2.columns[5:13])+\") AS m2_coef_1, \\\n",
    "get_usage_coefficients(2,\"+\",\".join(m2.columns[5:13])+\") AS m2_coef_2, \\\n",
    "get_usage_max(0,\"+\",\".join(m2.columns[5:13])+\") AS m2_max, \\\n",
    "get_usage_mean(0,\"+\",\".join(m2.columns[5:13])+\") AS m2_mean FROM m2\")\n",
    "\n",
    "temp_m3 = sqlCtx.sql(\"SELECT id, name, category, device, \\\n",
    "get_usage_coefficients(0,\"+\",\".join(m3.columns[5:13])+\") AS m3_coef_0, \\\n",
    "get_usage_coefficients(1,\"+\",\".join(m3.columns[5:13])+\") AS m3_coef_1, \\\n",
    "get_usage_coefficients(2,\"+\",\".join(m3.columns[5:13])+\") AS m3_coef_2, \\\n",
    "get_usage_max(0,\"+\",\".join(m3.columns[5:13])+\") AS m3_max, \\\n",
    "get_usage_mean(0,\"+\",\".join(m3.columns[5:13])+\") AS m3_mean FROM m3\")\n",
    "\n",
    "temp_m4 = sqlCtx.sql(\"SELECT id, name, category, device, \\\n",
    "get_usage_coefficients(0,\"+\",\".join(m4.columns[5:13])+\") AS m4_coef_0, \\\n",
    "get_usage_coefficients(1,\"+\",\".join(m4.columns[5:13])+\") AS m4_coef_1, \\\n",
    "get_usage_coefficients(2,\"+\",\".join(m4.columns[5:13])+\") AS m4_coef_2, \\\n",
    "get_usage_max(0,\"+\",\".join(m4.columns[5:13])+\") AS m4_max, \\\n",
    "get_usage_mean(0,\"+\",\".join(m4.columns[5:13])+\") AS m4_mean FROM m4\")\n",
    "\n",
    "temp_revenues = sqlCtx.sql(\"SELECT id, name, category, device, \\\n",
    "get_revenue_coefficients(0,\"+\",\".join(revenues.columns[4:])+\") AS rev_coef_0, \\\n",
    "get_revenue_coefficients(1,\"+\",\".join(revenues.columns[4:])+\") AS rev_coef_1, \\\n",
    "get_revenue_coefficients(2,\"+\",\".join(revenues.columns[4:])+\") AS rev_coef_2, \\\n",
    "get_revenue_max(0,\"+\",\".join(revenues.columns[4:])+\") AS rev_max, \\\n",
    "get_revenue_mean(0,\"+\",\".join(revenues.columns[4:])+\") AS rev_mean FROM revenues\")\n",
    "\n",
    "predictors = predictors.join(temp_downloads,[\"id\",\"name\",\"category\",\"device\"])\n",
    "predictors = predictors.join(temp_revenues,[\"id\", \"name\", \"category\",\"device\"])\n",
    "predictors = predictors.join(temp_m1,[\"id\", \"name\", \"category\",\"device\"])\n",
    "predictors = predictors.join(temp_m2,[\"id\", \"name\", \"category\",\"device\"])\n",
    "predictors = predictors.join(temp_m3,[\"id\", \"name\", \"category\",\"device\"])\n",
    "predictors = predictors.join(temp_m4,[\"id\", \"name\", \"category\",\"device\"])\n",
    "predictors = predictors.join(avg_score,[\"id\", \"name\", \"category\",\"device\"])\n",
    "#predictors = predictors.join(temp_usages,[\"id\", \"name\", \"category\",\"device\"])\n",
    "#predictors = predictors.join(,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, name: string, category: string, device: string, week_1: int, week_2: int, week_3: int, week_4: int, week_5: int, week_6: int, week_7: int, week_8: int, download_sum: int, coef_0: float, coef_1: float, coef_2: float, coef_3: float, max_step: int, min_step: int, downloads_std: float, nb_missing: int, daily_avg: float, rev_coef_0: float, rev_coef_1: float, rev_coef_2: float, rev_max: float, rev_mean: float, metric: int, m1_coef_0: float, m1_coef_1: float, m1_coef_2: float, m1_max: float, m1_mean: float, metric: int, m2_coef_0: float, m2_coef_1: float, m2_coef_2: float, m2_max: float, m2_mean: float, metric: int, m3_coef_0: float, m3_coef_1: float, m3_coef_2: float, m3_max: float, m3_mean: float, metric: int, m4_coef_0: float, m4_coef_1: float, m4_coef_2: float, m4_max: float, m4_mean: float, avg_sent_score: double]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# previous downloads addition\n",
    "predictors = predictors.join(prev_downloads,[\"id\",\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Days since release generation\n",
    "def get_days(date):\n",
    "    return (datetime.datetime.strptime('03/01/2015', '%m/%d/%Y').date() \\\n",
    "            - datetime.datetime.strptime(date, '%Y-%m-%d').date()).days\n",
    "\n",
    "sqlCtx.registerDataFrameAsTable(release_date, \"release_date\")\n",
    "sqlCtx.registerFunction(\"get_days\", get_days,returnType=IntegerType())\n",
    "temp_date = sqlCtx.sql(\"SELECT id,name \\\n",
    ", get_days(release_date) AS days_since_release \\\n",
    " FROM release_date\")\n",
    "\n",
    "predictors = predictors.join(temp_date,[\"id\",\"name\"],\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ratings generation\n",
    "sqlCtx.registerDataFrameAsTable(ratings, \"ratings\")\n",
    "temp_ratings = sqlCtx.sql(\"SELECT id,name,category \\\n",
    ", start1/(start1+star2+star3+star4+star5) AS star1 \\\n",
    ", star2/(start1+star2+star3+star4+star5) AS star2 \\\n",
    ", star3/(start1+star2+star3+star4+star5) AS star3 \\\n",
    ", star4/(start1+star2+star3+star4+star5) AS star4 \\\n",
    ", star5/(start1+star2+star3+star4+star5) AS star5 \\\n",
    ", (start1+star2+star3+star4+star5) AS num_ratings \\\n",
    " FROM ratings\")\n",
    "\n",
    "predictors = predictors.join(temp_ratings,[\"id\",\"name\",\"category\"],\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Categories\n",
    "list_categories = [ x.category.replace(\" \",\"_\") for x in sqlCtx.sql(\"SELECT category \\\n",
    " FROM downloads\\\n",
    " group by category \\\n",
    " \").collect()]\n",
    "for cat in list_categories:\n",
    "    sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "    predictors=sqlCtx.sql('''SELECT *, CASE WHEN (category = \"'''+cat+'''\") THEN 1 ELSE 0 END AS '''+cat+''' FROM predictors''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Device\n",
    "sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "predictors=sqlCtx.sql('''SELECT *, CASE WHEN (device = \"iphone\") THEN 1 ELSE 0 END AS iphone FROM predictors''')\n",
    "sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "predictors=sqlCtx.sql('''SELECT *, CASE WHEN (device = \"ipad\") THEN 1 ELSE 0 END AS ipad FROM predictors''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_language(x):\n",
    "    try:\n",
    "        detected = langdetect.detect_langs(x.decode('utf8','ignore'))[0]\n",
    "        if detected.prob < 0.7:\n",
    "            return \"other\"\n",
    "        else :\n",
    "            return detected.lang\n",
    "    except:\n",
    "        return \"other\"\n",
    "sqlCtx.registerFunction(\"get_language\", get_language,returnType=StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Language of the title\n",
    "lang = ['ja','zh-cn','ko','en']\n",
    "for l in lang:\n",
    "    sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "    predictors=sqlCtx.sql('''SELECT *, CASE WHEN (get_language(name) = \"'''+l+'''\") THEN 1 \\\n",
    "    ELSE 0 END AS '''+l.replace(\"-\",\"_\")+''' FROM predictors''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, name: string, category: string, device: string, week_1: int, week_2: int, week_3: int, week_4: int, week_5: int, week_6: int, week_7: int, week_8: int, download_sum: int, coef_0: float, coef_1: float, coef_2: float, coef_3: float, max_step: int, min_step: int, downloads_std: float, nb_missing: int, daily_avg: float, rev_coef_0: float, rev_coef_1: float, rev_coef_2: float, rev_max: float, rev_mean: float, metric: int, m1_coef_0: float, m1_coef_1: float, m1_coef_2: float, m1_max: float, m1_mean: float, metric: int, m2_coef_0: float, m2_coef_1: float, m2_coef_2: float, m2_max: float, m2_mean: float, metric: int, m3_coef_0: float, m3_coef_1: float, m3_coef_2: float, m3_max: float, m3_mean: float, metric: int, m4_coef_0: float, m4_coef_1: float, m4_coef_2: float, m4_max: float, m4_mean: float, avg_sent_score: double, cumulative_downloads_2015-02: int, days_since_release: int, star1: double, star2: double, star3: double, star4: double, star5: double, num_ratings: int, Social_Networking: int, Finance: int, Books: int, Business: int, Newsstand: int, Games: int, Navigation: int, News: int, Music: int, Weather: int, Catalogs: int, Health_and_Fitness: int, Food_and_Drink: int, Shopping: int, Lifestyle: int, Productivity: int, Sports: int, Reference: int, Utilities: int, Education: int, Photo_and_Video: int, Entertainment: int, Medical: int, Travel: int, iphone: int, ipad: int, ja: int, zh_cn: int, ko: int, en: int]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reviews\n",
    "sqlCtx.registerDataFrameAsTable(reviews, \"reviews\")\n",
    "temp_reviews=sqlCtx.sql('''SELECT *, get_language(text) AS text_language FROM reviews''')\n",
    "temp_reviews.head(1)\n",
    "\n",
    "#language\n",
    "sqlCtx.registerDataFrameAsTable(temp_reviews, \"temp_reviews\")\n",
    "list_languages = [ x.text_language.replace(\" \",\"_\") for x in sqlCtx.sql(\"SELECT text_language \\\n",
    " FROM temp_reviews\\\n",
    " group by text_language \\\n",
    " \").collect()]\n",
    "print list_languages\n",
    "#escape is used in case some asshole used - or [space] anywhere\n",
    "def escape(text):\n",
    "    return text.replace(\" \",\"_\").replace(\"-\",\"_\")\n",
    "sqlCtx.registerFunction(\"escape\", escape,returnType=StringType())\n",
    "#sentiment\n",
    "def get_sentiment(text,title):\n",
    "    return 0.5\n",
    "sqlCtx.registerFunction(\"get_sentiment\", get_sentiment,returnType=FloatType())\n",
    "# number of reviews\n",
    "def get_recentReviews(date):\n",
    "    return int((datetime.datetime.strptime('03/01/2015', '%m/%d/%Y').date() \\\n",
    "            - datetime.datetime.strptime(date, '%Y-%m-%d').date()).days >=0)\n",
    "sqlCtx.registerFunction(\"get_recentReviews\", get_recentReviews,returnType=IntegerType())\n",
    "\n",
    "#languages expanded\n",
    "for l in list_languages:\n",
    "    sqlCtx.registerDataFrameAsTable(temp_reviews, \"temp_reviews\")\n",
    "    temp_reviews=sqlCtx.sql('''SELECT *, CASE WHEN (text_language = \"'''+l+'''\") THEN 1 \\\n",
    "    ELSE 0 END AS '''+l.replace(\"-\",\"_\")+''' FROM temp_reviews''')\n",
    " \n",
    "#sentiment, 1 and recent\n",
    "sqlCtx.registerDataFrameAsTable(temp_reviews, \"temp_reviews\")\n",
    "temp_reviews=sqlCtx.sql('''SELECT *\\\n",
    ", get_sentiment(text,title) AS sentiment\\\n",
    ", get_recentReviews(date) AS recent_reviews\\\n",
    ", 1 AS nb_reviews \\\n",
    "FROM temp_reviews''')\n",
    "\n",
    "# Countries\n",
    "sqlCtx.registerDataFrameAsTable(temp_reviews, \"temp_reviews\")\n",
    "list_countries = [ x.country.replace(\" \",\"_\") for x in sqlCtx.sql(\"SELECT country \\\n",
    " FROM temp_reviews \\\n",
    " group by country \\\n",
    " \").collect()]\n",
    "\n",
    "# Countries expanded\n",
    "for c in list_countries:\n",
    "    sqlCtx.registerDataFrameAsTable(temp_reviews, \"temp_reviews\")\n",
    "    temp_reviews=sqlCtx.sql('''SELECT *\\\n",
    "    , CASE WHEN (escape(country) = \"'''+c+'''\") THEN 1 \\\n",
    "    ELSE 0 END AS '''+c.replace(\"-\",\"_\")+'''\\\n",
    "    FROM temp_reviews''')\n",
    "sqlCtx.registerDataFrameAsTable(temp_reviews, \"temp_reviews\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_languages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-31bc43c6f205>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Group the reviews first step (need to modify both in case a new column is added)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfields_to_group\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"nb_reviews\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"recent_reviews\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist_languages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mfields_to_group\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist_countries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'list_languages' is not defined"
     ]
    }
   ],
   "source": [
    "#Group the reviews first step (need to modify both in case a new column is added)\n",
    "fields_to_group = [\"nb_reviews\",\"recent_reviews\"]\n",
    "for l in list_languages:\n",
    "    fields_to_group.append(l)\n",
    "for c in list_countries:\n",
    "    fields_to_group.append(c)\n",
    "    \n",
    "sql_quiery = \"SELECT id, AVG(sentiment) AS sentiment, COUNT(DISTINCT version) AS versions\"\n",
    "for f in fields_to_group:\n",
    "    sql_quiery+= \", SUM(\"+f+\") AS \"+f\n",
    "sql_quiery+= \" FROM temp_reviews GROUP BY id\"\n",
    "\n",
    "#print sql_quiery\n",
    "\n",
    "grp_reviews = sqlCtx.sql(sql_quiery)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grp_reviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-be32578fcee1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0msqlCtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregisterFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"get_gini\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_gini\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreturnType\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFloatType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0msqlCtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregisterDataFrameAsTable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrp_reviews\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"grp_reviews\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0msql_quiery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"SELECT id , sentiment, nb_reviews,recent_reviews,versions\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist_countries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'grp_reviews' is not defined"
     ]
    }
   ],
   "source": [
    "#Group the reviews second step (need to modify both in case a new column is added)\n",
    "#Make the Gini and other post grouping predictors\n",
    "def get_gini(*args):\n",
    "    args = list(args)\n",
    "    tot = sum(args)\n",
    "    return sum([1.0*x / tot * (1 - 1.0*x / tot) for x in args])\n",
    "sqlCtx.registerFunction(\"get_gini\", get_gini,returnType=FloatType())\n",
    "\n",
    "sqlCtx.registerDataFrameAsTable(grp_reviews, \"grp_reviews\")\n",
    "sql_quiery = \"SELECT id , sentiment, nb_reviews,recent_reviews,versions\"\n",
    "for c in list_countries:\n",
    "    sql_quiery+= \",\"+c\n",
    "sql_quiery+= \" , get_gini(0\"\n",
    "for l in list_languages:\n",
    "    sql_quiery+= \",\"+l\n",
    "sql_quiery+= \") AS gini_reviews FROM grp_reviews\"\n",
    "    \n",
    "grp_reviews = sqlCtx.sql(sql_quiery)\n",
    "\n",
    "predictors = predictors.join(grp_reviews,[\"id\"],\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate DL Projection\n",
    "sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "dl_projection = sqlCtx.sql(\"SELECT id, device, (download_sum+7*`cumulative_downloads_2015-02`) AS dl_projection \\\n",
    "                                FROM predictors\")\n",
    "predictors = predictors.join(dl_projection,[\"id\",\"device\"],\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews = reviews.head(100)\n",
    "reviews = sqlCtx.createDataFrame(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grp_reviews.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_std([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Row(**dict(row.asDict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(lambda row: Row(**dict(row.asDict(), test=-1)))(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "downloads.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grp_reviews.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
