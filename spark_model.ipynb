{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imported libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import langdetect\n",
    "import datetime\n",
    "%matplotlib inline  \n",
    "from sklearn.svm import SVR\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "import findspark; findspark.init()\n",
    "import pyspark\n",
    "import os\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "  \"--packages com.databricks:spark-csv_2.11:1.4.0 pyspark-shell\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    sc = pyspark.SparkContext()\n",
    "except Exception as e:\n",
    "    print \"SparkContext exists... Continuing on.\"\n",
    "    \n",
    "sqlCtx = pyspark.sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "downloads = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true',inferSchema='true') \\\n",
    "    .load('train_app_downloads.csv').drop('')\n",
    "ratings = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true',inferSchema='true') \\\n",
    "    .load('train_app_rating.csv').drop('')\n",
    "usages = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true',inferSchema='true') \\\n",
    "    .load('train_usage.csv').drop('')\n",
    "revenues = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true',inferSchema='true') \\\n",
    "    .load('train_revenue.csv').drop('')\n",
    "output = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true',inferSchema='true') \\\n",
    "    .load('train_final_downloads.csv').drop('')\n",
    "prev_downloads = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true',inferSchema='true') \\\n",
    "    .load('train_cumulative_downloads_2015-02.csv').drop('')\n",
    "    \n",
    "reviews = pd.read_csv('train_app_review.csv')\n",
    "reviews_schema = StructType([\n",
    "    StructField(\"id\",IntegerType(),True),\n",
    "    StructField(\"name\",StringType(),True),\n",
    "    StructField(\"country\",StringType(),True),\n",
    "    StructField(\"rating\",IntegerType(),True),\n",
    "    StructField(\"date\",StringType(),True),\n",
    "    StructField(\"title\",StringType(),True),\n",
    "    StructField(\"version\",StringType(),True),\n",
    "    StructField(\"text\",StringType(),True),\n",
    "    StructField(\"reviewer\",StringType(),True)\n",
    "])\n",
    "reviews = sqlCtx.createDataFrame(reviews,reviews_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old_dateRange = pd.date_range('2015-03-01', periods=56).format(formatter=lambda x: x.strftime('%Y-%m-%d'))\n",
    "dateRange = pd.date_range('2015-03-01', periods=56).format(formatter=lambda x: x.strftime('%Y_%m_%d'))\n",
    "for d in range(56):\n",
    "    revenues = revenues.withColumnRenamed(old_dateRange[d],dateRange[d])\n",
    "    usages = usages.withColumnRenamed(old_dateRange[d],dateRange[d])\n",
    "    downloads = downloads.withColumnRenamed(old_dateRange[d],dateRange[d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = downloads['id','name','category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate the weekly downloads\n",
    "\n",
    "sqlCtx.registerDataFrameAsTable(downloads, \"downloads\")\n",
    "\n",
    "predictors = sqlCtx.sql(\"SELECT id, name, category, device , \"+\\\n",
    "           \"+\".join(dateRange[0:7])+\" AS week_1 ,\"+\\\n",
    "           \"+\".join(dateRange[7:14])+\" AS week_2 ,\"+\\\n",
    "           \"+\".join(dateRange[14:21])+\" AS week_3 ,\"+\\\n",
    "           \"+\".join(dateRange[21:28])+\" AS week_4 ,\"+\\\n",
    "           \"+\".join(dateRange[28:35])+\" AS week_5 ,\"+\\\n",
    "           \"+\".join(dateRange[35:42])+\" AS week_6 ,\"+\\\n",
    "           \"+\".join(dateRange[42:49])+\" AS week_7 ,\"+\\\n",
    "           \"+\".join(dateRange[49:56])+\" AS week_8 ,\"+\\\n",
    "           \"+\".join(dateRange)+\" AS download_sum \\\n",
    "           from downloads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make coefficients\n",
    "\n",
    "def get_coefficients(*args):\n",
    "    #The first element of the list is the degree of the coefficient\n",
    "    args = list(args)\n",
    "    return  float(np.polyfit(range(56),np.cumsum(args[1:]),args[0])[0])\n",
    "    \n",
    "#Generate the step max and min \n",
    "def get_maxStep(maximum,*args):\n",
    "    args=list(args)\n",
    "    if (np.count_nonzero(args) == 0):\n",
    "        return 0\n",
    "    m = 0\n",
    "    for d in range(1,56):\n",
    "        if (args[d]!=0 and args[d-1]!=0):\n",
    "            c = (args[d]-args[d-1])\n",
    "            if (maximum and m < c):\n",
    "                m = c\n",
    "            if ( not maximum and m > c):\n",
    "                m = c\n",
    "    return m\n",
    "\n",
    "def get_std(*args):\n",
    "    return float(np.std(list(args)))\n",
    "\n",
    "def get_nbMissing(*args):\n",
    "    return list(args).count(-1)\n",
    "\n",
    "sqlCtx.registerFunction(\"get_nbMissing\", get_nbMissing,returnType=IntegerType())\n",
    "sqlCtx.registerFunction(\"get_std\", get_std,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_maxStep\", get_maxStep,returnType=IntegerType())\n",
    "sqlCtx.registerFunction(\"get_coefficients\", get_coefficients,returnType=FloatType())\n",
    "temp_df = sqlCtx.sql(\"SELECT id,name,category, device \\\n",
    ", get_coefficients(0,\"+\",\".join(dateRange)+\") AS coef_0 \\\n",
    ",get_coefficients(1,\"+\",\".join(dateRange)+\") AS coef_1 \\\n",
    ",get_coefficients(2,\"+\",\".join(dateRange)+\") AS coef_2 \\\n",
    ",get_coefficients(3,\"+\",\".join(dateRange)+\") AS coef_3 \\\n",
    ",get_maxStep(True,\"+\",\".join(dateRange)+\") AS max_step \\\n",
    ",get_maxStep(False,\"+\",\".join(dateRange)+\") AS min_step \\\n",
    ",get_std(\"+\",\".join(dateRange)+\") AS downloads_std \\\n",
    ",get_nbMissing(\"+\",\".join(dateRange)+\") AS nb_missing \\\n",
    " FROM downloads\")\n",
    "\n",
    "\n",
    "predictors = predictors.join(temp_df,(temp_df.id==predictors.id) & (temp_df.name==predictors.name) \\\n",
    "& (temp_df.category==predictors.category),\"inner\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_std([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Row(**dict(row.asDict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(lambda row: Row(**dict(row.asDict(), test=-1)))(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "downloads.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
