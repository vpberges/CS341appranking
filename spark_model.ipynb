{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imported libraries\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import boto\n",
    "import numpy as np\n",
    "from numpy import matlib\n",
    "import scipy as sp\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import langdetect\n",
    "import datetime\n",
    "%matplotlib inline  \n",
    "from sklearn.svm import SVR\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "import os\n",
    "import findspark; findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import Row\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors \n",
    "from pyspark.mllib.regression import LabeledPoint  \n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD \n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "  \"--packages com.databricks:spark-csv_2.11:1.4.0 pyspark-shell\"\n",
    ")\n",
    "\n",
    "#NEED TO ADD \"  SPARK_DRIVER_MEMORY=5G   \"  to ./conf/spark-env.sh \n",
    "\n",
    "try:\n",
    "    conf = SparkConf().set(\"spark.executor.memory\", \"3g\")\n",
    "    sc = pyspark.SparkContext(conf=conf)\n",
    "except Exception as e:\n",
    "    print \"SparkContext exists... Continuing on.\"\n",
    "    \n",
    "sqlCtx = pyspark.sql.SQLContext(sc)\n",
    "sc.setCheckpointDir('checkpoint/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", os.environ['AWS_ACCESS_KEY_ID'])\n",
    "    sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", os.environ['AWS_SECRET_ACCESS_KEY'])\n",
    "    downloads = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('s3n://cs341bucket1/Data/train_app_downloads.csv').drop('')\n",
    "    ratings = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('s3n://cs341bucket1/Data/train_app_rating.csv').drop('')\n",
    "    usages = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('s3n://cs341bucket1/Data/train_usage.csv').drop('')\n",
    "    revenues = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('s3n://cs341bucket1/Data/train_revenue.csv').drop('')\n",
    "    output = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('s3n://cs341bucket1/Data/train_final_downloads.csv').drop('')\n",
    "    prev_downloads = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('s3n://cs341bucket1/Data/train_cumulative_downloads_2015-02.csv').drop('')  \n",
    "    release_date = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('s3n://cs341bucket1/Data/train_release_date.csv').drop('')\n",
    "    text_score = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='false',inferSchema='true') \\\n",
    "        .load('s3n://cs341bucket1/Data/sentiment.csv').drop('')\n",
    "    title_score = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='false',inferSchema='true') \\\n",
    "        .load('s3n://cs341bucket1/Data/t_sentiment.csv').drop('')\n",
    "    avg_score = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('s3n://cs341bucket1/Data/avg_sent_score.csv').drop('')\n",
    "    rating_country = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('s3n://cs341bucket1/Data/train_rating_by_country.csv').drop('')\n",
    "\n",
    "    reviews_schema = StructType([\n",
    "        StructField(\"id\",IntegerType(),True),\n",
    "        StructField(\"name\",StringType(),True),\n",
    "        StructField(\"country\",StringType(),True),\n",
    "        StructField(\"rating\",IntegerType(),True),\n",
    "        StructField(\"date\",StringType(),True),\n",
    "        StructField(\"title\",StringType(),True),\n",
    "        StructField(\"version\",StringType(),True),\n",
    "        StructField(\"text\",StringType(),True),\n",
    "        StructField(\"reviewer\",StringType(),True)\n",
    "    ])\n",
    "    reviews = pd.read_csv('s3://cs341bucket1/Data/train_app_review.csv')\n",
    "    reviews = sqlCtx.createDataFrame(reviews,reviews_schema)\n",
    "except:\n",
    "    downloads = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('train_app_downloads.csv').drop('')\n",
    "    ratings = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('train_app_rating.csv').drop('')\n",
    "    usages = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('train_usage.csv').drop('')\n",
    "    revenues = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('train_revenue.csv').drop('')\n",
    "    output = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('train_final_downloads.csv').drop('')\n",
    "    prev_downloads = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('train_cumulative_downloads_2015-02.csv').drop('')  \n",
    "    release_date = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('train_release_date.csv').drop('')\n",
    "    text_score = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='false',inferSchema='true') \\\n",
    "        .load('sentiment.csv').drop('')\n",
    "    title_score = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='false',inferSchema='true') \\\n",
    "        .load('t_sentiment.csv').drop('')\n",
    "    avg_score = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('avg_sent_score.csv').drop('')\n",
    "    rating_country = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('train_rating_by_country.csv').drop('')\n",
    "\n",
    "    reviews_schema = StructType([\n",
    "        StructField(\"id\",IntegerType(),True),\n",
    "        StructField(\"name\",StringType(),True),\n",
    "        StructField(\"country\",StringType(),True),\n",
    "        StructField(\"rating\",IntegerType(),True),\n",
    "        StructField(\"date\",StringType(),True),\n",
    "        StructField(\"title\",StringType(),True),\n",
    "        StructField(\"version\",StringType(),True),\n",
    "        StructField(\"text\",StringType(),True),\n",
    "        StructField(\"reviewer\",StringType(),True)\n",
    "    ])\n",
    "    reviews = pd.read_csv('train_app_review.csv')\n",
    "    reviews = sqlCtx.createDataFrame(reviews,reviews_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#try:\n",
    "#    sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", os.environ['AWS_ACCESS_KEY_ID'])\n",
    "#    sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", os.environ['AWS_SECRET_ACCESS_KEY'])\n",
    "downloads = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('C:\\\\Users/andre/Documents/StanfordMS/CS 341/Data/train_app_downloads.csv').drop('')\n",
    "ratings = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('C:\\\\Users\\\\andre\\\\Documents\\\\StanfordMS\\\\CS 341\\\\Data\\\\train_app_rating.csv').drop('')\n",
    "usages = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('C:\\\\Users\\\\andre\\\\Documents\\\\StanfordMS\\\\CS 341\\\\Data\\\\train_usage.csv').drop('')\n",
    "revenues = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('C:\\\\Users\\\\andre\\\\Documents\\\\StanfordMS\\\\CS 341\\\\Data\\\\train_revenue.csv').drop('')\n",
    "output = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('C:\\\\Users\\\\andre\\\\Documents\\\\StanfordMS\\\\CS 341\\\\Data\\\\train_final_downloads.csv').drop('')\n",
    "prev_downloads = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('C:\\\\Users\\\\andre\\\\Documents\\\\StanfordMS\\\\CS 341\\\\Data\\\\train_cumulative_downloads_2015-02.csv').drop('')\n",
    "release_date = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('C:\\\\Users\\\\andre\\\\Documents\\\\StanfordMS\\\\CS 341\\\\Data\\\\train_release_date.csv').drop('')\n",
    "text_score = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='false',inferSchema='true') \\\n",
    "        .load('C:\\\\Users\\\\andre\\\\Documents\\\\StanfordMS\\\\CS 341\\\\Data\\\\sentiment.csv').drop('')\n",
    "title_score = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='false',inferSchema='true') \\\n",
    "        .load('C:\\\\Users\\\\andre\\\\Documents\\\\StanfordMS\\\\CS 341\\\\Data\\\\t_sentiment.csv').drop('')\n",
    "avg_score = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('C:\\\\Users\\\\andre\\\\Documents\\\\StanfordMS\\\\CS 341\\\\Data\\\\avg_sent_score.csv').drop('')\n",
    "\n",
    "reviews_schema = StructType([\n",
    "        StructField(\"id\",IntegerType(),True),\n",
    "        StructField(\"name\",StringType(),True),\n",
    "        StructField(\"country\",StringType(),True),\n",
    "        StructField(\"rating\",IntegerType(),True),\n",
    "        StructField(\"date\",StringType(),True),\n",
    "        StructField(\"title\",StringType(),True),\n",
    "        StructField(\"version\",StringType(),True),\n",
    "        StructField(\"text\",StringType(),True),\n",
    "        StructField(\"reviewer\",StringType(),True)\n",
    "    ])\n",
    "reviews = pd.read_csv('C:\\\\Users\\\\andre\\\\Documents\\\\StanfordMS\\\\CS 341\\\\Data\\\\train_app_review.csv')\n",
    "reviews = sqlCtx.createDataFrame(reviews,reviews_schema)\n",
    "'''except:\n",
    "    downloads = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('train_app_downloads.csv').drop('')\n",
    "    ratings = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('train_app_rating.csv').drop('')\n",
    "    usages = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('train_usage.csv').drop('')\n",
    "    revenues = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('train_revenue.csv').drop('')\n",
    "    output = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('train_final_downloads.csv').drop('')\n",
    "    prev_downloads = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('train_cumulative_downloads_2015-02.csv').drop('')  \n",
    "    release_date = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('train_release_date.csv').drop('')\n",
    "    text_score = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='false',inferSchema='true') \\\n",
    "        .load('sentiment.csv').drop('')\n",
    "    title_score = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='false',inferSchema='true') \\\n",
    "        .load('t_sentiment.csv').drop('')\n",
    "    avg_score = sqlCtx.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true',inferSchema='true') \\\n",
    "        .load('avg_sent_score.csv').drop('')\n",
    "\n",
    "    reviews_schema = StructType([\n",
    "        StructField(\"id\",IntegerType(),True),\n",
    "        StructField(\"name\",StringType(),True),\n",
    "        StructField(\"country\",StringType(),True),\n",
    "        StructField(\"rating\",IntegerType(),True),\n",
    "        StructField(\"date\",StringType(),True),\n",
    "        StructField(\"title\",StringType(),True),\n",
    "        StructField(\"version\",StringType(),True),\n",
    "        StructField(\"text\",StringType(),True),\n",
    "        StructField(\"reviewer\",StringType(),True)\n",
    "    ])\n",
    "    reviews = pd.read_csv('train_app_review.csv')\n",
    "    reviews = sqlCtx.createDataFrame(reviews,reviews_schema)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#usage imputation\n",
    "imputer = Imputer(missing_values=-1, strategy='median', axis=0)\n",
    "pd_usages = usages.toPandas()\n",
    "category = list(set(pd_usages[\"category\"].values))\n",
    "imp = pd.DataFrame(columns = pd_usages.columns)\n",
    "for cat in category:\n",
    "    #for dev in [\"iphone\",\"ipad\"]:\n",
    "        for metric in range(1,5):\n",
    "            curr_df = pd_usages.ix[pd_usages[\"category\"]==cat,:]\n",
    "            #curr_df = curr_df.ix[curr_df[\"device\"]==dev,:]\n",
    "            curr_df = curr_df.ix[curr_df[\"metric\"]==metric,:]\n",
    "            name = curr_df.columns\n",
    "            df1 = curr_df.ix[:,0:6]\n",
    "            df2 = pd.DataFrame(imputer.fit_transform(curr_df.ix[:,6:]))\n",
    "            df2.index = df1.index\n",
    "            curr_df = pd.concat([df1,df2],axis = 1)\n",
    "            curr_df.columns = name \n",
    "            imp = pd.concat([imp,curr_df],axis = 0)\n",
    "usages = sqlCtx.createDataFrame(imp)\n",
    "\n",
    "#revenue imputation\n",
    "pd_revenues = revenues.toPandas()\n",
    "imp = pd.DataFrame(columns = revenues.columns)\n",
    "for cat in category:\n",
    "    for dev in [\"iphone\",\"ipad\"]:\n",
    "        #for metric in range(1,5):\n",
    "            curr_df = pd_revenues.ix[pd_revenues[\"category\"]==cat,:]\n",
    "            curr_df = curr_df.ix[curr_df[\"device\"]==dev,:]\n",
    "            #curr_df = curr_df.ix[curr_df[\"metric\"]==metric,:]\n",
    "            name = curr_df.columns\n",
    "            df1 = curr_df.ix[:,0:5]\n",
    "            df2 = pd.DataFrame(imputer.fit_transform(curr_df.ix[:,5:]))\n",
    "            df2.index = df1.index\n",
    "            curr_df = pd.concat([df1,df2],axis = 1)\n",
    "            curr_df.columns = name \n",
    "            imp = pd.concat([imp,curr_df],axis = 0)\n",
    "revenues = sqlCtx.createDataFrame(imp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old_dateRange = pd.date_range('03/01/2015', periods=56).format(formatter=lambda x: x.strftime('%Y-%m-%d'))\n",
    "dateRange = pd.date_range('03/01/2015', periods=56).format(formatter=lambda x: x.strftime('%m_%d_%Y'))\n",
    "for d in range(56):\n",
    "    revenues = revenues.withColumnRenamed(old_dateRange[d],dateRange[d])\n",
    "    usages = usages.withColumnRenamed(old_dateRange[d],dateRange[d])\n",
    "    downloads = downloads.withColumnRenamed(old_dateRange[d],dateRange[d])  \n",
    "output = output.withColumnRenamed(\"cumulative_downloads_2016-02\",\"cumulative_downloads_2016_02\")\n",
    "prev_downloads = prev_downloads.withColumnRenamed(\"cumulative_downloads_2015-02\",\"cumulative_downloads_2015_02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initialization\n",
    "predictors = downloads['id','name','category','device']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate the weekly downloads\n",
    "\n",
    "sqlCtx.registerDataFrameAsTable(downloads, \"downloads\")\n",
    "sqlCtx.registerDataFrameAsTable(usages, \"usages\")\n",
    "sqlCtx.registerDataFrameAsTable(revenues, \"revenues\")\n",
    "\n",
    "def get_log_week(*args):\n",
    "    args = [x if not (x<0) else 0 for x in list(args)]\n",
    "    nb_0 = args.count(0)\n",
    "    if nb_0 == 7:\n",
    "        return float(0)\n",
    "    return math.log(1.0*sum(args)/(7-nb_0))\n",
    "    Y = [np.log(c) for c in args if c > 0]\n",
    "    if len(Y) == 0 : return 0\n",
    "    return float(1.0*sum(Y))/(7-nb_0)\n",
    "sqlCtx.registerFunction(\"get_log_week\", get_log_week,returnType=FloatType())\n",
    "def get_download_sum(*args):\n",
    "    args = [x if not (x==-1) else 0 for x in list(args)]\n",
    "    return (1.0*sum(args))\n",
    "sqlCtx.registerFunction(\"get_download_sum\", get_download_sum,returnType=FloatType())\n",
    "\n",
    "predictors = sqlCtx.sql(\"SELECT id, name, category, device, \\\n",
    "           get_log_week(\"+ \",\".join(dateRange[0:7])+\") AS week_1 \\\n",
    "            ,get_log_week(\"+\",\".join(dateRange[7:14])+\") AS week_2 \\\n",
    "            ,get_log_week(\"+ \",\".join(dateRange[14:21])+\") AS week_3 \\\n",
    "           ,get_log_week(\"+\",\".join(dateRange[21:28])+\") AS week_4 \\\n",
    "           ,get_log_week(\"+\",\".join(dateRange[28:35])+\") AS week_5 \\\n",
    "           ,get_log_week(\"+\",\".join(dateRange[35:42])+\") AS week_6 \\\n",
    "           ,get_log_week(\"+\",\".join(dateRange[42:49])+\") AS week_7 \\\n",
    "           ,get_log_week(\"+\",\".join(dateRange[49:56])+\") AS week_8\\\n",
    "           ,get_download_sum(\"+ \",\".join(dateRange)+\") AS download_sum \\\n",
    "           from downloads\")\n",
    "sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "\n",
    "#I workaround the error by this modification. \n",
    "#I don't know why I couldn't run the code before but this workaround gives the same result.\n",
    "#predictors = sqlCtx.sql(\"SELECT \"+', '.join(predictors.columns)+\", week_1+week_2+week_3+week_4+week_5+week_6+week_7+week_8 AS download_sum FROM predictors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m1 = sqlCtx.sql(\"SELECT * FROM usages WHERE metric = 1\")\n",
    "m2 = sqlCtx.sql(\"SELECT * FROM usages WHERE metric = 2\")\n",
    "m3 = sqlCtx.sql(\"SELECT * FROM usages WHERE metric = 3\")\n",
    "m4 = sqlCtx.sql(\"SELECT * FROM usages WHERE metric = 4\")\n",
    "sqlCtx.registerDataFrameAsTable(m1,\"m1\")\n",
    "sqlCtx.registerDataFrameAsTable(m2,\"m2\")\n",
    "sqlCtx.registerDataFrameAsTable(m3,\"m3\")\n",
    "sqlCtx.registerDataFrameAsTable(m4,\"m4\")\n",
    "sqlCtx.registerDataFrameAsTable(avg_score,\"avg_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make coefficients\n",
    "\n",
    "def get_coefficients(*args):\n",
    "    #The first element of the list is the degree of the coefficient\n",
    "    #args = [x if not (x==-1) else 0 for x in list(args)]    \n",
    "    #return  float(np.polyfit(range(56),np.cumsum(args[1:]),args[0])[0])\n",
    "    Y = [np.log(c) for c in args[1:] if c>0]\n",
    "    if len(Y)<=1: return float(0)\n",
    "    return  float(np.polyfit(range(len(Y)),np.cumsum(Y),args[0])[0])\n",
    "    \n",
    "#Generate the step max and min \n",
    "def get_maxStep(maximum,*args):\n",
    "    args = [x if not (x==-1) else 0 for x in list(args)]\n",
    "    if (np.count_nonzero(args) == 0):\n",
    "        return float(0)\n",
    "    m = 0\n",
    "    for d in range(1,56):\n",
    "        if (args[d]!=0 and args[d-1]!=0):\n",
    "            #c = (args[d]-args[d-1])\n",
    "            c = float(args[d])/args[d-1]\n",
    "            if (maximum and m < c):\n",
    "                m = c\n",
    "            if ( not maximum and m > c):\n",
    "                m = c\n",
    "    #return m\n",
    "    if m==0: return float(0)\n",
    "    return float(np.log(m))\n",
    "\n",
    "def get_std(*args):\n",
    "    args = [x if not (x==-1) else 0 for x in list(args)]\n",
    "    return float(np.std(list(args)))\n",
    "\n",
    "def get_nbMissing(*args):\n",
    "    return list(args).count(-1)\n",
    "\n",
    "#Generate the daily average\n",
    "\n",
    "def get_revenue_coefficients(*args):\n",
    "    #The first element of the list is the degree of the coefficient\n",
    "    args = list(args)\n",
    "    time_series = [np.log(c) for c in args[1:] if c>0]\n",
    "    if len(time_series) <=1: return float(0)\n",
    "    #return float(np.polyfit(np.array(range(56)),args[1:],args[0])[0])\n",
    "    return  float(np.polyfit(np.array(range(len(time_series))),time_series,args[0])[0])\n",
    "\n",
    "def get_usage_coefficients(*args):\n",
    "    #The first element of the list is the degree of the coefficient\n",
    "    args = list(args)\n",
    "    if -1 in args: return 0\n",
    "    return  float(np.polyfit(range(8),args[1:],args[0])[0])\n",
    "\n",
    "def get_usage_max(*args):\n",
    "    #The first element of the list is the degree of the coefficient\n",
    "    args = list(args)\n",
    "    args = [c for c in args[1:] if c!=-1]\n",
    "    if len(args)==0: return 0\n",
    "    return  float(np.max(args))\n",
    "\n",
    "def get_usage_mean(*args):\n",
    "    #The first element of the list is the degree of the coefficient\n",
    "    args = list(args)\n",
    "    args = [c for c in args[1:] if c!=-1]\n",
    "    if len(args)==0: return 0\n",
    "    return  float(np.mean(args))\n",
    "\n",
    "def get_revenue_max(*args):\n",
    "    #The first element of the list is the degree of the coefficient\n",
    "    args = list(args)\n",
    "    time_series = [np.log(c) for c in args[1:] if c>0]\n",
    "    if len(time_series) <=1: return float(0)\n",
    "    #args = [c for c in args[1:] if c!=-1]\n",
    "    #if len(args)==0: return 0\n",
    "    #return  float(np.max(args))\n",
    "    return float(np.max(time_series))\n",
    "\n",
    "def get_revenue_mean(*args):\n",
    "    #The first element of the list is the degree of the coefficient\n",
    "    args = list(args)\n",
    "    #args = [c for c in args[1:] if c!=-1]\n",
    "    #if len(args)==0: return 0\n",
    "    #return  float(np.mean(args))\n",
    "    time_series = [np.log(c) for c in args[1:] if c>0]\n",
    "    if len(time_series) <=1: return float(0)\n",
    "    return float(np.mean(time_series))\n",
    "\n",
    "def get_dailyAvg(*args):\n",
    "    args = [x if not (x==-1) else 0 for x in list(args)]\n",
    "    if (np.count_nonzero(args) == 0):\n",
    "        return float(0)\n",
    "    #return  (1.0*sum(args)/np.count_nonzero(args))\n",
    "    return float(sum([np.log(c) for c in args if c>0]))/np.count_nonzero(args)\n",
    "\n",
    "sqlCtx.registerFunction(\"get_nbMissing\", get_nbMissing,returnType=IntegerType())\n",
    "sqlCtx.registerFunction(\"get_std\", get_std,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_maxStep\", get_maxStep,returnType=IntegerType())\n",
    "sqlCtx.registerFunction(\"get_maxStep\", get_maxStep,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_coefficients\", get_coefficients,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"daily_avg\", get_dailyAvg,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_usage_coefficients\", get_usage_coefficients,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_revenue_coefficients\", get_revenue_coefficients,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_usage_max\", get_usage_max,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_usage_mean\", get_usage_mean,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_revenue_max\", get_revenue_max,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_revenue_mean\", get_revenue_mean,returnType=FloatType())\n",
    "\n",
    "temp_downloads = sqlCtx.sql(\"SELECT id,name,category, device \\\n",
    ", get_coefficients(0,\"+\",\".join(dateRange)+\") AS coef_0 \\\n",
    ",get_coefficients(1,\"+\",\".join(dateRange)+\") AS coef_1 \\\n",
    ",get_coefficients(2,\"+\",\".join(dateRange)+\") AS coef_2 \\\n",
    ",get_coefficients(3,\"+\",\".join(dateRange)+\") AS coef_3 \\\n",
    ",get_maxStep(True,\"+\",\".join(dateRange)+\") AS max_step \\\n",
    ",get_maxStep(False,\"+\",\".join(dateRange)+\") AS min_step \\\n",
    ",get_std(\"+\",\".join(dateRange)+\") AS downloads_std \\\n",
    ",get_nbMissing(\"+\",\".join(dateRange)+\") AS nb_missing \\\n",
    ",daily_avg(\" + \",\".join(dateRange[0:56]) + \") AS daily_avg \\\n",
    " FROM downloads\")\n",
    "\n",
    "predictors = predictors.join(temp_downloads,[\"id\",\"name\",\"category\",\"device\"],how='left_outer')\n",
    "\n",
    "temp_m1 = sqlCtx.sql(\"SELECT id, name, category, device, \\\n",
    "get_usage_coefficients(0,\"+\",\".join(m1.columns[5:13])+\") AS m1_coef_0, \\\n",
    "get_usage_coefficients(1,\"+\",\".join(m1.columns[5:13])+\") AS m1_coef_1, \\\n",
    "get_usage_coefficients(2,\"+\",\".join(m1.columns[5:13])+\") AS m1_coef_2, \\\n",
    "get_usage_max(0,\"+\",\".join(m1.columns[5:13])+\") AS m1_max, \\\n",
    "get_usage_mean(0,\"+\",\".join(m1.columns[5:13])+\") AS m1_mean FROM m1\")\n",
    "\n",
    "temp_m2 = sqlCtx.sql(\"SELECT id, name, category, device, \\\n",
    "get_usage_coefficients(0,\"+\",\".join(m2.columns[5:13])+\") AS m2_coef_0, \\\n",
    "get_usage_coefficients(1,\"+\",\".join(m2.columns[5:13])+\") AS m2_coef_1, \\\n",
    "get_usage_coefficients(2,\"+\",\".join(m2.columns[5:13])+\") AS m2_coef_2, \\\n",
    "get_usage_max(0,\"+\",\".join(m2.columns[5:13])+\") AS m2_max, \\\n",
    "get_usage_mean(0,\"+\",\".join(m2.columns[5:13])+\") AS m2_mean FROM m2\")\n",
    "\n",
    "temp_m3 = sqlCtx.sql(\"SELECT id, name, category, device, \\\n",
    "get_usage_coefficients(0,\"+\",\".join(m3.columns[5:13])+\") AS m3_coef_0, \\\n",
    "get_usage_coefficients(1,\"+\",\".join(m3.columns[5:13])+\") AS m3_coef_1, \\\n",
    "get_usage_coefficients(2,\"+\",\".join(m3.columns[5:13])+\") AS m3_coef_2, \\\n",
    "get_usage_max(0,\"+\",\".join(m3.columns[5:13])+\") AS m3_max, \\\n",
    "get_usage_mean(0,\"+\",\".join(m3.columns[5:13])+\") AS m3_mean FROM m3\")\n",
    "\n",
    "temp_m4 = sqlCtx.sql(\"SELECT id, name, category, device, \\\n",
    "get_usage_coefficients(0,\"+\",\".join(m4.columns[5:13])+\") AS m4_coef_0, \\\n",
    "get_usage_coefficients(1,\"+\",\".join(m4.columns[5:13])+\") AS m4_coef_1, \\\n",
    "get_usage_coefficients(2,\"+\",\".join(m4.columns[5:13])+\") AS m4_coef_2, \\\n",
    "get_usage_max(0,\"+\",\".join(m4.columns[5:13])+\") AS m4_max, \\\n",
    "get_usage_mean(0,\"+\",\".join(m4.columns[5:13])+\") AS m4_mean FROM m4\")\n",
    "\n",
    "temp_revenues = sqlCtx.sql(\"SELECT id, name, category, device, \\\n",
    "get_revenue_coefficients(0,\"+\",\".join(revenues.columns[4:])+\") AS rev_coef_0, \\\n",
    "get_revenue_coefficients(1,\"+\",\".join(revenues.columns[4:])+\") AS rev_coef_1, \\\n",
    "get_revenue_coefficients(2,\"+\",\".join(revenues.columns[4:])+\") AS rev_coef_2, \\\n",
    "get_revenue_max(0,\"+\",\".join(revenues.columns[4:])+\") AS rev_max, \\\n",
    "get_revenue_mean(0,\"+\",\".join(revenues.columns[4:])+\") AS rev_mean FROM revenues\")\n",
    "\n",
    "\n",
    "predictors = predictors.join(temp_revenues,[\"id\", \"name\", \"category\",\"device\"],how='left_outer')\n",
    "predictors = predictors.join(temp_m1,[\"id\", \"name\", \"category\",\"device\"],how='left_outer')\n",
    "predictors = predictors.join(temp_m2,[\"id\", \"name\", \"category\",\"device\"],how='left_outer')\n",
    "predictors = predictors.join(temp_m3,[\"id\", \"name\", \"category\",\"device\"],how='left_outer')\n",
    "predictors = predictors.join(temp_m4,[\"id\", \"name\", \"category\",\"device\"],how='left_outer')\n",
    "predictors = predictors.join(avg_score,[\"id\", \"name\", \"category\",\"device\"],how='left_outer')\n",
    "#predictors = predictors.join(temp_usages,[\"id\", \"name\", \"category\",\"device\"],how='left_outer')\n",
    "#predictors = predictors.join(,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# previous downloads addition\n",
    "predictors = predictors.join(prev_downloads,[\"id\",\"device\"],how='left_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Days since release generation with imputation\n",
    "def get_days(date, id):\n",
    "    if date == \"no_date\":\n",
    "        # return 0\n",
    "        id = 1.0*id/100000000\n",
    "        return int(5856.25394104 -1731.74798728*id+195.553086*id**2  -8.12861635*id**3)\n",
    "    else:\n",
    "        try:\n",
    "            return (datetime.datetime.strptime('03/01/2015', '%m/%d/%Y').date() \\\n",
    "                - datetime.datetime.strptime(date, '%m/%d/%Y').date()).days\n",
    "        except:\n",
    "            return (datetime.datetime.strptime('03/01/2015', '%m/%d/%Y').date() \\\n",
    "                - datetime.datetime.strptime(date, '%Y-%m-%d').date()).days\n",
    "\n",
    "\n",
    "\n",
    "release_date = downloads[[\"id\"]].dropDuplicates().join(release_date,[\"id\"],\"left\").fillna(\"no_date\",[\"release_date\"])\n",
    "sqlCtx.registerDataFrameAsTable(release_date, \"release_date\")\n",
    "sqlCtx.registerFunction(\"get_days\", get_days,returnType=IntegerType())\n",
    "temp_date = sqlCtx.sql(\"SELECT id\\\n",
    ", get_days(release_date, id) AS days_since_release \\\n",
    " FROM release_date\")\n",
    "\n",
    "predictors = predictors.join(temp_date,[\"id\"],\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ratings generation\n",
    "sqlCtx.registerDataFrameAsTable(ratings, \"ratings\")\n",
    "temp_ratings = sqlCtx.sql(\"SELECT id,name,category \\\n",
    ", CAST(1.0*start1/(start1+star2+star3+star4+star5) AS float) AS star1 \\\n",
    ", CAST(1.0*star2/(start1+star2+star3+star4+star5) AS float) AS star2 \\\n",
    ", CAST(1.0*star3/(start1+star2+star3+star4+star5) AS float) AS star3 \\\n",
    ", CAST(1.0*star4/(start1+star2+star3+star4+star5) AS float) AS star4 \\\n",
    ", CAST(1.0*star5/(start1+star2+star3+star4+star5) AS float) AS star5 \\\n",
    ", (start1+star2+star3+star4+star5) AS num_ratings \\\n",
    " FROM ratings\")\n",
    "\n",
    "predictors = predictors.join(temp_ratings,[\"id\",\"name\",\"category\"],\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Categories\n",
    "list_categories = [ x.category.replace(\" \",\"_\") for x in sqlCtx.sql(\"SELECT category \\\n",
    " FROM downloads\\\n",
    " group by category \\\n",
    " \").collect()]\n",
    "for cat in list_categories[:-1]:\n",
    "    sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "    predictors=sqlCtx.sql('''SELECT *, CASE WHEN (category = \"'''+cat+'''\") THEN 1 ELSE 0 END AS '''+cat+''' FROM predictors''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Device\n",
    "sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "predictors=sqlCtx.sql('''SELECT *, CASE WHEN (device = \"iphone\") THEN 1 ELSE 0 END AS iphone FROM predictors''')\n",
    "sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "#predictors=sqlCtx.sql('''SELECT *, CASE WHEN (device = \"ipad\") THEN 1 ELSE 0 END AS ipad FROM predictors''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lang = ['ja','zh-cn','ko','en','other']\n",
    "def get_language(x):\n",
    "    try:\n",
    "        detected = langdetect.detect_langs(x.decode('utf8','ignore'))[0]\n",
    "        if detected.prob < 0.7:\n",
    "            return \"other\"\n",
    "        elif  detected.lang in lang:\n",
    "            return detected.lang\n",
    "        else:\n",
    "            return \"other\"\n",
    "    except:\n",
    "        return \"other\"\n",
    "sqlCtx.registerFunction(\"get_language\", get_language,returnType=StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Language of the title\n",
    "for l in lang:\n",
    "    sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "    predictors=sqlCtx.sql('''SELECT *, CASE WHEN (get_language(name) = \"'''+l+'''\") THEN 1 \\\n",
    "    ELSE 0 END AS '''+l.replace(\"-\",\"_\")+''' FROM predictors''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reviews \n",
    "#escape is used in case some asshole used - or [space] anywhere\n",
    "def escape(text):\n",
    "    return text.replace(\" \",\"_\").replace(\"-\",\"_\")\n",
    "# number of reviews\n",
    "def get_recentReviews(date):\n",
    "    return int((datetime.datetime.strptime('04/01/2015', '%m/%d/%Y').date() \\\n",
    "            - datetime.datetime.strptime(date, '%Y-%m-%d').date()).days >=0)\n",
    "\n",
    "#First step\n",
    "list_countries =['United_States', 'France', 'Japan', 'Spain', 'United_Kingdom','Saudi_Arabia', 'Germany'\\\n",
    "     , 'Hong_Kong', 'Switzerland', 'Turkey','Netherlands', 'Australia', 'Norway', 'Sweden', 'China', 'Canada'\\\n",
    "     ,'Tanzania', 'Denmark', 'South_Korea', 'Italy', 'Finland', 'Taiwan','Russia', 'Philippines', 'Slovenia'\\\n",
    "     , 'Ireland', 'Belgium', 'Mexico','Austria', 'India', 'Brazil', 'Benin', 'New_Zealand','United_Arab_Emirates'\\\n",
    "     , 'Ukraine', 'Poland', 'Israel', 'Portugal','Tunisia', 'Mali', 'Slovakia', 'Zimbabwe', 'Thailand', 'Panama'\\\n",
    "     ,'Indonesia', 'Singapore', 'Greece', 'Senegal', 'Nicaragua','Hungary', 'Czech_Republic', 'Macedonia', 'Chile'\\\n",
    "     , 'Uruguay','Malaysia', 'Algeria', 'Nepal', 'Mauritania', 'Croatia']\n",
    "\n",
    "cmd = '''review_rdd = reviews\\\n",
    ".map(lambda x : (x.id , Row(id = x.id , avg_review = x.rating \\\n",
    ", recent_review = get_recentReviews(x.date), nb_review = 1\\\n",
    ",version = set([x.version])'''\n",
    "cmd+=\",country = set([x.country])\"\n",
    "#for c in list_countries:\n",
    "#    cmd+=\",\"+c+''' = int( escape(x.country) == \"'''+c+'''\")'''\n",
    "for i in range(1,6):\n",
    "    cmd+=\",review_rating_\"+str(i)+\" = int(x.rating == \"+str(i)+\")  \"\n",
    "cmd+=\")))\"\n",
    "exec cmd\n",
    "\n",
    "#Group step\n",
    "cmd = '''review_rdd = review_rdd.reduceByKey(lambda x1 ,x2 : Row(\\\n",
    " avg_review = x1.avg_review + x2.avg_review\\\n",
    "   ,recent_review = x1.recent_review + x2.recent_review, nb_review = x1.nb_review + x2.nb_review'''\n",
    "#for c in list_countries:\n",
    "#    cmd+=\" , \"+c+\" = x1.\"+c+\" + x2.\"+c\n",
    "cmd+=\", country = x1.country.union(x2.country)\"\n",
    "cmd+=\", version = x1.version.union(x2.version)\"\n",
    "for i in range(1,6):\n",
    "    cmd+=\", review_rating_\"+str(i)+\" = x1.review_rating_\"+str(i)+\" + x2.review_rating_\"+str(i)+\" \"\n",
    "cmd+=\"))\"\n",
    "exec cmd\n",
    "\n",
    "# Clean the grouped rdd\n",
    "cmd = '''review_rdd = review_rdd.map(lambda (id , x) : [ id \\\n",
    ",  1.0*x.avg_review /  x.nb_review\\\n",
    "   , x.recent_review,  x.nb_review'''\n",
    "#for c in list_countries:\n",
    "#    cmd+=\" , 1.0* x.\"+c+\"/ x.nb_review\"\n",
    "cmd+=\",  escape(x.country.pop())\"\n",
    "cmd+=\",  len(x.version) \" # -1 if want number of updates\n",
    "for i in range(1,6):\n",
    "    cmd+=\", 1.0*x.review_rating_\"+str(i)+\" / x.nb_review\"\n",
    "cmd+=\"])\"\n",
    "exec cmd\n",
    "\n",
    "#Put back into dataframe\n",
    "grp_reviews = sqlCtx.createDataFrame(review_rdd, [\"id\",\"avg_review\"\\\n",
    "      ,\"recent_reviews\",\"nb_reviews\",\"country\",\"versions\"]+[\"review_rating_\"+str(i) for i in range(1,6)])\n",
    "    #,\"recent_reviews\",\"nb_reviews\"] + list_countries + [\"versions\"]+[\"review_rating_\"+str(i) for i in range(1,6)])\n",
    "    \n",
    "#Join with predictors \n",
    "predictors = predictors.join(grp_reviews,[\"id\"],\"left\").fillna(\"no_country\",[\"country\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate DL Projection\n",
    "sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "derived_feats = sqlCtx.sql(\"SELECT id, device\\\n",
    "    ,CAST(LOG(7*download_sum+cumulative_downloads_2015_02) AS float ) AS dl_projection \\\n",
    "    ,CAST((1000000.0*num_ratings/(cumulative_downloads_2015_02 + download_sum))AS float )  AS ratings_per_downloads \\\n",
    "    ,CAST((1.0*num_ratings/(days_since_release+60))AS float )  AS ratings_per_day \\\n",
    "    ,CAST((1000000.0*nb_reviews/download_sum)AS float )  AS review_per_downloads \\\n",
    "    ,CAST((1.0*recent_reviews/nb_reviews)AS float )  AS review_recent_over_old \\\n",
    "    ,CAST((1.0*cumulative_downloads_2015_02/(days_since_release+1))AS float )  AS downloads_per_day_before \\\n",
    "    FROM predictors\")\n",
    "predictors = predictors.join(derived_feats,[\"id\",\"device\"],\"left\")\n",
    "\n",
    "#we could group by continent or use the market potential\n",
    "list_countries+=[\"no_country\"]\n",
    "for co in list_countries:\n",
    "    sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "    predictors=sqlCtx.sql('''SELECT *, CASE WHEN (country = \"'''+co+'''\") THEN 1 ELSE 0 END AS '''+co+''' FROM predictors''')\n",
    "def get_market(country,device):\n",
    "    #http://blog.nelso.com/2010/06/iphone-os-penetration-by-country.html\n",
    "    if device == \"iphone\":\n",
    "        return {\n",
    "            \"United_States\" : 10683403,\n",
    "            \"France\" : 2248817,\n",
    "            \"Japan\" : 1378903,\n",
    "            \"Spain\" : 377346,\n",
    "            \"United_Kingdom\" : 2551128,\n",
    "            \"Germany\" : 1117716,\n",
    "            \"Hong_Kong\" : 299720,\n",
    "            \"Switzerland\" : 399364,\n",
    "            \"Netherlands\" : 372539,\n",
    "            \"Australia\" : 1207428,\n",
    "            \"Norway\" : 154218,\n",
    "            \"Sweden\" : 281622,\n",
    "            \"China\" : 725358,\n",
    "            \"Canada\" : 919074,\n",
    "            \"Denmark\" : 151426,\n",
    "            \"Italy\" : 648718,\n",
    "            \"Taiwan\" : 174226,\n",
    "            \"Mexico\" : 215326,\n",
    "            \"Austria\" : 156322,\n",
    "            \"Brazil\" : 219339,\n",
    "            \"Poland\" : 72114,\n",
    "            \"Singapore\" : 402922,\n",
    "            \"Hungary\" : 33219,\n",
    "            \"Czech_Republic\" : 42753,\n",
    "            'South_Korea': 530235,\n",
    "            \"Russia\" :246421   \n",
    "    }.get(country, 15000) \n",
    "    else:\n",
    "        return{\n",
    "            \"United_States\" : 223269,\n",
    "            \"France\" : 2724,\n",
    "            \"Japan\" : 2293,\n",
    "            \"Spain\" : 1494,\n",
    "            \"United_Kingdom\" : 4197,\n",
    "            \"Germany\" : 3403,\n",
    "            \"Hong_Kong\" : 2306,\n",
    "            \"Switzerland\" : 1698,\n",
    "            \"Netherlands\" : 2554,\n",
    "            \"Australia\" : 1400,\n",
    "            \"Norway\" : 1333,\n",
    "            \"Sweden\" : 1188,\n",
    "            \"China\" : 12516,\n",
    "            \"Canada\" : 6275,\n",
    "            \"Denmark\" : 753,\n",
    "            \"Italy\" : 1370,\n",
    "            \"Taiwan\" : 1356,\n",
    "            \"Mexico\" : 3380,\n",
    "            \"Austria\" : 493,\n",
    "            \"Brazil\" : 2014,\n",
    "            \"Poland\" : 324,\n",
    "            \"Singapore\" : 1453,\n",
    "            \"Hungary\" : 211,\n",
    "            \"Czech_Republic\" : 203,\n",
    "            'South_Korea': 2416,\n",
    "            \"Russia\" :2183\n",
    "        }.get(country, 100) \n",
    "    \n",
    "    \n",
    "    return 1\n",
    "sqlCtx.registerFunction(\"get_market\", get_market,returnType=IntegerType())\n",
    "\n",
    "sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "predictors=sqlCtx.sql('''SELECT *, get_market(country,device) AS market_size FROM predictors''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlCtx.registerDataFrameAsTable(rating_country, \"rating_country\")\n",
    "rating_country = sqlCtx.sql('SELECT id\\\n",
    ", SUM(star1) AS total_star1\\\n",
    ", SUM(star2) AS total_star2\\\n",
    ", SUM(star3) AS total_star3\\\n",
    ", SUM(star4) AS total_star4\\\n",
    ", SUM(star5) AS total_star5\\\n",
    ", COUNT(1) AS nb_countries FROM rating_country GROUP BY id')\n",
    "sqlCtx.registerDataFrameAsTable(rating_country, \"rating_country\")\n",
    "\n",
    "predictors = predictors.join(rating_country,[\"id\"],how='left_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''#Taking the log\n",
    "to_log = [\"download_sum\",\"cumulative_downloads_2015_02\",\"dl_projection\"]\n",
    "def get_log(x) : \n",
    "    try:\n",
    "        return math.log(x)\n",
    "    except:\n",
    "        return 0\n",
    "sqlCtx.registerFunction(\"get_log\", get_log,returnType=FloatType())\n",
    "\n",
    "for name in to_log:\n",
    "    cmd = 'SELECT '+','.join(filter(lambda x : x not in  to_log ,predictors.columns))\n",
    "    for t in to_log:\n",
    "        cmd+= \" , get_log(\"+t+\") AS \"+t\n",
    "    cmd+=\" FROM predictors\"\n",
    "    sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "    predictors=sqlCtx.sql(cmd)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = predictors.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors.toPandas().to_csv(\"predictors_spark.csv\", sep='\\t',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#to delete\n",
    "predictors = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true',inferSchema='true') \\\n",
    "    .load('predictors_spark.csv').drop('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### ML  pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictor_names = predictors.rdd.top(1)[0].asDict().keys()\n",
    "predictor_names.remove(\"name\")\n",
    "predictor_names.remove(\"id\")\n",
    "predictor_names.remove(\"category\")\n",
    "predictor_names.remove(\"device\")\n",
    "predictor_names.remove(\"country\")\n",
    "\n",
    "predictors_labelPoints = predictors.rdd\\\n",
    ".map(lambda x: ((x.id,x.device) ,x))\\\n",
    ".join(output.rdd.map(lambda x: ((x.id,x.device) ,x.cumulative_downloads_2016_02)))\\\n",
    ".map(lambda x : (x[0] , LabeledPoint(x[1][1], [x[1][0].asDict()[col] for col in predictor_names])))\n",
    "\n",
    "predictors_labelPoints.cache()\n",
    "predictors_labelPoints.top(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_preformance_old(labelsAndPredictions):\n",
    "    total = labelsAndPredictions.count()\n",
    "    pred_threshold = labelsAndPredictions.takeOrdered(int(0.01*total),lambda x:-x.pred)[-1].pred\n",
    "    true_threshold = labelsAndPredictions.takeOrdered(int(0.01*total),lambda x:-x.true)[-1].true\n",
    "    pred_top = labelsAndPredictions.filter(lambda x: x.pred >= pred_threshold ).map(lambda x : ((x.id,x.device),1))\n",
    "    true_top = labelsAndPredictions.filter(lambda x: x.true >= true_threshold ).map(lambda x : ((x.id,x.device),1)) \n",
    "    \n",
    "    if pred_top.count() > total*0.01 :  print \"Error \"+str(pred_top.count()) + \"   \"+str(total*0.01)\n",
    "    \n",
    "    return 10000.0*pred_top.join(true_top).count()/total\n",
    "\n",
    "\n",
    "def get_preformance(labelsAndPredictions):\n",
    "    total = labelsAndPredictions.count()\n",
    "    pred_top = labelsAndPredictions.takeOrdered(int(0.01*total),lambda x:-x.pred).map(lambda x : ((x.id,x.device),1))\n",
    "    true_top = labelsAndPredictions.takeOrdered(int(0.01*total),lambda x:-x.true).map(lambda x : ((x.id,x.device),1))\n",
    "    \n",
    "    if pred_top.count() > total*0.01 : return print \"Error \"+str(pred_top.count()) + \"   \"+str(total*0.01)\n",
    "    \n",
    "    return 10000.0*pred_top.join(true_top).count()/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 5\n",
    "predictors_cvSplit = predictors_labelPoints.randomSplit([1.0/K] * K)\n",
    "\n",
    "for fold in range(K):\n",
    "    test_rdd = predictors_cvSplit[fold]\n",
    "    train_rdd = sc.union([predictors_cvSplit[i]  for i in range(K) if (i!= fold)])\n",
    "    \n",
    "    model = GradientBoostedTrees.trainRegressor(train_rdd.map(lambda x:x[1])\\\n",
    "                    ,categoricalFeaturesInfo={predictor_names.index(\"categorical_country\"): 1+len(list_countries)\\\n",
    "                              ,predictor_names.index(\"categorical_category\"):1+len(list_categories)}\\\n",
    "                                                , numIterations=100\\\n",
    "                                               ,maxDepth=1\\\n",
    "                                               ,learningRate = 0.1\\\n",
    "                                               ,maxBins = 100)\n",
    "    \n",
    "    prediction = model.predict(test_rdd.map(lambda x:x[1].features))\n",
    "    labelsAndPredictions = test_rdd.map(lambda x: x[1].label)\\\n",
    "    .zip(prediction)\\\n",
    "    .zip(test_rdd.map(lambda x : x[0]))\\\n",
    "    .map(lambda (tuple_true_pred , tuple_id) :\\\n",
    "         Row(id = tuple_id[0], device = tuple_id[1] , pred = tuple_true_pred[1],true = tuple_true_pred[0]))\n",
    "    #print get_preformance(labelsAndPredictions)\n",
    "    print get_preformance_old(labelsAndPredictions)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictor_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "   \n",
    "    \n",
    "\n",
    "print list(predictors.columns.values)\n",
    "np.random.seed(1)\n",
    "kf = KFold(len(predictors), n_folds=K)\n",
    "new_top = []\n",
    "for train, test in kf:\n",
    "\n",
    "    old_mod=linear_model.LinearRegression(fit_intercept=False).fit(predictors.as_matrix()[train,4:12], output.as_matrix()[train,5])\n",
    "    y_pred =  old_mod.predict(predictors.as_matrix()[test,4:12])\n",
    "\n",
    "\n",
    "    #model\n",
    "    #mod=linear_model.LinearRegression(fit_intercept=False)\\ \n",
    "    #mod=linear_model.Lasso(alpha=100,fit_intercept=False)\\\n",
    "    #mod = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0, loss='ls')\\ \n",
    "    #mod= RandomForestRegressor(max_features = 1.0/3.0,n_estimators = 100)\\\n",
    "    #mod= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100)\\\n",
    "    \n",
    "    #mod = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01,max_depth=2, random_state=0, loss='ls')\\\n",
    "    #.fit(predictors.as_matrix()[train,4:], output.as_matrix()[train,5]) \n",
    "\n",
    "    #y_pred =  mod.predict(predictors.as_matrix()[test,4:])\n",
    "    new_top.append(metric(y_pred,output.as_matrix()[test,5]))\n",
    "\n",
    "print 1.0*sum(new_top)/len(new_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictor_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LabeledPoint(1,[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use something like this in the classification step\n",
    "def even(x): return x % 2 == 0\n",
    "def odd(x): return not even(x)\n",
    "rdd = sc.parallelize(range(20))\n",
    "\n",
    "rdd_odd, rdd_even = (rdd.filter(f) for f in (odd, even))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "predictors_pd = pd.read_csv('predictors_spark.csv',sep='\\t').drop('Unnamed: 0', 1).fillna(0).sort_values(by=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors_pd = predictors.toPandas().sort_values(by=\"id\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors_pd = predictors_pd.sort_values([\"id\",\"device\"])\n",
    "output_pd = pd.merge(predictors_pd,output.toPandas(),on = [\"id\",\"device\"],how=\"left\")\\\n",
    "[[\"id\",\"device\", \"cumulative_downloads_2016_02\"]]\n",
    "\n",
    "#predictors_pd[\"id\"]=(predictors_pd[\"id\"]).astype(str)+predictors_pd[\"device\"]\n",
    "\n",
    "numerical_predictors = list(predictors_pd.columns.values)\n",
    "numerical_predictors.remove(\"id\");\n",
    "numerical_predictors.remove(\"device\");\n",
    "numerical_predictors.remove(\"category\");\n",
    "numerical_predictors.remove(\"name\");\n",
    "numerical_predictors.remove(\"country\");\n",
    "\n",
    "\n",
    "#numerical_predictors.remove(\"downloads_std\");\n",
    "#numerical_predictors.remove(\"nb_missing\");\n",
    "#numerical_predictors.remove(\"max_step\");\n",
    "\n",
    "predictors_id = predictors_pd[[\"id\",\"device\"]].as_matrix()\n",
    "\n",
    "predictors_matrix = predictors_pd[numerical_predictors].as_matrix()\n",
    "\n",
    "\n",
    "#output_pd[\"id\"]=(output_pd[\"id\"]).astype(str)+output_pd[\"device\"]\n",
    "output_matrix = output_pd[\"cumulative_downloads_2016_02\"].as_matrix()\n",
    "\n",
    "#predictors_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is the metric we use to determine our performance\n",
    "def metric(y_pred,y_test,percent=1):\n",
    "    top = int(len(y_pred)/100.0*percent)\n",
    "    return (len(set([i[0] for i in sorted(enumerate(y_pred), key=lambda x:x[1],reverse=True)][0:top])\n",
    "       .intersection([i[0] for i in sorted(enumerate(y_test), key=lambda x:x[1],reverse=True)][0:top])\n",
    "               ))/(percent/100.0)/top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numerical_perdictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in [0.1,1.0,10,100,1000]:\n",
    "    for e in [0.001,0.01,0.1,1.0,10,100,1000]:\n",
    "        K = 5\n",
    "        #print list(numerical_predictors)\n",
    "        np.random.seed(4)\n",
    "        kf = KFold(len(predictors_pd), n_folds=K,shuffle=True)\n",
    "        new_top = []\n",
    "        for train, test in kf:\n",
    "            train_features = predictors_pd[numerical_predictors].as_matrix()[train,:]\n",
    "            train_output = output_pd[\"cumulative_downloads_2016_02\"].as_matrix()[train]\n",
    "\n",
    "            test_features = predictors_pd[numerical_predictors].as_matrix()[test,:]\n",
    "            test_output = output_pd[\"cumulative_downloads_2016_02\"].as_matrix()[test]\n",
    "\n",
    "\n",
    "            #mod = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=2, random_state=0, loss='ls')\n",
    "            #mod = linear_model.Lasso(alpha=100)\n",
    "            mod = SVR(kernel='rbf', degree=3, gamma='auto', coef0=0.0, tol=0.001, C=1.0\\\n",
    "                      , epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=100)\n",
    "            mod.fit(train_features, train_output )\n",
    "            y_pred = mod.predict(test_features)\n",
    "\n",
    "            err = (metric(test_output,y_pred))\n",
    "            new_top.append(err)\n",
    "            #print err\n",
    "\n",
    "        print \"SVR for c= \\t\"+str(c)+\" and e =\\t \"+str(e)+\"  \\t -->  \"+str(1.0*sum(new_top)/len(new_top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "K = 5\n",
    "\n",
    "np.random.seed(4)\n",
    "kf = KFold(len(predictors_pd), n_folds=K,shuffle=True)\n",
    "new_top = []\n",
    "for train, test in kf:\n",
    "    train_features = predictors_pd[numerical_predictors].as_matrix()[train,:]\n",
    "    train_output = output_pd[\"cumulative_downloads_2016_02\"].as_matrix()[train]\n",
    "\n",
    "    #train_order =(range(len(train)))\n",
    "    #train_order.sort(key=lambda x: predictors_pd[[\"id\"]].iloc(x))\n",
    "    \n",
    "    #train_features = train_features[train_order].astype(float)\n",
    "    #train_output = train_output[train_order]\n",
    "    \n",
    "    train_features_1 = train_features[:int(3.0*len(test)/5)]\n",
    "    train_features_2 = train_features[int(3.0*len(test)/5):]\n",
    "    train_output_1 = train_output[:int(3.0*len(test)/5)]\n",
    "    train_output_2 = train_output[int(3.0*len(test)/5):]\n",
    "    \n",
    "    test_features = predictors_pd[numerical_predictors].as_matrix()[test,:]\n",
    "    test_output = output_pd[\"cumulative_downloads_2016_02\"].as_matrix()[test]\n",
    "    \n",
    "    #test_order =(range(len(test)))\n",
    "    #test_order.sort(key=lambda x: predictors_pd[[\"id\"]].iloc(x))\n",
    "    \n",
    "    #test_features = test_features[test_order].astype(float)\n",
    "    #test_output = test_output[test_order]\n",
    "    \n",
    "    test_features_1 = test_features[:int(4.0*len(test)/5)]\n",
    "    test_features_2 = test_features[int(4.0*len(test)/5):]\n",
    "    test_output_1 = test_output[:int(4.0*len(test)/5)]\n",
    "    test_output_2 = test_output[int(4.0*len(test)/5):]\n",
    "    \n",
    "\n",
    "    mod_1 = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01,max_depth=2, random_state=0, loss='ls')\\\n",
    "    .fit(train_features_1, train_output_1 )\n",
    "    mod_2 = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01,max_depth=2, random_state=0, loss='ls')\\\n",
    "    .fit(train_features_2, train_output_2 )\n",
    "\n",
    "    mod = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01,max_depth=2, random_state=0, loss='ls')\\\n",
    "    .fit(train_features, train_output )\n",
    "    \n",
    "    y_pred_1 = mod_1.predict(test_features_1)\n",
    "    y_pred_2 = mod_2.predict(test_features_2)\n",
    "\n",
    "    \n",
    "    y_pred = np.concatenate((y_pred_1,y_pred_2))\n",
    "    \n",
    "    y_pred_old = mod.predict(test_features)\n",
    "    \n",
    "    err = (metric(test_output,y_pred))\n",
    "    new_top.append(err)\n",
    "    print err\n",
    "    print \"old \"+str(metric(test_output,y_pred_old))\n",
    "\n",
    "print 1.0*sum(new_top)/len(new_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split in 2 64.3185961284\n",
    "No split sorted 64.6278585376\n",
    "no split unsorted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for s in range(100):\n",
    "    np.random.seed(s)\n",
    "    K = 5\n",
    "\n",
    "    top_percent_classif = 20\n",
    "\n",
    "\n",
    "    kf = KFold(len(predictors_pd), n_folds=K,shuffle=True)\n",
    "    new_top = []\n",
    "    top_10 = []\n",
    "    for train, test in kf:\n",
    "        train_features = predictors_pd[numerical_predictors].as_matrix()[train,:]\n",
    "        train_output = output_pd[[\"cumulative_downloads_2016_02\"]].as_matrix()[train,0]\n",
    "        test_features = predictors_pd[numerical_predictors].as_matrix()[test,:]\n",
    "        test_output = output_pd[[\"cumulative_downloads_2016_02\"]].as_matrix()[test,0]\n",
    "\n",
    "        '''true_mask = test_output > sorted(test_output,reverse = True)[int(1.0/100*len(test))]\n",
    "\n",
    "        #Classifications\n",
    "        mod_class= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100)   \n",
    "        mod_class.fit(train_features, train_output)\n",
    "        y_pred =  mod_class.predict(test_features)\n",
    "        mask_1 = y_pred > (sorted(y_pred,reverse = True))[int(top_percent_classif*1.0/100*len(test))]\n",
    "\n",
    "        mod_class = linear_model.Lasso(alpha=100)\n",
    "        mod_class.fit(train_features, train_output)\n",
    "        y_pred =  mod_class.predict(test_features)\n",
    "        mask_2 = y_pred > (sorted(y_pred,reverse = True))[int(top_percent_classif*1.0/100*len(test))]\n",
    "\n",
    "        mask = mask_1*mask_2\n",
    "\n",
    "\n",
    "        print \"top classif 1: \"+str(100.0*sum(true_mask*mask_1)/sum(true_mask))\n",
    "        print \"top classif 2: \"+str(100.0*sum(true_mask*mask_2)/sum(true_mask))\n",
    "        print \"top classif inter: \"+str(100.0*sum(mask_1*mask_2)/sum(mask_1))'''\n",
    "        mask = np.ones(len(test))\n",
    "\n",
    "        threshold_20 = sorted(train_output,reverse = True)[int(len(train)/5)]\n",
    "        threshold_10 = sorted(train_output,reverse = True)[int(len(train)/10)]\n",
    "        threshold_5 = sorted(train_output,reverse = True)[int(len(train)/20)]\n",
    "        threshold_1 = sorted(train_output,reverse = True)[int(len(train)/20)]\n",
    "        threshold_0 = 0\n",
    "\n",
    "        relevance =(range(len(train)))\n",
    "        relevance.sort(key=lambda x: (train_output[x]))\n",
    "        relevance = (np.asarray(relevance)-int(99.0*len(train)/100))\n",
    "        relevance[relevance<0] = 0\n",
    "        \n",
    "        mod_train = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01,max_depth=2, random_state=0, loss='ls')\\\n",
    "        .fit(train_features,np.log(train_output))\n",
    "        train_pred =  mod_top1.predict(train_features)\n",
    "        \n",
    "        pred_train_mask = train_pred > sorted(train_pred,reverse = True)[int(1.0/100*len(train))]\n",
    "        true_train_mask = train_output > sorted(train_output,reverse = True)[int(1.0/100*len(train))]\n",
    "        hard_class = np.logical_xor(pred_train_mask,true_train_mask)\n",
    "        \n",
    "\n",
    "        weights = 1*(train_output >  threshold_0).astype(int)\\\n",
    "        +1*(train_output >  threshold_20).astype(int)\\\n",
    "        +0*(train_output >  threshold_10).astype(int)\\\n",
    "        +0*(train_output >  threshold_5).astype(int)\\\n",
    "        + 1*(train_output >  threshold_1).astype(int)\\\n",
    "        + 0*relevance\\\n",
    "        + 1*hard_class\n",
    "\n",
    "        mod_top1 = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01,max_depth=2, random_state=0, loss='ls')\\\n",
    "        .fit(train_features,np.log(train_output) ,sample_weight \\\n",
    "        =(weights)*1.0/sum(weights))\n",
    "\n",
    "        y_pred_2 =  mod_top1.predict(test_features)*mask\n",
    "        estimation_error = metric(y_pred_2,test_output)\n",
    "        new_top.append(estimation_error)\n",
    "        print estimation_error\n",
    "    print \"____\"\n",
    "    print \"Top1% with classif1 : \" + str(1.0*sum(new_top)/len(new_top))\n",
    "    print \"____\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "64.0625\n",
    "67.1875\n",
    "70.3125\n",
    "64.0625\n",
    "71.875\n",
    "Top10%              : 99.6875\n",
    "Top1% with classif1 : 67.5\n",
    "    \n",
    "    \n",
    "    20 + 1\n",
    "65.625\n",
    "67.1875\n",
    "70.3125\n",
    "64.0625\n",
    "71.875\n",
    "Top10%              : 91.25\n",
    "Top1% with classif1 : 67.8125    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(estimate_top1)\n",
    "print len(test)\n",
    "print len(output_test_top_1_precent)\n",
    "estimate_top1.merge(output_test_top_1_precent, how='inner', on=[\"id\",\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(estimate_top1.merge(output_test_top_1_precent, how='inner', on=[\"id\",\"device\"]))\n",
    "print 46.0/64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metric(y_pred_2,test_output)\n",
    "1.0*len(inter)/len(lst)\n",
    "\n",
    "percent = 1\n",
    "top = int(len(y_pred_2)/100.0*percent)\n",
    "print (len(set([i[0] for i in sorted(enumerate(y_pred_2), key=lambda x:x[1],reverse=True)][0:top])\n",
    "   .intersection([i[0] for i in sorted(enumerate(test_output), key=lambda x:x[1],reverse=True)][0:top])\n",
    "           ))*1.0/(percent/100.0)/top\n",
    "\n",
    "print len(set([i[0] for i in sorted(enumerate(y_pred_2), key=lambda x:x[1],reverse=True)][0:top]))\n",
    "print len([i[0] for i in sorted(enumerate(test_output), key=lambda x:x[1],reverse=True)][0:top])\n",
    "print (len(set([i[0] for i in sorted(enumerate(y_pred_2), key=lambda x:x[1],reverse=True)][0:top])\n",
    "   .intersection([i[0] for i in sorted(enumerate(test_output), key=lambda x:x[1],reverse=True)][0:top])\n",
    "           ))/64.0\n",
    "print (1.0/(1.0/100.0)/len(y_pred_2)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inter = []\n",
    "for x in [str(y[0])+str(y[1]) for y in lst_true]:\n",
    "    if x in [str(y[0])+str(y[1]) for y in lst]:\n",
    "        inter.append( x)\n",
    "print len(inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thres = sorted(y_pred_2,reverse=True)[int(len(test)*1.0/100)]\n",
    "lst= []\n",
    "for x in enumerate(y_pred_2):\n",
    "    if x[1]>thres:\n",
    "        lst.append( predictors_id[test[x[0]]])\n",
    "        \n",
    "thres = sorted(test_output,reverse=True)[int(len(test)*1.0/100)]\n",
    "lst_true= []\n",
    "for x in enumerate(test_output):\n",
    "    if x[1]>thres:\n",
    "        lst_true.append( predictors_id[test[x[0]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len((test_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for nb_tree in [10,50,100,500,1000,5000,10000]:\n",
    "    np.random.seed(1)\n",
    "    K = 5\n",
    "\n",
    "    top_percent_classif = 20\n",
    "\n",
    "\n",
    "\n",
    "    kf = KFold(len(predictors_pd), n_folds=K,shuffle=True)\n",
    "    new_top = []\n",
    "    top_10 = []\n",
    "    for train, test in kf:\n",
    "\n",
    "        mod_class10= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100)\\\n",
    "        .fit(predictors_pd[numerical_predictors].as_matrix()[train,:], output_pd[[\"cumulative_downloads_2016_02\"]].as_matrix()[train,0])\n",
    "\n",
    "        y_pred =  mod_class10.predict(predictors_pd[numerical_predictors].as_matrix()[test,:])\n",
    "\n",
    "        #estimate of the top 10% of the test set\n",
    "        estimate_class10= predictors_pd.iloc[test].copy()\n",
    "        estimate_class10[\"firstEstimate\"] = y_pred\n",
    "        estimate_class10 = estimate_class10.sort_values(by= \"firstEstimate\",ascending = False).iloc[0:int(1.0*top_percent_classif/100.0*len(estimate_class10))]\n",
    "        estimate_class10 = estimate_class10.drop(\"firstEstimate\",1)\n",
    "        #estimate_class10 = estimate_class10.sort_values(by= \"daily_avg\",ascending = False).iloc[0:int(1.0*top_percent_classif/100.0*len(estimate_class10))]\n",
    "\n",
    "\n",
    "        #top 10% of the trainning set\n",
    "        output_train_top_10_precent = output_pd.iloc[train].copy().sort_values(by= 'cumulative_downloads_2016_02',ascending = False).iloc[0:int(1.0*top_percent_classif/100.0*len(output_pd.iloc[train]))]\n",
    "        predictor_train_top_10_precent = output_train_top_10_precent.merge(predictors_pd, how='left', on=[\"id\",\"device\"]).copy()\n",
    "        predictor_train_top_10_precent = predictor_train_top_10_precent.drop('cumulative_downloads_2016_02',1)\n",
    "        #predictor_train_top_10_precent = predictor_train_top_10_precent.drop('firstEstimate',1)\n",
    "\n",
    "        #This is the actual top 1% of the test set\n",
    "        output_test_top_1_precent = output_pd.iloc[test].sort_values(by= 'cumulative_downloads_2016_02',ascending = False).iloc[0:int(0.01*len(output_pd.iloc[test]))].copy()\n",
    "\n",
    "\n",
    "        #second model -> Regression on the top obtainned by regression\n",
    "        mod_top1 = GradientBoostingRegressor(n_estimators=nb_tree, learning_rate=0.015,max_depth=2, random_state=0, loss='ls')\\\n",
    "        .fit(predictor_train_top_10_precent[numerical_predictors].as_matrix(), np.log(output_train_top_10_precent[\"cumulative_downloads_2016_02\"].as_matrix()))\n",
    "\n",
    "\n",
    "        y_pred_2 =  mod_top1.predict(estimate_class10[numerical_predictors].as_matrix())\n",
    "\n",
    "        estimate_top1 = estimate_class10.copy()\n",
    "        estimate_top1[\"secondEstimate\"] = y_pred_2\n",
    "        estimate_top1 = estimate_top1.sort_values(by= \"secondEstimate\",ascending = False).iloc[:int(0.01*len(output_pd.iloc[test]))]\n",
    "\n",
    "        #estimate_top1_select = estimate_top1_select.sort_values(by= \"thirdEstimate\",ascending = False).iloc[0:int(0.01*len(output.iloc[test]))]\n",
    "        #estimate_top1_noClassif = estimate_top1_noClassif.sort_values(by= \"noClassifEstimate\",ascending = False).iloc[0:int(0.01*len(output.iloc[test]))]\n",
    "\n",
    "        estimation_error = len(estimate_top1.merge(output_test_top_1_precent, how='inner', on=[\"id\",\"device\"]))*100.0/len(output_test_top_1_precent)\n",
    "        new_top.append(estimation_error)\n",
    "        top_10.append(len(estimate_class10.merge(output_test_top_1_precent, how='inner', on=[\"id\",\"device\"]))*100.0/len(output_test_top_1_precent))\n",
    "\n",
    "    #print \"Old model           : \" + str(1.0*sum(old_top)/len(old_top))\n",
    "    print nb_tree\n",
    "    print \"Top10%              : \" + str(1.0*sum(top_10)/len(top_10))\n",
    "    print \"Top1% with classif1 : \" + str(1.0*sum(new_top)/len(new_top))\n",
    "    print \"______\"\n",
    "    #print \"Top1% with classif2 : \" + str(1.0*sum(new_top_select)/len(new_top_select))\n",
    "    #print \"Top1% no classif1   : \" + str(1.0*sum(new_top_noClassif)/len(new_top_noClassif))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors_pd.drop(list_categories,1).sort_values([\"id\",\"device\"])[3456:3500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old_predictors.drop(list_categories,1).sort_values([\"id\",\"device\"])[3456:3500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors_pd.drop(list_categories,1).sort_values([\"id\",\"device\"])[\"market_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old_predictors = pd.read_csv('predictors.csv').drop('Unnamed: 0', 1).fillna(0)\\\n",
    ".rename(columns={'maxStep': 'max_step', 'minStep': 'min_step','std':'downloads_std','Health and Fitness':'Health_and_Fitness'\\\n",
    "                ,'versions':'num_versions', 'cumulative_downloads_2015-02':'cumulative_downloads_2015_02'\\\n",
    "                ,'Social Networking':'Social_Networking', 'Food and Drink':'Food_and_Drink' ,'Photo and Video':'Photo_and_Video'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old_list = list(old_predictors.columns.values)\n",
    "new_list = (predictors_pd.columns.values)\n",
    "print old_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for n in new_list:\n",
    "    try:\n",
    "        print str(n) + \" : \"+str((sum(np.absolute(predictors_pd.sort_values([\"id\",\"device\"])[n].as_matrix()\\\n",
    "                                                 -old_predictors.sort_values([\"id\",\"device\"])[n].as_matrix())))\\\n",
    "                                /sum(np.absolute(predictors_pd.sort_values([\"id\",\"device\"])[n].as_matrix())))\n",
    "    except:\n",
    "        print \"error on: \"+str(n)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(predictors_pd[predictors_pd.days_since_release != 0].sort_values([\"id\",\"device\"])[\"id\"].as_matrix()\\\n",
    "     ,predictors_pd[predictors_pd.days_since_release != 0].sort_values([\"id\",\"device\"])[\"days_since_release\"].as_matrix()\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = 3\n",
    "\n",
    "train_days = np.power(\\\n",
    "                      np.matlib.repmat(predictors_pd[predictors_pd.days_since_release != 0]\\\n",
    "                              .sort_values([\"id\",\"device\"])[[\"id\"]].as_matrix()*1.0/100000000,1,temp+1),np.array(range(temp+1)))\n",
    "\n",
    "test_days = predictors_pd[predictors_pd.days_since_release != 0]\\\n",
    "                              .sort_values([\"id\",\"device\"])[[\"days_since_release\"]].as_matrix()\n",
    "    \n",
    "day_mod=linear_model.LinearRegression(fit_intercept=False).fit(train_days,test_days )\n",
    "\n",
    "plt.scatter(np.concatenate((train_days[:,1],train_days[:,1])), \\\n",
    "        np.concatenate((test_days[:,0],np.sum(np.dot(train_days,\\\n",
    "          np.diag(day_mod.coef_[0,:])),axis=1))),\\\n",
    "       c=np.concatenate((np.ones(len(train_days)),2*np.ones(len(train_days)))),\\\n",
    "       alpha=.1)\n",
    "\n",
    "print day_mod.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "day_mod=linear_model.LinearRegression(fit_intercept=False).fit(train_days,test_days )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "day_mod.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(np.concatenate((train_days[:,1],train_days[:,1])), \\\n",
    "            np.concatenate((test_days[:,0],np.sum(np.dot(train_days,\\\n",
    "              np.diag(day_mod.coef_[0,:])),axis=1))),\\\n",
    "           c=np.concatenate((np.ones(len(train_days)),2*np.ones(len(train_days)))),\\\n",
    "           alpha=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum(np.dot(train_days,np.diag([  1.73879033e-24,  -8.16149354e-06,   4.19123763e-15,\\\n",
    "         -2.24536991e-16,  -2.24586827e-16])),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.concatenate((test_days[:,0],np.sum(np.dot(train_days,\\\n",
    "              np.diag([  1.73879033e-24,  -8.16149354e-06,   4.19123763e-15,\\\n",
    "         -2.24536991e-16,  -2.24586827e-16])),axis=1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(np.ones(len(train_days)),2*np.ones(len(train_days)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.transpose(test_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.concatenate((train_days[:,1],train_days[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_days[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_days = np.power(\\\n",
    "                      np.matlib.repmat(predictors_pd[predictors_pd.days_since_release != 0]\\\n",
    "                              .sort_values([\"id\",\"device\"])[[\"id\"]].as_matrix()*1.0/100000000,1,temp+1),np.array(range(temp+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "281704574**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors_pd.sort_values([\"id\",\"device\"])[[\"nb_reviews\",\"cumulative_downloads_2015_02\",\"review_per_downloads\",\"review_per_day\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old_predictors.sort_values([\"id\",\"device\"])[[\"num_review\",\"cumulative_downloads_2015-02\",\"review_per_downloads\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mem = pd.read_csv('../iphones_per_country.csv').fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "int(mem[mem.Country == \"Mexico\"][\"iPhone\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mem[\"Country\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "int(mem[mem.Country == \"Mexico\"][\"iPhone\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s=[]\n",
    "for l in list_countries:\n",
    "    try:\n",
    "        print '''\"'''+l+'''\" : '''+str(int(mem[mem.Country == l.replace(\"_\",\" \")][\"iPad\"]))+\",\"\n",
    "    except:\n",
    "        s.append(l)\n",
    "        t+=1\n",
    "print s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors_pd.sort_values([\"id\",\"device\"])[[\"id\"]].as_matrix()[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print predictors_pd.sort_values([\"id\",\"device\"])[[\"id\"]].as_matrix()[[int(32339.0/5),int(2*32339.0/5),int(3*32339.0/5),int(4*32339.0/5)],0]\n",
    "\n",
    "plt.scatter(predictors_pd.sort_values([\"id\",\"device\"])[[\"id\"]].as_matrix()[:,0],\\\n",
    "        output_pd.sort_values([\"id\",\"device\"])[[\"cumulative_downloads_2016_02\"]].as_matrix()[:,0]\\\n",
    "       ,alpha=.1\\\n",
    "           ,c = 2*(output_pd.sort_values([\"id\",\"device\"])[[\"cumulative_downloads_2016_02\"]].as_matrix()[:,0]\\\n",
    "                   >sorted(output_pd['cumulative_downloads_2016_02'])\\\n",
    "            [int(len(output_pd['cumulative_downloads_2016_02'])*0.99)])\\\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for train,test in t:\n",
    "    print test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors_pd.sort_values([\"id\",\"device\"])[[\"id\"]].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "np.seterr(all='raise')\n",
    "\n",
    "labels = []\n",
    "features = []\n",
    "f = open('data/train.txt')\n",
    "for line in f:\n",
    "    # strip off comments\n",
    "    line = line[:line.find('#') - 1]\n",
    "    ls = line.split()\n",
    "    labels.append(int(ls[0]))\n",
    "    features.append([float(x[x.find(':') + 1:]) for x in ls[1:]])\n",
    "f.close()\n",
    "\n",
    "labels = np.asarray(labels, dtype=np.int32)\n",
    "features = np.asarray(features)\n",
    "\n",
    "# to test with gbm\n",
    "np.savetxt('labels.csv', labels, delimiter=',')\n",
    "np.savetxt('features.csv', features, delimiter=',')\n",
    "\n",
    "query = features[:, 0]\n",
    "features = features[:, 1:]\n",
    "\n",
    "gb = GradientBoostingRegressor(loss='ndcg', learning_rate=0.1,\n",
    "                               n_estimators=5, max_depth=4, verbose=2, subsample=0.5,\n",
    "                               random_state=1)\n",
    "print(gb)\n",
    "gb.fit(features, labels, query)\n",
    "\n",
    "joblib.dump(gb, 'gb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import math\n",
    "# import pandas\n",
    "from optparse import OptionParser\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from multiprocessing import Pool\n",
    "from itertools import chain\n",
    "import time\n",
    "\n",
    "class Ensemble:\n",
    "    def __init__(self, rate):\n",
    "        self.trees = []\n",
    "        self.rate = rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.trees)\n",
    "\n",
    "    def add(self, tree):\n",
    "        self.trees.append(tree)\n",
    "\n",
    "    def eval_one(self, object):\n",
    "        return self.eval([object])[0]\n",
    "\n",
    "    def eval(self, objects):\n",
    "        results = np.zeros(len(objects))\n",
    "        for tree in self.trees:\n",
    "            results += tree.predict(objects) * self.rate\n",
    "        return results\n",
    "\n",
    "    def remove(self, number):\n",
    "        self.trees = self.trees[:-number]\n",
    "\n",
    "\n",
    "def groupby(score, query):\n",
    "    result = []\n",
    "    this_query = None\n",
    "    this_list = -1\n",
    "    for s, q in zip(score, query):\n",
    "        if q != this_query:\n",
    "            result.append([])\n",
    "            this_query = q\n",
    "            this_list += 1\n",
    "        result[this_list].append(s)\n",
    "    result = map(np.array, result)\n",
    "    return result\n",
    "\n",
    "\n",
    "def compute_point_dcg(arg):\n",
    "    rel, i = arg\n",
    "    return (2 ** rel - 1) / math.log(i + 2, 2)\n",
    "\n",
    "\n",
    "def compute_point_dcg2(arg):\n",
    "    rel, i = arg\n",
    "    if i == 0:\n",
    "        return rel\n",
    "    else:\n",
    "        return rel / (math.log(1 + i, 2))\n",
    "    return\n",
    "\n",
    "\n",
    "def compute_dcg(array):\n",
    "    dcg = map(compute_point_dcg, zip(array, range(len(array))))\n",
    "    return sum(dcg)\n",
    "\n",
    "\n",
    "def compute_ndcg(page, k=10):\n",
    "    idcg = compute_dcg(np.sort(page)[::-1][:k])\n",
    "    dcg = compute_dcg(page[:k])\n",
    "\n",
    "    if idcg == 0:\n",
    "        return 1\n",
    "\n",
    "    return dcg / idcg\n",
    "\n",
    "\n",
    "def ndcg(prediction, true_score, query, k=10):\n",
    "    true_pages = groupby(true_score, query)\n",
    "    pred_pages = groupby(prediction, query)\n",
    "\n",
    "    total_ndcg = []\n",
    "    for q in range(len(true_pages)):\n",
    "        total_ndcg.append(compute_ndcg(true_pages[q][np.argsort(pred_pages[q])[::-1]], k))\n",
    "    return sum(total_ndcg) / len(total_ndcg)\n",
    "\n",
    "\n",
    "def query_lambdas(page):\n",
    "    true_page, pred_page = page\n",
    "    worst_order = np.argsort(true_page)\n",
    "    true_page = true_page[worst_order]\n",
    "    pred_page = pred_page[worst_order]\n",
    "\n",
    "    page = true_page[np.argsort(pred_page)]\n",
    "    idcg = compute_dcg(np.sort(page)[::-1])\n",
    "    position_score = np.zeros((len(true_page), len(true_page)))\n",
    "\n",
    "    for i in xrange(len(true_page)):\n",
    "        for j in xrange(len(true_page)):\n",
    "            position_score[i, j] = compute_point_dcg((page[i], j))\n",
    "\n",
    "    lambdas = np.zeros(len(true_page))\n",
    "\n",
    "    for i in xrange(len(true_page)):\n",
    "        for j in xrange(len(true_page)):\n",
    "                if page[i] > page[j]:\n",
    "\n",
    "                    delta_dcg = position_score[i][j] - position_score[i][i]\n",
    "                    delta_dcg += position_score[j][i] - position_score[j][j]\n",
    "\n",
    "                    delta_ndcg = abs(delta_dcg / idcg)\n",
    "\n",
    "                    rho = 1 / (1 + math.exp(page[i] - page[j]))\n",
    "                    lam = rho * delta_ndcg\n",
    "\n",
    "                    lambdas[i] -= lam\n",
    "                    lambdas[j] += lam\n",
    "    return lambdas\n",
    "\n",
    "\n",
    "def compute_lambdas(prediction, true_score, query, k=10):\n",
    "    true_pages = groupby(true_score, query)\n",
    "    pred_pages = groupby(prediction, query)\n",
    "\n",
    "    print len(true_pages), \"pages\"\n",
    "\n",
    "    pool = Pool()\n",
    "    lambdas = pool.map(query_lambdas, zip(true_pages, pred_pages))\n",
    "    return list(chain(*lambdas))\n",
    "\n",
    "\n",
    "def mart_responces(prediction, true_score):\n",
    "    return true_score - prediction\n",
    "\n",
    "\n",
    "def learn(features,scores, queries = \"no_query\", n_trees=10, learning_rate=0.1, k=10, verbiose = False):\n",
    "    if verbiose : print \"Loading train file\"\n",
    "    #train = np.loadtxt(train_file, delimiter=\",\", skiprows=1)\n",
    "    #validation = np.loadtxt(validation_file, delimiter=\",\", skiprows=1)\n",
    "\n",
    "    #I should replace this input\n",
    "    \n",
    "    #scores = train[:, 0]\n",
    "    #val_scores = train[:, 0]\n",
    "\n",
    "    #queries = train[:, 1]\n",
    "    #val_queries = validation[:, 1]\n",
    "\n",
    "    #features = train[:, 3:]\n",
    "    #val_features = validation[:, 3:]\n",
    "    \n",
    "    if queries == \"no_query\":\n",
    "        quieries = np.ones(len(scores))\n",
    "\n",
    "    ensemble = Ensemble(learning_rate)\n",
    "\n",
    "    if verbiose : print \"Training starts...\"\n",
    "    model_output = np.array([float(0)] * len(features))\n",
    "    val_output = np.array([float(0)] * len(validation))\n",
    "\n",
    "    # print model_output\n",
    "    # best_validation_score = 0\n",
    "    time.clock()\n",
    "    for i in range(n_trees):\n",
    "        if verbiose :print \" Iteration: \" + str(i + 1)\n",
    "\n",
    "        # Compute psedo responces (lambdas)\n",
    "        # witch act as training label for document\n",
    "        start = time.clock()\n",
    "        if verbiose :print \"  --generating labels\"\n",
    "        #lambdas = compute_lambdas(model_output, scores, queries, k)\n",
    "        lambdas = mart_responces(model_output, scores)\n",
    "        if verbiose :print \"  --done\", str(time.clock() - start) + \"sec\"\n",
    "\n",
    "        # create tree and append it to the model\n",
    "        if verbiose :print \"  --fitting tree\"\n",
    "        start = time.clock()\n",
    "        tree = DecisionTreeRegressor(max_depth=2)\n",
    "        # print \"Distinct lambdas\", set(lambdas)\n",
    "        tree.fit(features, lambdas)\n",
    "\n",
    "        if verbiose :print \"  ---done\", str(time.clock() - start) + \"sec\"\n",
    "        if verbiose :print \"  --adding tree to ensemble\"\n",
    "        ensemble.add(tree)\n",
    "\n",
    "        # update model score\n",
    "        if verbiose :print \"  --generating step prediction\"\n",
    "        prediction = tree.predict(features)\n",
    "        # print \"Distinct answers\", set(prediction)\n",
    "\n",
    "        if verbiose :print \"  --updating full model output\"\n",
    "        model_output += learning_rate * prediction\n",
    "        # print set(model_output)\n",
    "\n",
    "        # train_score\n",
    "        start = time.clock()\n",
    "        if verbiose :print \"  --scoring on train\"\n",
    "        train_score = ndcg(model_output, scores, queries, k)\n",
    "        if verbiose :print \"  --iteration train score \" + str(train_score) + \", took \" + str(time.clock() - start) + \"sec to calculate\"\n",
    "\n",
    "        # validation score\n",
    "        #if verbiose :print \"  --scoring on validation\"\n",
    "        #val_output += learning_rate * tree.predict(val_features)\n",
    "        #val_score = ndcg(val_output, val_scores, val_queries, k)\n",
    "\n",
    "        if verbiose :print \"  --iteration validation score \" + str(val_score)\n",
    "\n",
    "        # if(validation_score > best_validation_score):\n",
    "        #         best_validation_score = validation_score\n",
    "        #         best_model_len = len(ensemble)\n",
    "\n",
    "        # # have we assidently break the celling?\n",
    "        # if (best_validation_score > 0.9):\n",
    "        #     break\n",
    "\n",
    "    # rollback to best\n",
    "    # if len(ensemble) > best_model_len:\n",
    "        # ensemble.remove(len(ensemble) - best_model_len)\n",
    "\n",
    "    # finishing up\n",
    "    if verbiose :print \"final quality evaluation\"\n",
    "    # train_score = compute_ndcg(ensemble.eval(features), scores)\n",
    "    # test_score = compute_ndcg(ensemble.eval(validation), validation_score)\n",
    "\n",
    "    # print \"train %s, test %s\" % (train_score, test_score)\n",
    "    if verbiose :print \"Finished sucessfully.\"\n",
    "    if verbiose :print \"------------------------------------------------\"\n",
    "    return ensemble\n",
    "\n",
    "\n",
    "def evaluate(model, predict, queries = \"no_query\"):\n",
    "    #predict = np.loadtxt(fn, delimiter=\",\", skiprows=1)\n",
    "\n",
    "    #queries = predict[:, 1]\n",
    "    #doc_id  = predict[:, 2] \n",
    "    #features = predict[:, 3:]\n",
    "    #if ids == \"no_id\":\n",
    "    #    ids = np.asarray([\"no_id\"]*len(predict))\n",
    "    if queries == \"no_query\":\n",
    "        queries = np.ones(len(predict))\n",
    "    features=predict\n",
    "\n",
    "    results = model.eval(features)\n",
    "    #writer = csv.writer(open(\"result.csv\"))\n",
    "    #for line in zip(queries, results, doc_id):\n",
    "    #        writer.writerow(line)\n",
    "    #return \"OK\"\n",
    "    return results\n",
    "\n",
    "\n",
    "'''if __name__ == \"__main__\":\n",
    "    parser = OptionParser()\n",
    "    parser.add_option(\"-t\", \"--train\", action=\"store\", type=\"string\", dest=\"train_file\")\n",
    "    parser.add_option(\"-v\", \"--validation\", action=\"store\", type=\"string\", dest=\"val_file\")\n",
    "    parser.add_option(\"-p\", \"--predict\", action=\"store\", type=\"string\", dest=\"predict_file\")\n",
    "    options, args = parser.parse_args()\n",
    "    iterations = 30\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    model = learn(options.train_file, options.val_file, n_trees=200)\n",
    "    evaluate(model, options.predict_file)\n",
    "'''\n",
    "\n",
    "print \"functions loaded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ndcg_predictors = np.concatenate((np.ones((32339,1))\\\n",
    "      ,predictors_pd[[\"id\"]].as_matrix(),predictors_pd[numerical_predictors].as_matrix()),axis=1)\n",
    "ndcg_output = output_pd.sort_values([\"cumulative_downloads_2016_02\"],ascending=False)\n",
    "\n",
    "ndcg_output['true_ranking'] = pd.Series(np.asarray(range(1,32340)), index=ndcg_output.index)\n",
    "ndcg_output = pd.merge(predictors_pd[[\"id\"]],ndcg_output,on= \"id\",how = \"left\")[[\"id\",\"true_ranking\",\"cumulative_downloads_2016_02\"]]\n",
    "\n",
    "#ndcg_predictors = np.concatenate((ndcg_output[[\"true_ranking\"]].as_matrix(), ndcg_predictors),axis=1)\n",
    "ndcg_predictors = np.concatenate((ndcg_output[[\"cumulative_downloads_2016_02\"]].as_matrix(), ndcg_predictors),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lst = range(32339)\n",
    "#random.shuffle(lst)\n",
    "train = lst[:20000]\n",
    "validation = lst[20000:31339]\n",
    "test = lst[31339:]\n",
    "\n",
    "print train[-1]\n",
    "print validation[0]\n",
    "print validation[-1]\n",
    "print test[0]\n",
    "print len(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ndcg_model = learn(train_features,train_output,  n_trees=100, learning_rate=0.1, k=10)\n",
    "predi = evaluate(ndcg_model,test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predi = evaluate(ndcg_model,test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zip_metric(predi,test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#calculate performance\n",
    "def zip_metric(true,pred):\n",
    "    nb_tot = len(true)\n",
    "    pred_thred = sorted(pred, reverse=True)[int(nb_tot*1.0/100)]\n",
    "    true_thred = sorted(true, reverse=True)[int(nb_tot*1.0/100)]\n",
    "    try:\n",
    "        return len([x for x in range(nb_tot) if (pred[x]>pred_thred and true[x]>true_thred)])*1.0/nb_tot*100*100\n",
    "    except:\n",
    "        print \"length of true and pred are different : \"+str(len(pred))+\"   :   \"+str(len(pred))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true = range(30000)\n",
    "pred = range(30050,30000,-1)+range(29950)\n",
    "print zip_metric(range(30000),range(30000))\n",
    "\n",
    "nb_tot = len(true)\n",
    "pred_thred = sorted(pred, reverse=True)[int(nb_tot*1.0/100)]\n",
    "true_thred = sorted(true, reverse=True)[int(nb_tot*1.0/100)]\n",
    "try:\n",
    "    print  len([x for x in range(nb_tot) if (pred[x]>pred_thred and true[x]>true_thred)])*1.0/nb_tot*100*100\n",
    "except:\n",
    "    print \"length of true and pred are different : \"+str(len(pred))+\"   :   \"+str(len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1.0*250/300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(zip(sorted(zip(predi[:10],ndcg_predictors[test,0][:10])),range(10)),key = lambda x : x[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "kf = KFold(len(predictors_pd), n_folds=K,shuffle = True)\n",
    "for train, test in kf:\n",
    "    ndcg_model = learn(predictors_pd[numerical_predictors].as_matrix()[train,:]\\\n",
    "                       ,output_pd[\"cumulative_downloads_2016_02\"].as_matrix()[train]\\\n",
    "                       ,  n_trees=100, learning_rate=0.1, k=int(len(test)*1.0/100))\n",
    "    predi = evaluate(ndcg_model,predictors_pd[numerical_predictors].as_matrix()[test,:])\n",
    "    print zip_metric(output_pd[\"cumulative_downloads_2016_02\"].as_matrix()[test],predi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "kf = KFold(len(predictors_pd), n_folds=K,shuffle = True)\n",
    "for train, test in kf:\n",
    "    ndcg_model = learn(predictors_pd[numerical_predictors].as_matrix()[train,:]\\\n",
    "                       ,output_pd[\"cumulative_downloads_2016_02\"].as_matrix()[train]\\\n",
    "                       ,  n_trees=100, learning_rate=0.1, k=int(len(train)*1.0/100))\n",
    "    predi = evaluate(ndcg_model,predictors_pd[numerical_predictors].as_matrix()[test,:])\n",
    "    print zip_metric(output_pd[\"cumulative_downloads_2016_02\"].as_matrix()[test],predi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "kf = KFold(len(predictors_pd), n_folds=K,shuffle = True)\n",
    "for train, test in kf:\n",
    "    ndcg_model = learn(predictors_pd[numerical_predictors].as_matrix()[train,:]\\\n",
    "                       ,output_pd[\"cumulative_downloads_2016_02\"].as_matrix()[train]\\\n",
    "                       ,  n_trees=1000, learning_rate=0.1, k=int(len(test)*1.0/100))\n",
    "    predi = evaluate(ndcg_model,predictors_pd[numerical_predictors].as_matrix()[test,:])\n",
    "    print zip_metric(output_pd[\"cumulative_downloads_2016_02\"].as_matrix()[test],predi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "kf = KFold(len(predictors_pd), n_folds=K,shuffle = True)\n",
    "for train, test in kf:\n",
    "    ndcg_model = learn(predictors_pd[numerical_predictors].as_matrix()[train,:]\\\n",
    "                       ,output_pd[\"cumulative_downloads_2016_02\"].as_matrix()[train]\\\n",
    "                       ,  n_trees=1000, learning_rate=0.1, k=int(len(train)*1.0/100))\n",
    "    predi = evaluate(ndcg_model,predictors_pd[numerical_predictors].as_matrix()[test,:])\n",
    "    print zip_metric(output_pd[\"cumulative_downloads_2016_02\"].as_matrix()[test],predi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "kf = KFold(len(predictors_pd), n_folds=K,shuffle = True)\n",
    "for train, test in kf:\n",
    "    ndcg_model = learn(predictors_pd[numerical_predictors].as_matrix()[train,:]\\\n",
    "                       ,output_pd[\"cumulative_downloads_2016_02\"].as_matrix()[train]\\\n",
    "                       ,  n_trees=1000, learning_rate=0.01, k=int(len(test)*1.0/100))\n",
    "    predi = evaluate(ndcg_model,predictors_pd[numerical_predictors].as_matrix()[test,:])\n",
    "    print zip_metric(output_pd[\"cumulative_downloads_2016_02\"].as_matrix()[test],predi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "kf = KFold(len(predictors_pd), n_folds=K,shuffle = True)\n",
    "for train, test in kf:\n",
    "    ndcg_model = learn(predictors_pd[numerical_predictors].as_matrix()[train,:]\\\n",
    "                       ,output_pd[\"cumulative_downloads_2016_02\"].as_matrix()[train]\\\n",
    "                       ,  n_trees=1000, learning_rate=0.01, k=int(len(train)*1.0/100))\n",
    "    predi = evaluate(ndcg_model,predictors_pd[numerical_predictors].as_matrix()[test,:])\n",
    "    print zip_metric(output_pd[\"cumulative_downloads_2016_02\"].as_matrix()[test],predi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mod = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01,max_depth=2, random_state=0, loss='ls')\\\n",
    "    .fit(predictors_pd[numerical_predictors].as_matrix()[train,:], output_pd[\"cumulative_downloads_2016_02\"].as_matrix()[train] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "modmod= AdaBoostRegressor(base_estimator=mod, n_estimators=1, learning_rate=1, loss='linear', random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modmod.fit(predictors_pd[numerical_predictors].as_matrix()[train,:],output_pd[\"cumulative_downloads_2016_02\"].as_matrix()[train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predi = modmod.predict(predictors_pd[numerical_predictors].as_matrix()[test,:])\n",
    "print zip_metric(output_pd[\"cumulative_downloads_2016_02\"].as_matrix()[test],predi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predi = mod.predict(predictors_pd[numerical_predictors].as_matrix()[test,:])\n",
    "print zip_metric(output_pd[\"cumulative_downloads_2016_02\"].as_matrix()[test],predi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "mod = GradientBoostingRegressor(n_estimators=100, learning_rate=0.01,max_depth=2, random_state=0, loss='ls')\\\n",
    "    .fit(predictors_pd[numerical_predictors].as_matrix()[train,:], output_pd[\"cumulative_downloads_2016_02\"].as_matrix()[train] )\n",
    "predi = modmod.predict(predictors_pd[numerical_predictors].as_matrix()[test,:])\n",
    "print zip_metric(output_pd[\"cumulative_downloads_2016_02\"].as_matrix()[test],predi)\n",
    "for i in range(100):\n",
    "    modmod= AdaBoostRegressor(base_estimator=modmod, n_estimators=1, learning_rate=1, loss='linear', random_state=None)\n",
    "    modmod.fit(predictors_pd[numerical_predictors].as_matrix()[train,:],output_pd[\"cumulative_downloads_2016_02\"].as_matrix()[train])\n",
    "    predi = modmod.predict(predictors_pd[numerical_predictors].as_matrix()[test,:])\n",
    "    print zip_metric(output_pd[\"cumulative_downloads_2016_02\"].as_matrix()[test],predi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for train,test in KFold(len(predictors_pd), n_folds=K,shuffle = True):\n",
    "    train_features = predictors_pd[numerical_predictors].as_matrix()[train,:].astype(float)\n",
    "    train_output = output_pd[\"cumulative_downloads_2016_02\"].as_matrix()[train]\n",
    "    #temp = sorted(zip(train_output,train_features),key = lambda x:-x[0])\n",
    "    #train_feature = np.vstack([x[1] for x in temp])\n",
    "    #train_output = [x[0] for x in temp]\n",
    "    test_features = predictors_pd[numerical_predictors].as_matrix()[test,:].astype(float)\n",
    "    test_output = output_pd[\"cumulative_downloads_2016_02\"].as_matrix()[test]\n",
    "    #temp = sorted(zip(test_output,test_features),key = lambda x:-x[0])\n",
    "    #test_feature = np.vstack([x[1] for x in temp])\n",
    "    #test_output = [x[0] for x in temp]\n",
    "    relevance =(range(len(train)))\n",
    "    relevance.sort(key=lambda x: train_output[x])\n",
    "    relevance = (np.asarray(relevance)-int(99.0*len(train)/100)-np.zeros(len(train)))\n",
    "    relevance[relevance<0] = 0\n",
    "    #int(99.0*len(train)/100)\n",
    "    \n",
    "    modmod = GradientBoostingRegressor(n_estimators=1, learning_rate=0.1,max_depth=2, random_state=0, loss='ls')\\\n",
    "    .fit(train_features,train_output )\n",
    "    predi = modmod.predict(test_features)\n",
    "    print zip_metric(test_output,predi)\n",
    "    \n",
    "    train_predi = modmod.predict(train_features)\n",
    "    point_error = (train_output - train_predi )\n",
    "    \n",
    "    pred_relevance =(range(len(train)))\n",
    "    pred_relevance.sort(key=lambda x: train_predi[x])\n",
    "    pred_relevance = (np.asarray(pred_relevance)-int(99.0*len(train)/100)-np.zeros(len(train)))\n",
    "    pred_relevance[pred_relevance<0] = 0\n",
    "    \n",
    "    for i in range(100):\n",
    "        print \"score : \"+str(np.dot(test_output-predi,test_output-predi))\n",
    "        modmod= AdaBoostRegressor(base_estimator=modmod, n_estimators=1, learning_rate=0.01, loss='linear', random_state=None)\n",
    "        modmod.fit(train_features,train_output,np.abs(pred_relevance-relevance))\n",
    "        predi = modmod.predict(test_features)\n",
    "        print zip_metric(test_output,predi)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ndcg_model = learn(train_features,train_output,  n_trees=100, learning_rate=0.1, k=10)\n",
    "predi = evaluate(ndcg_model,test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "for s in range(15):\n",
    "    for train,test in KFold(len(predictors_pd), n_folds=K,shuffle = True):\n",
    "        train_features = predictors_pd[numerical_predictors].as_matrix()[train,:].astype(float)\n",
    "        train_output = output_pd[\"cumulative_downloads_2016_02\"].as_matrix()[train]\n",
    "        test_features = predictors_pd[numerical_predictors].as_matrix()[test,:].astype(float)\n",
    "        test_output = output_pd[\"cumulative_downloads_2016_02\"].as_matrix()[test]\n",
    "        relevance =(range(len(train)))\n",
    "        relevance.sort(key=lambda x: train_output[x])\n",
    "        #relevance = (np.asarray(relevance)-int(99.0*len(train)/100))\n",
    "        #relevance[relevance<0] = 0\n",
    "        ndcg_model = learn(train_features,train_output,  n_trees=100, learning_rate=0.1, k=10)\n",
    "        predi = evaluate(ndcg_model,test_features)\n",
    "        print zip_metric(predi,test_output)\n",
    "\n",
    "        modmod = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=2, random_state=0, loss='ls')\\\n",
    "        .fit(train_features,train_output )\n",
    "        predi = modmod.predict(test_features)\n",
    "        print zip_metric(test_output,predi)\n",
    "        \n",
    "        print \"**********\"\n",
    "    print \"______________\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = predi\n",
    "true = test_output\n",
    "nb_tot = len(true)\n",
    "pred_thred = sorted(pred, reverse=True)[int(nb_tot*1.0/100)]\n",
    "true_thred = sorted(true, reverse=True)[int(nb_tot*1.0/100)]\n",
    "print len([x for x in range(nb_tot) if (pred[x]>pred_thred and true[x]>true_thred)])*1.0/nb_tot*100*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indices.sort(key=lambda x: input[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "point_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.ones(len(train))/point_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors_pd[predictors_pd.week_1 >8.62 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_pd[output_pd.id == \"911686788iphone\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
