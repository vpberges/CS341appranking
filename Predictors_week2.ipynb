{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this markdown to generate the predictors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imported libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "from sklearn import linear_model\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "#Creating of the input data\n",
    "downloads = pd.read_csv('train_app_downloads.csv')\n",
    "reviews = pd.read_csv('train_app_review.csv')\n",
    "ratings = pd.read_csv('train_app_rating.csv')\n",
    "usages = pd.read_csv('train_usage.csv')\n",
    "revenues = pd.read_csv('train_revenue.csv')\n",
    "output = pd.read_csv('train_final_downloads.csv')\n",
    "dateRange = pd.date_range('2015-03-01', periods=56).format(formatter=lambda x: x.strftime('%Y-%m-%d'))\n",
    "\n",
    "sentiment = pd.read_csv('sentiment.csv',header=-1).ix[:,0]\n",
    "sentiment.values[sentiment.values==0.0] = 0.5\n",
    "reviews[\"sentiment_score\"] = sentiment.values \n",
    "\n",
    "#We map -1 to 0 in the downloads (there are no 0 in the initial data)\n",
    "replacementValue=0\n",
    "downloads = downloads.replace(-1,replacementValue)\n",
    "\n",
    "#Minor corrections\n",
    "ratings = ratings.rename(columns={'start1': 'star1'})\n",
    "ratings = pd.merge(downloads.drop(dateRange,1), ratings.drop('Unnamed: 0', 1), how='left',\n",
    "                   on=[\"id\",\"name\",\"category\"]).replace(\"NaN\",replacementValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = pd.read_csv('train_final_downloads.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rank = [0]* output.shape[0]\n",
    "top = [0]* output.shape[0]\n",
    "#dic = {}\n",
    "#ranklist = list(output.ix[:,\"cumulative_downloads_2016-02\"].argsort()[::-1])\n",
    "\n",
    "#for i in range(len(ranklist)):\n",
    " #   dic[ranklist[i]] = i\n",
    "#for i in range(output.shape[0]):\n",
    " #   rank[i] = dic[i]\n",
    "#output[\"rank\"] = rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_index = output.sort_values(by = \"cumulative_downloads_2016-02\" , ascending = False).iloc[0:6000,:].index #drop apps with the lowest final downloads\n",
    "top_index\n",
    "for i in range(output.shape[0]):\n",
    "    if i in top_index: top[i] = 1\n",
    "output[\"top\"] = top\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = output.sort_values(by = \"cumulative_downloads_2016-02\" , ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the predictors matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = pd.concat([downloads[\"id\"],downloads[\"name\"],downloads[\"category\"],downloads[\"device\"]],\n",
    "                       axis=1,keys=[\"id\",\"name\",\"category\",\"device\"])\n",
    "predictors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the train_app_downloads.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate the logWeekly average\n",
    "def generator_weekAvg(inp,w):\n",
    "    if (np.count_nonzero(inp[5+w*7:12+w*7] - replacementValue*np.ones(len(inp[5+w*7:12+w*7]))) == 0):\n",
    "        return 0\n",
    "    return  math.log(1.0*sum(inp[5+w*7:12+w*7])/np.count_nonzero(inp[5+w*7:12+w*7] \n",
    "                                                                 - replacementValue*np.ones(len(inp[5+w*7:12+w*7]))))\n",
    "\n",
    "for w in range(8):\n",
    "    predictors[\"week_\"+str(w+1)] = downloads.apply(generator_weekAvg,axis=1,args=(w,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate the daily average\n",
    "def generator_dailyAvg(inp):\n",
    "    if (np.count_nonzero(inp[5:] - replacementValue*np.ones(len(inp[5:]))) == 0):\n",
    "        return 0\n",
    "    return  (1.0*sum(inp[5:])/np.count_nonzero(inp[5:] - replacementValue*np.ones(len(inp[5:]))))\n",
    "    #return  math.log(1.0*sum(inp[5:])/len(inp[5:]))\n",
    "\n",
    "predictors[\"daily_avg\"] = downloads.apply(generator_dailyAvg,axis=1)\n",
    "#This one is bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate the polynomial coefficients\n",
    "def generator_coef(inp, coef):\n",
    "    return  np.polyfit(range(56),inp[5:],coef)[0]\n",
    "#Redo by ignoring the -1 / 0 ? w = [1110011]     w = np.not_equal(inp[5:],np.ones(len(inp[5:]))*replacementValue\n",
    "\n",
    "for c in range(4):\n",
    "    predictors[\"coef_\"+str(c)] = downloads.apply(generator_coef,axis=1,args=(c,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate the step max and min (we ignore the values of 0)\n",
    "def generator_maxStep(inp,maximum):\n",
    "    if (np.count_nonzero(inp[5:] - replacementValue*np.ones(len(inp[5:]))) == 0):\n",
    "        return 0\n",
    "    m = 0\n",
    "    for d in range(1,56):\n",
    "        if (inp[5+d]!=replacementValue and inp[4+d]!=replacementValue):\n",
    "            c = (inp[5+d]-inp[4+d])\n",
    "            if (maximum and m < c):\n",
    "                m = c\n",
    "            if ( not maximum and m > c):\n",
    "                m = c\n",
    "    return m\n",
    "                \n",
    "predictors[\"maxStep\"] = downloads.apply(generator_maxStep,axis=1,args=(True,))\n",
    "predictors[\"minStep\"] = downloads.apply(generator_maxStep,axis=1,args=(False,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Standard Deviation\n",
    "def generator_std(inp):\n",
    "    if (np.count_nonzero(inp[5:] - replacementValue*np.ones(len(inp[5:]))) == 0):\n",
    "        return 0\n",
    "    return np.std(inp[5:])\n",
    "\n",
    "predictors[\"std\"] = downloads.apply(generator_std,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Number of missing Values\n",
    "def generator_missing(inp):\n",
    "    return list(inp[5:]).count(replacementValue)\n",
    "    #return np.count_zero(inp[5:] -replacementValue*np.ones(len(inp[5:])))\n",
    "\n",
    "predictors[\"nb_missing\"] = downloads.apply(generator_missing,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#device\n",
    "def generator_iphone(inp):\n",
    "    if (inp[4] == \"iphone\"):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def generator_ipad(inp):\n",
    "    if(inp[4] == \"ipad\"):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "predictors[\"iphone\"] = downloads.apply(generator_iphone,axis = 1)\n",
    "predictors[\"ipad\"] = downloads.apply(generator_ipad,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Categories\n",
    "def generator_categories(inp,cat):\n",
    "    if (inp[3] == cat):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "for cat in list(set(downloads[\"category\"])):\n",
    "    predictors[cat] = downloads.apply(generator_categories,axis = 1,args=(cat,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the train_app_rating.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate ratings   This is bad\n",
    "def generator_weightSum(inp):\n",
    "    return -2*inp[5]-inp[6]+inp[8]+2*inp[9]\n",
    "\n",
    "predictors[\"ratingsWeightSum\"] = ratings.apply(generator_weightSum,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_reviews = reviews.groupby('id').agg('mean')\n",
    "avg_reviews['rating']\n",
    "predictors['avg_review'] = predictors.join(avg_reviews['rating'],on='id')['rating'].replace(\"NaN\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_reviews = reviews.groupby('id').agg('var')\n",
    "avg_reviews['rating']\n",
    "predictors['var_review'] = predictors.join(avg_reviews['rating'],on='id')['rating'].replace(\"NaN\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#raw ratings\n",
    "predictors = pd.merge(predictors, ratings.drop('Unnamed: 0', 1), how='left',\n",
    "                   on=[\"id\",\"name\",\"category\",\"device\"]).replace(\"NaN\",replacementValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Later we can compute the weighted average of sentiment scores based on reviewers.\n",
    "#add positive and negative columns to indicate the app's popularity\n",
    "avg_score = [0]*predictors.shape[0]\n",
    "predictors[\"positive\"] = [0]*predictors.shape[0]\n",
    "predictors[\"negative\"] = [0]*predictors.shape[0]\n",
    "for i in range(predictors.shape[0]):\n",
    "    avg_score[i] = reviews.ix[reviews[\"id\"]==predictors.id[i],:][\"sentiment_score\"].mean()\n",
    "    if avg_score[i]>0.52: \n",
    "        predictors[\"positive\"].values[i] = 1\n",
    "    elif avg_score[i]<0.48: predictors[\"negative\"].values[i] = 1\n",
    "predictors[\"avg_sentiment_score\"] = avg_score\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use coeficients of metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    predictors[\"m\"+str(i+1)+\"_max\"] = np.zeros(predictors.shape[0]) \n",
    "    predictors[\"m\"+str(i+1)+\"_min\"] = np.zeros(predictors.shape[0])\n",
    "    predictors[\"m\"+str(i+1)+\"_mean\"] = np.zeros(predictors.shape[0])\n",
    "    predictors[\"m\"+str(i+1)+\"_std\"] = np.zeros(predictors.shape[0])\n",
    "    for j in range(3):\n",
    "        predictors[\"m\"+str(i+1)+\"_coef_\"+str(j)] = np.zeros(predictors.shape[0])\n",
    "\n",
    "for i in range(predictors.shape[0]):\n",
    "    if predictors[\"id\"].values[i] not in usages[\"id\"].values: continue\n",
    "    for j in range(4):\n",
    "        tmp = usages.ix[usages[\"id\"] == predictors[\"id\"].values[i],:]\n",
    "        time_series = np.array(tmp.ix[tmp[\"metric\"] == j+1,6:14])[0]   \n",
    "        if -1 in time_series: continue\n",
    "        predictors[\"m\"+str(j+1)+\"_max\"].values[i] = time_series.max()\n",
    "        predictors[\"m\"+str(j+1)+\"_min\"].values[i] = time_series.min()\n",
    "        predictors[\"m\"+str(j+1)+\"_mean\"].values[i] = time_series.mean()\n",
    "        predictors[\"m\"+str(j+1)+\"_std\"].values[i] = time_series.std()\n",
    "        X = np.array(range(8))\n",
    "        fit = np.polyfit(X,time_series,2)\n",
    "        for k in range(3):\n",
    "            predictors[\"m\"+str(j+1)+\"_coef_\"+str(k)].values[i] = fit[2-k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use coeficients of revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for j in range(3):  \n",
    "    predictors[\"rev_coef_\"+str(j)] = np.zeros(predictors.shape[0])\n",
    "    predictors[\"rev_max\"] = np.zeros(predictors.shape[0])\n",
    "    predictors[\"rev_min\"] = np.zeros(predictors.shape[0])\n",
    "    predictors[\"rev_mean\"] = np.zeros(predictors.shape[0])\n",
    "    predictors[\"rev_std\"] = np.zeros(predictors.shape[0])\n",
    "for i in range(predictors.shape[0]):\n",
    "    if predictors[\"id\"].values[i] in revenues[\"id\"].values: \n",
    "        curr_rev  = revenues.ix[revenues[\"id\"]== predictors[\"id\"].values[i],:]\n",
    "        if predictors[\"device\"].values[i] in curr_rev[\"device\"].values:\n",
    "            curr_rev = curr_rev.ix[curr_rev[\"device\"] == predictors[\"device\"].values[i],:]\n",
    "            time_series = np.array(curr_rev.ix[:,5:61])[0]\n",
    "            Y = []\n",
    "            for t in range(49): Y.append(time_series[t:t+7].sum()) #reduce seasonality\n",
    "            Y = np.array(Y)\n",
    "            if -1 in time_series: continue \n",
    "            predictors[\"rev_max\"].values[i] = Y.max()\n",
    "            predictors[\"rev_min\"].values[i] = Y.min()\n",
    "            predictors[\"rev_mean\"].values[i] = Y.mean()\n",
    "            predictors[\"rev_std\"].values[i] = Y.std()\n",
    "            X = np.array(range(49))\n",
    "            fit = np.polyfit(X,Y,2)\n",
    "            for k in range(3):  predictors[\"rev_coef_\"+str(k)].values[i] = fit[2-k]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print predictors.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How good did the predictor perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is the metric we use to determine our performance\n",
    "def metric(y_pred,y_test, top  = 20):\n",
    "    return len(set([i[0] for i in sorted(enumerate(y_pred), key=lambda x:x[1],reverse=True)][0:top])\n",
    "               .intersection([i[0] for i in sorted(enumerate(y_test), key=lambda x:x[1],reverse=True)][0:top]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## To csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors.to_csv(\"predictors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = pd.read_csv('predictors.csv').drop('Unnamed: 0', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print predictors.shape\n",
    "print output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#predictors = pd.read_csv('predictors.csv').drop('Unnamed: 0', 1)\n",
    "#predictors = predictors.drop('Unnamed: 0', 1)\n",
    "predictors = predictors.fillna(0)\n",
    "\n",
    "#Standardize\n",
    "#for p in [ 'daily_avg', 'coef_0', 'coef_1', 'coef_2', 'coef_3', 'maxStep', 'minStep', 'std', 'nb_missing', 'ratingsWeightSum', 'star1', 'star2', 'star3', 'star4', 'star5']:\n",
    "#    predictors[p] = (predictors[p] - predictors[p].mean())/predictors[p].std() \n",
    "\n",
    "#drop daily\n",
    "#predictors = predictors.drop(\"week1\",1)\n",
    "#predictors = predictors.drop(\"week2\",1)\n",
    "#predictors = predictors.drop(\"week3\",1)\n",
    "#predictors = predictors.drop(\"week4\",1)\n",
    "#predictors = predictors.drop(\"week5\",1)\n",
    "#predictors = predictors.drop(\"week6\",1)\n",
    "#predictors = predictors.drop(\"week7\",1)\n",
    "#predictors = predictors.drop(\"week8\",1)\n",
    "# predictors = predictors.drop(\"daily_avg\",1)\n",
    "# predictors = predictors.drop(\"star1\",1)\n",
    "# predictors = predictors.drop(\"star2\",1)\n",
    "# predictors = predictors.drop(\"star3\",1)\n",
    "# predictors = predictors.drop(\"star4\",1)\n",
    "# predictors = predictors.drop(\"star5\",1)\n",
    "# predictors = predictors.drop(list(set(downloads[\"category\"])),1)\n",
    "# predictors = predictors.drop(\"ratingsWeightSum\",1)\n",
    "#predictors = predictors.drop(\"coef_0\",1)\n",
    "# predictors = predictors.drop(\"coef_1\",1)\n",
    "# predictors = predictors.drop(\"coef_2\",1)\n",
    "# predictors = predictors.drop(\"coef_3\",1)\n",
    "# predictors = predictors.drop(\"maxStep\",1)\n",
    "# predictors = predictors.drop(\"minStep\",1)\n",
    "# predictors = predictors.drop(\"std\",1)\n",
    "# predictors = predictors.drop(\"nb_missing\",1)\n",
    "#predictors = predictors.drop(\"iphone\",1)\n",
    "# predictors = predictors.drop(\"ipad\",1)\n",
    "\n",
    "# predictors = predictors.drop('avg_sentiment_score',1)\n",
    "# predictors = predictors.drop('positive',1)\n",
    "# predictors = predictors.drop('negative',1)\n",
    "#predictors = predictors.drop('m1_coef_0',1)\n",
    "# predictors = predictors.drop('m1_coef_1',1)\n",
    "# predictors = predictors.drop('m1_coef_2',1)\n",
    "#predictors = predictors.drop('m2_coef_0',1)\n",
    "#predictors = predictors.drop('m2_coef_1',1)\n",
    "# predictors = predictors.drop('m2_coef_2',1)\n",
    "#predictors = predictors.drop('m3_coef_0',1)\n",
    "# predictors = predictors.drop('m3_coef_1',1)\n",
    "# predictors = predictors.drop('m3_coef_2',1)\n",
    "#predictors = predictors.drop('m4_coef_0',1)\n",
    "# predictors = predictors.drop('m4_coef_1',1)\n",
    "# predictors = predictors.drop('m4_coef_2',1)\n",
    "#predictors = predictors.drop('rev_coef_0',1)\n",
    "#predictors = predictors.drop('rev_coef_1',1)\n",
    "#predictors = predictors.drop('rev_coef_2',1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_frac = 0.31  #Fraction of test points\n",
    "N = 20   #number of iterations\n",
    "l=100  #Size of the top\n",
    "y_pred = np.array([0]*predictors.shape[0])   \n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "for i in range(1,N):\n",
    "    r = np.random.randint(1,429496729)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors.as_matrix()[:,4:], output.as_matrix()[:,7], test_size=test_frac, random_state=r)  \n",
    "    #print list(y_train)\n",
    "    mod= RandomForestClassifier(max_features = 1.0/3.0,n_estimators = 100) \n",
    "    mod.fit(X_train,list(y_train))    \n",
    "    y_pred = y_pred + np.array(mod.predict(predictors.as_matrix()[:,4:]))\n",
    "\n",
    "top_pred = []\n",
    "for i in range(predictors.shape[0]):\n",
    "    if y_pred[i] >N/2: top_pred.append(i)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#predictors = pd.read_csv('predictors.csv').drop('Unnamed: 0', 1)\n",
    "#predictors = predictors.drop('Unnamed: 0', 1)\n",
    "#predictors = predictors.ix[top_pred,:]\n",
    "#output = output.ix[top_pred,:]\n",
    "#Standardize\n",
    "#for p in [ 'daily_avg', 'coef_0', 'coef_1', 'coef_2', 'coef_3', 'maxStep', 'minStep', 'std', 'nb_missing', 'ratingsWeightSum', 'star1', 'star2', 'star3', 'star4', 'star5']:\n",
    "#    predictors[p] = (predictors[p] - predictors[p].mean())/predictors[p].std() \n",
    "\n",
    "#drop daily\n",
    "#predictors = predictors.drop(\"week1\",1)\n",
    "#predictors = predictors.drop(\"week2\",1)\n",
    "#predictors = predictors.drop(\"week3\",1)\n",
    "#predictors = predictors.drop(\"week4\",1)\n",
    "#predictors = predictors.drop(\"week5\",1)\n",
    "#predictors = predictors.drop(\"week6\",1)\n",
    "#predictors = predictors.drop(\"week7\",1)\n",
    "#predictors = predictors.drop(\"week8\",1)\n",
    "# predictors = predictors.drop(\"daily_avg\",1)\n",
    "# predictors = predictors.drop(\"star1\",1)\n",
    "# predictors = predictors.drop(\"star2\",1)\n",
    "# predictors = predictors.drop(\"star3\",1)\n",
    "# predictors = predictors.drop(\"star4\",1)\n",
    "# predictors = predictors.drop(\"star5\",1)\n",
    "# predictors = predictors.drop(list(set(downloads[\"category\"])),1)\n",
    "# predictors = predictors.drop(\"ratingsWeightSum\",1)\n",
    "#predictors = predictors.drop(\"coef_0\",1)\n",
    "# predictors = predictors.drop(\"coef_1\",1)\n",
    "# predictors = predictors.drop(\"coef_2\",1)\n",
    "# predictors = predictors.drop(\"coef_3\",1)\n",
    "# predictors = predictors.drop(\"maxStep\",1)\n",
    "# predictors = predictors.drop(\"minStep\",1)\n",
    "# predictors = predictors.drop(\"std\",1)\n",
    "# predictors = predictors.drop(\"nb_missing\",1)\n",
    "#predictors = predictors.drop(\"iphone\",1)\n",
    "# predictors = predictors.drop(\"ipad\",1)\n",
    "\n",
    "# predictors = predictors.drop('avg_sentiment_score',1)\n",
    "# predictors = predictors.drop('positive',1)\n",
    "# predictors = predictors.drop('negative',1)\n",
    "#predictors = predictors.drop('m1_coef_0',1)\n",
    "# predictors = predictors.drop('m1_coef_1',1)\n",
    "# predictors = predictors.drop('m1_coef_2',1)\n",
    "#predictors = predictors.drop('m2_coef_0',1)\n",
    "#predictors = predictors.drop('m2_coef_1',1)\n",
    "# predictors = predictors.drop('m2_coef_2',1)\n",
    "#predictors = predictors.drop('m3_coef_0',1)\n",
    "# predictors = predictors.drop('m3_coef_1',1)\n",
    "# predictors = predictors.drop('m3_coef_2',1)\n",
    "#predictors = predictors.drop('m4_coef_0',1)\n",
    "# predictors = predictors.drop('m4_coef_1',1)\n",
    "# predictors = predictors.drop('m4_coef_2',1)\n",
    "#predictors = predictors.drop('rev_coef_0',1)\n",
    "#predictors = predictors.drop('rev_coef_1',1)\n",
    "#predictors = predictors.drop('rev_coef_2',1)\n",
    "\n",
    "\n",
    "\n",
    "NUM = predictors.shape[0]\n",
    "old_top = []\n",
    "new_top = []\n",
    "\n",
    "test_frac = 0.31  #Fraction of test points\n",
    "N = 20   #number of iterations\n",
    "l=100  #Size of the top\n",
    "np.random.seed(1)\n",
    "for i in range(1,N):\n",
    "    r = np.random.randint(1,429496729)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors.as_matrix()[:,4:12], output.as_matrix()[:,5], test_size=test_frac, random_state=r)\n",
    "    #X_test = X_test[0:NUM/5]\n",
    "    #y_test = y_test[0:NUM/5]\n",
    "    old_mod=linear_model.LinearRegression(fit_intercept=False).fit(X_train,y_train)\n",
    "    old_y_pred =  old_mod.predict(X_test)\n",
    "    old_top.append(metric(old_y_pred,y_test,l))\n",
    "    \n",
    "'''np.random.seed(1)\n",
    "for i in range(1,N):\n",
    "    r = np.random.randint(1,429496729)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors.as_matrix()[:,4:], output.as_matrix()[:,5], test_size=test_frac, random_state=r)\n",
    "    X_test = X_test[0:10000]\n",
    "    y_test = y_test[0:10000]\n",
    "    #mod=linear_model.LinearRegression(fit_intercept=False).fit(X_train,y_train)\n",
    "    mod=linear_model.Lasso(fit_intercept=False).fit(X_train,y_train) \n",
    "    y_pred =  mod.predict(X_test)\n",
    "    new_top.append(metric(y_pred,y_test,l))'''\n",
    "\n",
    "np.random.seed(1)\n",
    "for i in range(1,N):\n",
    "    r = np.random.randint(1,429496729)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors.as_matrix()[:,4:], output.as_matrix()[:,5], test_size=test_frac, random_state=r)\n",
    "    #X_test = X_test[0:NUM/5]\n",
    "    #y_test = y_test[0:NUM/5]\n",
    "    #mod=linear_model.LinearRegression(fit_intercept=False).fit(X_train,y_train)\n",
    "    #mod= RandomForestRegressor(max_features = 1.0/3.0,n_estimators = 100) \n",
    "    #mod.fit(X_train,y_train)\n",
    "    #importance_list = list(reversed(np.argsort(mod.feature_importances_)))[:50]\n",
    "    mod=linear_model.Lasso(fit_intercept=False).fit(X_train,y_train) \n",
    "    importance_list =[i for i in  range(len(mod.coef_))if abs(mod.coef_[i])>10]\n",
    "    \n",
    "    '''X_boost_train = X_train[:,importance_list]\n",
    "    y_boost_train = y_train\n",
    "    X_boost_test = X_test[:,importance_list]\n",
    "    y_boost_test = y_test\n",
    "    params = {'n_estimators': 200, 'max_depth': 1, 'learning_rate': 0.01, 'loss': 'ls'}\n",
    "    boost = GradientBoostingRegressor(**params) \n",
    "    boost.fit(X_boost_train,y_boost_train)\n",
    "    y_pred =  boost.predict(X_boost_test)'''\n",
    "    \n",
    "    X_rf_train = X_train[:,importance_list]\n",
    "    y_rf_train = y_train\n",
    "    X_rf_test = X_test[:,importance_list]\n",
    "    y_rf_test = y_test\n",
    "    mod2= RandomForestRegressor(max_features = 1.0/3.0,n_estimators = 200) \n",
    "    mod2.fit(X_rf_train,y_rf_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    y_pred = mod2.predict(X_rf_test)\n",
    "    new_top.append(metric(y_pred,y_rf_test,l))\n",
    "    \n",
    "if (N<300):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(range(1,N),old_top,label=\"old\")\n",
    "    plt.plot(range(1,N),new_top,label=\"performance\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "print 1.0*sum(old_top)/len(old_top)\n",
    "print list(predictors.columns.values)\n",
    "print 1.0*sum(new_top)/len(new_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "for i in range(1,N):\n",
    "    r = np.random.randint(1,429496729)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors.as_matrix()[:,4:], output.as_matrix()[:,5], test_size=test_frac, random_state=r)\n",
    "    X_test = X_test[0:10000]\n",
    "    y_test = y_test[0:10000]\n",
    "    #mod=linear_model.LinearRegression(fit_intercept=False).fit(X_train,y_train)\n",
    "    mod= RandomForestRegressor(max_features = 1.0/3.0,n_estimators = 100) \n",
    "    mod.fit(X_train,y_train)\n",
    "    '''importance_list = list(reversed(np.argsort(mod.feature_importances_)))[:50]\n",
    "    X_boost_train = X_train[:,importance_list]\n",
    "    y_boost_train = y_train\n",
    "    X_boost_test = X_test[:,importance_list]\n",
    "    y_boost_test = y_test\n",
    "    params = {'n_estimators': 200, 'max_depth': 1, 'learning_rate': 0.01, 'loss': 'ls'}\n",
    "    boost = GradientBoostingRegressor(**params) \n",
    "    boost.fit(X_boost_train,y_boost_train)\n",
    "    y_pred =  boost.predict(X_boost_test)'''\n",
    "    y_pred = mod.predict(X_test)\n",
    "    new_top.append(metric(y_pred,y_test,l))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['id', 'name', 'category', 'device', 'week_1', 'week_2', 'week_3', 'week_4', 'week_5', 'week_6', 'week_7', 'week_8', 'daily_avg', 'coef_0', 'coef_1', 'coef_2', 'coef_3', 'maxStep', 'minStep', 'std', 'nb_missing', 'iphone', 'ipad', 'Productivity', 'Entertainment', 'Travel', 'Sports', 'Music', 'Shopping', 'Finance', 'Business', 'Navigation', 'Food and Drink', 'Utilities', 'Newsstand', 'Health and Fitness', 'News', 'Lifestyle', 'Medical', 'Weather', 'Games', 'Catalogs', 'Social Networking', 'Photo and Video', 'Reference', 'Books', 'Education', 'ratingsWeightSum', 'avg_review', 'var_review', 'star1', 'star2', 'star3', 'star4', 'star5', 'positive', 'negative', 'avg_sentiment_score', 'm1_max', 'm1_min', 'm1_mean', 'm1_std', 'm1_coef_1', 'm1_coef_2', 'm2_max', 'm2_min', 'm2_mean', 'm2_std', 'm2_coef_1', 'm2_coef_2', 'm3_max', 'm3_min', 'm3_mean', 'm3_std', 'm3_coef_1', 'm3_coef_2', 'm4_max', 'm4_min', 'm4_mean', 'm4_std', 'm4_coef_1', 'm4_coef_2', 'rev_coef_1', 'rev_coef_2', 'rev_max', 'rev_min', 'rev_mean', 'rev_std']\n",
    "58.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "54.8947368421  \n",
    "55.0526315789  #with #missing and Lasso <br/>\n",
    "54.7894736842  #including raw ratings Lasso :( <br/>\n",
    "55.0526315789  #with #missing weightedSumRatings and Lasso <br/>\n",
    "55.1052631579  #adding the categories <br/>\n",
    "55.3684210526  #adding average sentiment score and positive/negative label <br/>\n",
    "55.5263157895  #adding coeficients of usages\n",
    "\n",
    "\n",
    "With 10000 only:\n",
    "54.8947368421 all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_reviews = reviews.groupby('id').agg('mean')\n",
    "avg_reviews['rating']\n",
    "predictors['avg_review'] = predictors.join(avg_reviews['rating'],on='id')['rating'].replace(\"NaN\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_reviews = reviews.groupby('id').agg('var')\n",
    "avg_reviews['rating']\n",
    "predictors['var_review'] = predictors.join(avg_reviews['rating'],on='id')['rating'].replace(\"NaN\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = predictors.drop(['avg_sentiment_score'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors[\"avg_sentiment_score\"][16].replace('nan',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
