{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this markdown to generate the predictors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imported libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import langdetect\n",
    "import datetime\n",
    "%matplotlib inline  \n",
    "from sklearn.svm import SVR\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "#Creating of the input data\n",
    "downloads = pd.read_csv('train_app_downloads.csv')\n",
    "reviews = pd.read_csv('train_app_review.csv')\n",
    "ratings = pd.read_csv('train_app_rating.csv')\n",
    "usages = pd.read_csv('train_usage.csv')\n",
    "revenues = pd.read_csv('train_revenue.csv')\n",
    "output = pd.read_csv('train_final_downloads.csv')\n",
    "dateRange = pd.date_range('2015-03-01', periods=56).format(formatter=lambda x: x.strftime('%Y-%m-%d'))\n",
    "sentiment = pd.read_csv('sentiment.csv',header=-1).ix[:,0]\n",
    "sentiment.values[sentiment.values==0.0] = 0.5\n",
    "reviews[\"sentiment_score\"] = sentiment.values \n",
    "\n",
    "#We map -1 to 0 in the downloads (there are no 0 in the initial data)\n",
    "replacementValue=0\n",
    "#downloads = downloads.replace(-1,replacementValue)\n",
    "imputer = Imputer(missing_values=-1, strategy='median', axis=0)\n",
    "name = downloads.columns\n",
    "downloads = pd.concat([downloads.ix[:,0:5],pd.DataFrame(imputer.fit_transform(downloads.ix[:,5:]))],axis = 1)\n",
    "downloads.columns = name \n",
    "\n",
    "name = usages.columns\n",
    "usages = pd.concat([usages.ix[:,0:6],pd.DataFrame(imputer.fit_transform(usages.ix[:,6:]))],axis = 1)\n",
    "usages.columns = name \n",
    "\n",
    "name = revenues.columns\n",
    "revenues = pd.concat([revenues.ix[:,0:5],pd.DataFrame(imputer.fit_transform(revenues.ix[:,5:]))],axis = 1)\n",
    "revenues.columns = name \n",
    "\n",
    "\n",
    "\n",
    "#Minor corrections\n",
    "ratings = ratings.rename(columns={'start1': 'star1'})\n",
    "ratings = pd.merge(downloads.drop(dateRange,1), ratings.drop('Unnamed: 0', 1), how='left',\n",
    "                   on=[\"id\",\"name\",\"category\"]).replace(\"NaN\",replacementValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the predictors matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>device</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>281704574</td>\n",
       "      <td>AIM: Chat, Free Text, Photo Share, Voice Message</td>\n",
       "      <td>Social Networking</td>\n",
       "      <td>iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>281922769</td>\n",
       "      <td>Mobile MIM</td>\n",
       "      <td>Medical</td>\n",
       "      <td>ipad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>281922769</td>\n",
       "      <td>Mobile MIM</td>\n",
       "      <td>Medical</td>\n",
       "      <td>iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>281935788</td>\n",
       "      <td>Epocrates References &amp; Tools for Healthcare Pr...</td>\n",
       "      <td>Medical</td>\n",
       "      <td>ipad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>281935788</td>\n",
       "      <td>Epocrates References &amp; Tools for Healthcare Pr...</td>\n",
       "      <td>Medical</td>\n",
       "      <td>iphone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               name  \\\n",
       "0  281704574   AIM: Chat, Free Text, Photo Share, Voice Message   \n",
       "1  281922769                                         Mobile MIM   \n",
       "2  281922769                                         Mobile MIM   \n",
       "3  281935788  Epocrates References & Tools for Healthcare Pr...   \n",
       "4  281935788  Epocrates References & Tools for Healthcare Pr...   \n",
       "\n",
       "            category  device  \n",
       "0  Social Networking  iphone  \n",
       "1            Medical    ipad  \n",
       "2            Medical  iphone  \n",
       "3            Medical    ipad  \n",
       "4            Medical  iphone  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors = pd.concat([downloads[\"id\"],downloads[\"name\"],downloads[\"category\"],downloads[\"device\"]],\n",
    "                       axis=1,keys=[\"id\",\"name\",\"category\",\"device\"])\n",
    "predictors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the train_app_downloads.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate the logWeekly average\n",
    "def generator_weekAvg(inp,w):\n",
    "    if (np.count_nonzero(inp[5+w*7:12+w*7] - replacementValue*np.ones(len(inp[5+w*7:12+w*7]))) == 0):\n",
    "        return 0\n",
    "    return  math.log(1.0*sum(inp[5+w*7:12+w*7])/np.count_nonzero(inp[5+w*7:12+w*7] \n",
    "                                                                 - replacementValue*np.ones(len(inp[5+w*7:12+w*7]))))\n",
    "\n",
    "for w in range(8):\n",
    "    predictors[\"week_\"+str(w+1)] = downloads.apply(generator_weekAvg,axis=1,args=(w,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate the daily average\n",
    "def generator_dailyAvg(inp):\n",
    "    if (np.count_nonzero(inp[5:] - replacementValue*np.ones(len(inp[5:]))) == 0):\n",
    "        return 0\n",
    "    return  (1.0*sum(inp[5:])/np.count_nonzero(inp[5:] - replacementValue*np.ones(len(inp[5:]))))\n",
    "    #return  math.log(1.0*sum(inp[5:])/len(inp[5:]))\n",
    "\n",
    "predictors[\"daily_avg\"] = downloads.apply(generator_dailyAvg,axis=1)\n",
    "predictors[\"download_sum\"] = downloads.ix[:,5:].sum(axis=1)\n",
    "#This one is bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate the polynomial coefficients\n",
    "def generator_coef(inp, coef):\n",
    "    return  np.polyfit(range(56),inp[5:],coef)[0]\n",
    "#Redo by ignoring the -1 / 0 ? w = [1110011]     w = np.not_equal(inp[5:],np.ones(len(inp[5:]))*replacementValue\n",
    "\n",
    "for c in range(4):\n",
    "    predictors[\"coef_\"+str(c)] = downloads.apply(generator_coef,axis=1,args=(c,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate the step max and min (we ignore the values of 0)\n",
    "def generator_maxStep(inp,maximum):\n",
    "    if (np.count_nonzero(inp[5:] - replacementValue*np.ones(len(inp[5:]))) == 0):\n",
    "        return 0\n",
    "    m = 0\n",
    "    for d in range(1,56):\n",
    "        if (inp[5+d]!=replacementValue and inp[4+d]!=replacementValue):\n",
    "            c = (inp[5+d]-inp[4+d])\n",
    "            if (maximum and m < c):\n",
    "                m = c\n",
    "            if ( not maximum and m > c):\n",
    "                m = c\n",
    "    return m\n",
    "                \n",
    "predictors[\"maxStep\"] = downloads.apply(generator_maxStep,axis=1,args=(True,))\n",
    "predictors[\"minStep\"] = downloads.apply(generator_maxStep,axis=1,args=(False,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Standard Deviation\n",
    "def generator_std(inp):\n",
    "    if (np.count_nonzero(inp[5:] - replacementValue*np.ones(len(inp[5:]))) == 0):\n",
    "        return 0\n",
    "    return np.std(inp[5:])\n",
    "\n",
    "predictors[\"std\"] = downloads.apply(generator_std,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Number of missing Values\n",
    "def generator_missing(inp):\n",
    "    return list(inp[5:]).count(replacementValue)\n",
    "    #return np.count_zero(inp[5:] -replacementValue*np.ones(len(inp[5:])))\n",
    "\n",
    "predictors[\"nb_missing\"] = downloads.apply(generator_missing,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#device\n",
    "def generator_iphone(inp):\n",
    "    if (inp[4] == \"iphone\"):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def generator_ipad(inp):\n",
    "    if(inp[4] == \"ipad\"):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "predictors[\"iphone\"] = downloads.apply(generator_iphone,axis = 1)\n",
    "predictors[\"ipad\"] = downloads.apply(generator_ipad,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Categories\n",
    "def generator_categories(inp,cat):\n",
    "    if (inp[3] == cat):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "for cat in list(set(downloads[\"category\"])):\n",
    "    predictors[cat] = downloads.apply(generator_categories,axis = 1,args=(cat,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def detect_language(x):\n",
    "    try:\n",
    "        detected = langdetect.detect(x.decode('utf8','ignore'))\n",
    "        if detected in ['ja']:\n",
    "            return 'japanese'\n",
    "        elif detected in ['zh-cn']:\n",
    "            return 'chinese'\n",
    "        elif detected in ['ko']:\n",
    "            return 'korean'\n",
    "        elif detected in ['en']:\n",
    "            return 'english'\n",
    "        return 'other'\n",
    "    except:\n",
    "        return 'other'\n",
    "        #return None\n",
    "\n",
    "def set_lang_categories(x, cat):\n",
    "    if x == cat:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "lang_series = predictors['name'].apply(detect_language)\n",
    "for cat in list(set(lang_series)):\n",
    "    predictors[cat] = lang_series.apply(set_lang_categories, args=(cat,))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the train_cumulative_downloads_2015-02.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prev_downloads = pd.read_csv('train_cumulative_downloads_2015-02.csv').drop('Unnamed: 0', 1)\n",
    "predictors = pd.merge(predictors, prev_downloads, how='left',\n",
    "                  on=[\"id\",\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors['dl_projection'] = predictors['cumulative_downloads_2015-02'] + 7 * predictors['download_sum']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the train_app_rating.csv file and reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_reviews = reviews.groupby('id').agg('mean')\n",
    "avg_reviews['rating']\n",
    "predictors['avg_review'] = predictors.join(avg_reviews['rating'],on='id')['rating'].replace(\"NaN\",0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_reviews = reviews.groupby('id').agg('var')\n",
    "avg_reviews['rating']\n",
    "predictors['var_review'] = predictors.join(avg_reviews['rating'],on='id')['rating'].replace(\"NaN\",0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratings['num_ratings'] = ratings.ix[:,['star1','star2','star3','star4','star5']].sum(axis=1)\n",
    "#Scaling the ratings\n",
    "for i in range(1,6):\n",
    "    ratings['star'+str(i)]=ratings['star'+str(i)].divide(ratings['num_ratings']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#raw ratings\n",
    "predictors = pd.merge(predictors, ratings.drop('Unnamed: 0', 1), how='left',\n",
    "                   on=[\"id\",\"name\",\"category\",\"device\"]).replace(\"NaN\",replacementValue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reviews per daily downloads\n",
    "#predictors['reviewPerDailyDownloads'] = predictors['num_ratings'].divide(predictors['daily_avg']+1)\n",
    "predictors['ratings_per_daily_downloads'] = predictors['num_ratings'].divide(predictors['cumulative_downloads_2015-02']+1)\n",
    "\n",
    "#Drop either this or num_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_versions = reviews.groupby('id').version.nunique()\n",
    "num_versions.name = 'num_versions'\n",
    "predictors = predictors.join(num_versions, how='left', on='id')\n",
    "predictors['num_versions'] = predictors['num_versions'].replace('NaN',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_review = reviews.groupby('id').rating.count()\n",
    "num_review.name = 'num_review'\n",
    "predictors = predictors.join(num_review, how='left', on='id')\n",
    "predictors['num_review_per_daily_downloads'] = predictors['num_review'].replace('NaN',0).divide(predictors['cumulative_downloads_2015-02']+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the train_release_date.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_days_since_release(x):\n",
    "    return (datetime.datetime.strptime('03/01/2015', '%m/%d/%Y').date() - datetime.datetime.strptime(x, '%m/%d/%Y').date()).days\n",
    "\n",
    "release_date = pd.read_csv('train_release_date.csv').drop('Unnamed: 0', 1)\n",
    "release_date['days_since_release'] = release_date['release_date'].apply(generate_days_since_release)\n",
    "predictors = pd.merge(predictors, release_date.drop('release_date', 1), how='left',\n",
    "                  on=[\"id\",\"name\"])\n",
    "\n",
    "# replacing any missing values with 0; not a good idea but placeholder for now\n",
    "predictors['days_since_release'] = predictors['days_since_release'].replace('NaN', 0)\n",
    "\n",
    "# The number of downloads divided by time\n",
    "predictors['downloads_per_day_before'] = predictors['cumulative_downloads_2015-02'].divide(predictors['days_since_release']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Generate the polynomial coefficients\\ndef generator_coef(inp, coef):\\n    tmp = list(inp.values)\\n    #print tmp[60:61]\\n    #print int(tmp[59:60][0])\\n    Y = np.array(tmp[60:61]*(int(tmp[59:60][0])/7)+tmp[4:12])\\n    X = range(Y.shape[0])\\n    return  np.polyfit(X,Y,coef)[0]\\n#Redo by ignoring the -1 / 0 ? w = [1110011]     w = np.not_equal(inp[5:],np.ones(len(inp[5:]))*replacementValue\\n\\nfor c in range(4):\\n    predictors[\"coef_\"+str(c)] = predictors.apply(generator_coef,axis=1,args=(c,))\\n    '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "#Generate the polynomial coefficients\n",
    "def generator_coef(inp, coef):\n",
    "    tmp = list(inp.values)\n",
    "    #print tmp[60:61]\n",
    "    #print int(tmp[59:60][0])\n",
    "    Y = np.array(tmp[60:61]*(int(tmp[59:60][0])/7)+tmp[4:12])\n",
    "    X = range(Y.shape[0])\n",
    "    return  np.polyfit(X,Y,coef)[0]\n",
    "#Redo by ignoring the -1 / 0 ? w = [1110011]     w = np.not_equal(inp[5:],np.ones(len(inp[5:]))*replacementValue\n",
    "\n",
    "for c in range(4):\n",
    "    predictors[\"coef_\"+str(c)] = predictors.apply(generator_coef,axis=1,args=(c,))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Later we can compute the weighted average of sentiment scores based on reviewers.\n",
    "#add positive and negative columns to indicate the app's popularity\n",
    "\n",
    "#print reviews.ix[id==predictors.id[0],:][\"sentiment_score\"]\n",
    "#print reviews.ix[id==predictors.id[1],:][\"sentiment_score\"]\n",
    "\n",
    "avg_score = [0]*predictors.shape[0]\n",
    "predictors[\"positive\"] = [0]*predictors.shape[0]\n",
    "predictors[\"negative\"] = [0]*predictors.shape[0]\n",
    "\n",
    "reviewers = {}\n",
    "for i in range(reviews.shape[0]):\n",
    "    user = reviews['reviewer'].values[i]\n",
    "    item_id = reviews['id'].values[i]\n",
    "    if user in reviewers:\n",
    "        reviewers[user] = reviewers[user] + week_8[item_id]\n",
    "    else:\n",
    "        reviewers[user] = week_8[item_id]\n",
    "        \n",
    "for i in range(predictors.shape[0]):\n",
    "    score_list = reviews.ix[reviews[\"id\"]==predictors.id[0],:]['sentiment_score']\n",
    "    reviewer_list =  reviews.ix[reviews[\"id\"]==predictors.id[0],:]['reviewer']\n",
    "    s = 0\n",
    "    rs = 0 \n",
    "    for j in range(score_list.shape[0]):\n",
    "        s = s + score_list.values[j]*reviewers[reviewer_list.values[j]]\n",
    "        rs = rs + reviewers[reviewer_list.values[j]]\n",
    "    avg_score[i] = s/rs\n",
    "    #avg_score[i] = reviews.ix[reviews[\"id\"]==predictors.id[i],:][\"sentiment_score\"].mean()\n",
    "    if avg_score[i]>0.55: \n",
    "        predictors[\"positive\"].values[i] = 1\n",
    "    elif avg_score[i]<0.45: predictors[\"negative\"].values[i] = 1\n",
    "#predictors[\"avg_sentiment_score\"] = avg_score\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use coeficients of metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    predictors[\"m\"+str(i+1)+\"_max\"] = np.zeros(predictors.shape[0]) \n",
    "    predictors[\"m\"+str(i+1)+\"_min\"] = np.zeros(predictors.shape[0])\n",
    "    predictors[\"m\"+str(i+1)+\"_mean\"] = np.zeros(predictors.shape[0])\n",
    "    predictors[\"m\"+str(i+1)+\"_std\"] = np.zeros(predictors.shape[0])\n",
    "    for j in range(3):\n",
    "        predictors[\"m\"+str(i+1)+\"_coef_\"+str(j)] = np.zeros(predictors.shape[0])\n",
    "\n",
    "for i in range(predictors.shape[0]):\n",
    "    if predictors[\"id\"].values[i] not in usages[\"id\"].values: continue\n",
    "    for j in range(4):\n",
    "        tmp = usages.ix[usages[\"id\"] == predictors[\"id\"].values[i],:]\n",
    "        time_series = np.array(tmp.ix[tmp[\"metric\"] == j+1,6:14])[0]   \n",
    "        if -1 in time_series: continue\n",
    "        X = np.array(range(8))\n",
    "        fit = np.polyfit(X,time_series,2)\n",
    "        for k in range(3):\n",
    "            predictors[\"m\"+str(j+1)+\"_coef_\"+str(k)].values[i] = fit[2-k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use coeficients of revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for j in range(3):\n",
    "    predictors[\"rev_coef_\"+str(j)] = np.zeros(predictors.shape[0])\n",
    "    predictors[\"rev_max\"] = np.zeros(predictors.shape[0])\n",
    "    predictors[\"rev_min\"] = np.zeros(predictors.shape[0])\n",
    "    predictors[\"rev_mean\"] = np.zeros(predictors.shape[0])\n",
    "    predictors[\"rev_std\"] = np.zeros(predictors.shape[0])\n",
    "for i in range(predictors.shape[0]):\n",
    "    \n",
    "    if predictors[\"id\"].values[i] in revenues[\"id\"].values: \n",
    "        curr_rev  = revenues.ix[revenues[\"id\"]== predictors[\"id\"].values[i],:]\n",
    "        if predictors[\"device\"].values[i] in curr_rev[\"device\"].values:\n",
    "            curr_rev = curr_rev.ix[curr_rev[\"device\"] == predictors[\"device\"].values[i],:]\n",
    "            time_series = np.array(curr_rev.ix[:,5:61])[0]\n",
    "            if -1 in time_series: continue \n",
    "            X = np.array(range(56))\n",
    "            fit = np.polyfit(X,time_series,2)\n",
    "            for k in range(3):  predictors[\"rev_coef_\"+str(k)].values[i] = fit[2-k]            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## To csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors.to_csv(\"predictors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = pd.read_csv('predictors.csv').drop('Unnamed: 0', 1)\n",
    "predictors = predictors.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How good did the predictor perform   --> Start running from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imported libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "%matplotlib inline  \n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "#Creating of the input data\n",
    "downloads = pd.read_csv('train_app_downloads.csv')\n",
    "reviews = pd.read_csv('train_app_review.csv')\n",
    "ratings = pd.read_csv('train_app_rating.csv')\n",
    "usages = pd.read_csv('train_usage.csv')\n",
    "revenues = pd.read_csv('train_revenue.csv')\n",
    "output = pd.read_csv('train_final_downloads.csv')\n",
    "dateRange = pd.date_range('2015-03-01', periods=56).format(formatter=lambda x: x.strftime('%Y-%m-%d'))\n",
    "\n",
    "sentiment = pd.read_csv('sentiment.csv',header=-1).ix[:,0]\n",
    "sentiment.values[sentiment.values==0.0] = 0.5\n",
    "reviews[\"sentiment_score\"] = sentiment.values \n",
    "\n",
    "#We map -1 to 0 in the downloads (there are no 0 in the initial data)\n",
    "replacementValue=0\n",
    "downloads = downloads.replace(-1,replacementValue)\n",
    "\n",
    "#Minor corrections\n",
    "ratings = ratings.rename(columns={'start1': 'star1'})\n",
    "ratings = pd.merge(downloads.drop(dateRange,1), ratings.drop('Unnamed: 0', 1), how='left',\n",
    "                   on=[\"id\",\"name\",\"category\"]).replace(\"NaN\",replacementValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This is the metric we use to determine our performance\n",
    "def metric(y_pred,y_test,percent=1):\n",
    "    top = int(len(y_pred)/100.0*percent)\n",
    "    return (len(set([i[0] for i in sorted(enumerate(y_pred), key=lambda x:x[1],reverse=True)][0:top])\n",
    "       .intersection([i[0] for i in sorted(enumerate(y_test), key=lambda x:x[1],reverse=True)][0:top])\n",
    "               ))/(percent/100.0)/len(y_pred)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = pd.read_csv('predictors.csv').drop('Unnamed: 0', 1)\n",
    "predictors = predictors.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor selection 2 methods. Cannot run both..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Predictor selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Use a model to do a forward recursive predictor selection\n",
    "#mod=linear_model.Lasso(alpha=100,fit_intercept=False)\n",
    "mod=linear_model.LinearRegression(fit_intercept=False)\n",
    "#mod= RandomForestRegressor(max_features = 1.0/3.0,n_estimators = 100) \n",
    "#mod= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100) \n",
    "rfe = RFE(estimator=mod, n_features_to_select=1, step=1)\n",
    "rfe.fit(predictors.as_matrix()[:,4:], output.as_matrix()[:,5])\n",
    "ranking = rfe.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for e in [(i[0],i[1]) for i in sorted(zip(predictors.columns[4:],ranking),key=lambda x: x[1])]:\n",
    "    print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#manually drop predictors \n",
    "#predictors_to_drop = []\n",
    "nb_pred_to_keep = 60\n",
    "predictors_to_drop = [i[0] for i in sorted(zip(predictors.columns[4:],ranking),key=lambda x: x[1])][nb_pred_to_keep:]\n",
    "for col in predictors_to_drop:\n",
    "    predictors = predictors.drop(col,1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other predictor selection method using Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['week_1', 'week_3', 'week_4', 'week_5', 'week_6', 'week_7', 'daily_avg', 'download_sum', 'coef_1', 'maxStep', 'minStep', 'std', 'iphone', 'ipad', 'cumulative_downloads_2015-02', 'dl_projection', 'avg_review', 'var_review', 'num_ratings', 'num_review', 'days_since_release', 'downloads_per_day_before', 'm3_coef_0', 'm4_coef_0', 'm4_coef_1', 'm4_coef_2', 'rev_coef_0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:466: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Other method\n",
    "predictors = pd.read_csv('predictors.csv').drop('Unnamed: 0', 1)\n",
    "predictors = predictors.fillna(0)\n",
    "#var_select = linear_model.Lasso(alpha = 0.01).fit(predictor_train_top_10_precent.as_matrix()[:,4:],np.log(output_train_top_10_precent[\"cumulative_downloads_2016-02\"].as_matrix()))\n",
    "var_select = linear_model.Lasso(alpha = 0.1).fit(predictors.as_matrix()[:,4:],np.log(output[\"cumulative_downloads_2016-02\"].as_matrix()))\n",
    "\n",
    "importance_list = [i for i in range(len(var_select.coef_)) if abs(var_select.coef_[i])>0]\n",
    "important_predictors = [predictors.columns.values[i+4] for i in range(len(var_select.coef_)) if (i in importance_list)]\n",
    "print important_predictors\n",
    "for col in predictors.columns.values[4:]:\n",
    "    if col not in important_predictors:\n",
    "        predictors = predictors.drop(col,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Classification + Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'name', 'category', 'device', 'week_1', 'week_3', 'week_4', 'week_5', 'week_6', 'week_7', 'daily_avg', 'download_sum', 'coef_1', 'maxStep', 'minStep', 'std', 'iphone', 'ipad', 'cumulative_downloads_2015-02', 'dl_projection', 'avg_review', 'var_review', 'num_ratings', 'num_review', 'days_since_release', 'downloads_per_day_before', 'm3_coef_0', 'm4_coef_0', 'm4_coef_1', 'm4_coef_2', 'rev_coef_0']\n",
      "Old model           : 53.8049173871\n",
      "Top10%              : 99.6875\n",
      "Top1% with classif1 : 67.8125\n",
      "Top1% with classif2 : 67.5\n",
      "Top1% no classif1   : 67.1875\n"
     ]
    }
   ],
   "source": [
    "print list(predictors.columns.values) \n",
    "np.random.seed(1)\n",
    "K = 5\n",
    "\n",
    "top_percent_classif = 20\n",
    "\n",
    "\n",
    "kf = KFold(len(predictors), n_folds=K)\n",
    "old_top = []\n",
    "new_top = []\n",
    "new_top_noClassif = []\n",
    "top_10 = []\n",
    "new_top_select = [] \n",
    "new_top_noClassif = [] \n",
    "for train, test in kf:\n",
    "    #base model\n",
    "    old_mod=linear_model.LinearRegression(fit_intercept=False).fit(predictors.as_matrix()[train,4:12], output.as_matrix()[train,5])\n",
    "    old_y_pred =  old_mod.predict(predictors.as_matrix()[test,4:12])\n",
    "    old_top.append(metric(old_y_pred,output.as_matrix()[test,5]))\n",
    "    \n",
    "    #model to determine the top 10%   (CLASSIFICATION)\n",
    "    mod_class10= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100).fit(predictors.as_matrix()[train,4:], output.as_matrix()[train,5])\n",
    "    \n",
    "    y_pred =  mod_class10.predict(predictors.as_matrix()[test,4:])\n",
    "    \n",
    "    #estimate of the top 10% of the test set\n",
    "    estimate_class10 = predictors.iloc[test].copy()\n",
    "    estimate_class10[\"firstEstimate\"] = y_pred\n",
    "    estimate_class10 = estimate_class10.sort_values(by= \"firstEstimate\",ascending = False).iloc[0:int(1.0*top_percent_classif/100.0*len(estimate_class10))]\n",
    "    estimate_class10 = estimate_class10.drop(\"firstEstimate\",1)\n",
    "#     estimate_class10 = estimate_class10.sort_values(by= \"daily_avg\",ascending = False).iloc[0:int(1.0*top_percent_classif/100.0*len(estimate_class10))]\n",
    "\n",
    "    \n",
    "    #top 10% of the trainning set\n",
    "    output_train_top_10_precent = output.iloc[train].copy().sort_values(by= 'cumulative_downloads_2016-02',ascending = False).iloc[0:int(1.0*top_percent_classif/100.0*len(output.iloc[train]))].drop('Unnamed: 0',1)\n",
    "    predictor_train_top_10_precent = output_train_top_10_precent.merge(predictors, how='left', on=[\"id\",\"name\",\"category\",\"device\"]).copy()\n",
    "    predictor_train_top_10_precent = predictor_train_top_10_precent.drop('cumulative_downloads_2016-02',1)\n",
    "    #predictor_train_top_10_precent = predictor_train_top_10_precent.drop('firstEstimate',1)\n",
    "    \n",
    "    #This is the actual top 1% of the test set\n",
    "    output_test_top_1_precent = output.iloc[test].sort_values(by= 'cumulative_downloads_2016-02',ascending = False).iloc[0:int(0.01*len(output.iloc[test]))].copy()\n",
    "\n",
    "    \n",
    "    #second model -> Regression on the top obtainned by regression\n",
    "    #mod_top1= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100)\\\n",
    "    #mod_top1= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100)\\\n",
    "    #mod_top1=linear_model.Lasso(alpha=100,fit_intercept=False)\\\n",
    "    #mod_top1=GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0, loss='ls')\\\n",
    "    mod_top1 = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01,max_depth=2, random_state=0, loss='ls')\\\n",
    "    .fit(predictor_train_top_10_precent.as_matrix()[:,4:], np.log(output_train_top_10_precent[\"cumulative_downloads_2016-02\"].as_matrix()))\n",
    "\n",
    "    \n",
    "    \n",
    "    y_pred_2 =  mod_top1.predict(estimate_class10.as_matrix()[:,4:])\n",
    "    \n",
    "    \n",
    "    #Andrew's regression with additionnal lasso predictor selection\n",
    "    #(1) feature selection : lasso or random forest\n",
    "    #var_select = linear_model.Lasso(alpha = 0.01).fit(predictor_train_top_10_precent.as_matrix()[:,4:],np.log(output_train_top_10_precent[\"cumulative_downloads_2016-02\"].as_matrix()))\n",
    "    var_select = RandomForestRegressor(max_features = \"sqrt\",n_estimators = 100).fit(predictor_train_top_10_precent.as_matrix()[:,4:], np.log(output_train_top_10_precent[\"cumulative_downloads_2016-02\"].as_matrix()))\n",
    "      \n",
    "    #importance_list = [i for i in  range(len(var_select.coef_)) if abs(var_select.coef_[i])>0]    \n",
    "    importance_list = list(reversed(np.argsort(var_select.feature_importances_)))[0:20]\n",
    "    \n",
    "    #new train and test set with the selected variables\n",
    "    X_rf_train = predictor_train_top_10_precent.as_matrix()[:,4:][:,importance_list]\n",
    "    y_rf_train = output_train_top_10_precent[\"cumulative_downloads_2016-02\"].as_matrix()\n",
    "    X_rf_test = estimate_class10.as_matrix()[:,4:][:,importance_list]  \n",
    "    \n",
    "    #(2) regression: random forest or boosting \n",
    "    #mod_top1_select= RandomForestRegressor(max_features = \"sqrt\",n_estimators = 100).fit(X_rf_train, y_rf_train)\n",
    "    params = {'n_estimators': 1000, 'max_depth': 2, 'learning_rate': 0.01, 'loss': 'ls'}\n",
    "    mod_top1_select= GradientBoostingRegressor(**params).fit(X_rf_train, y_rf_train)\n",
    "    #mod_top1_select = SVR(kernel = 'poly',degree = 3).fit(X_rf_train,y_rf_train)\n",
    "    y_pred_3 = mod_top1_select.predict(X_rf_test)\n",
    "    \n",
    "    #No Classification model\n",
    "    mod_noClassif= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100).fit(predictors.as_matrix()[train,4:], output.as_matrix()[train,5])\n",
    "    y_pred_noClassif =  mod_noClassif.predict(predictors.as_matrix()[test,4:])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    estimate_top1 = estimate_class10.copy()\n",
    "    estimate_top1_select = estimate_class10.copy()\n",
    "    estimate_top1_noClassif = predictors.iloc[test].copy()\n",
    "    estimate_top1[\"secondEstimate\"] = y_pred_2\n",
    "    estimate_top1_select[\"thirdEstimate\"] = y_pred_3\n",
    "    estimate_top1_noClassif[\"noClassifEstimate\"] = y_pred_noClassif\n",
    "    estimate_top1 = estimate_top1.sort_values(by= \"secondEstimate\",ascending = False).iloc[:int(0.01*len(output.iloc[test]))]\n",
    "    estimate_top1_select = estimate_top1_select.sort_values(by= \"thirdEstimate\",ascending = False).iloc[0:int(0.01*len(output.iloc[test]))]\n",
    "    estimate_top1_noClassif = estimate_top1_noClassif.sort_values(by= \"noClassifEstimate\",ascending = False).iloc[0:int(0.01*len(output.iloc[test]))]\n",
    "\n",
    "    estimation_error = len(estimate_top1.merge(output_test_top_1_precent, how='inner', on=[\"id\",\"name\",\"category\",\"device\"]))*100.0/len(output_test_top_1_precent)\n",
    "    new_top.append(estimation_error)\n",
    "    new_top_select.append(len(estimate_top1_select.merge(output_test_top_1_precent, how='inner', on=[\"id\",\"name\",\"category\",\"device\"]))*100.0/len(output_test_top_1_precent))\n",
    "    new_top_noClassif.append(len(estimate_top1_noClassif.merge(output_test_top_1_precent, how='inner', on=[\"id\",\"name\",\"category\",\"device\"]))*100.0/len(output_test_top_1_precent))\n",
    "    top_10.append(len(estimate_class10.merge(output_test_top_1_precent, how='inner', on=[\"id\",\"name\",\"category\",\"device\"]))*100.0/len(output_test_top_1_precent))\n",
    "       \n",
    "print \"Old model           : \" + str(1.0*sum(old_top)/len(old_top))\n",
    "print \"Top10%              : \" + str(1.0*sum(top_10)/len(top_10))\n",
    "print \"Top1% with classif1 : \" + str(1.0*sum(new_top)/len(new_top))\n",
    "print \"Top1% with classif2 : \" + str(1.0*sum(new_top_select)/len(new_top_select))\n",
    "print \"Top1% no classif1   : \" + str(1.0*sum(new_top_noClassif)/len(new_top_noClassif))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var_select.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(predictor_train_top_10_precent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation k-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 5\n",
    "print list(predictors.columns.values)\n",
    "np.random.seed(1)\n",
    "kf = KFold(len(predictors), n_folds=K)\n",
    "new_top = []\n",
    "for train, test in kf:\n",
    "\n",
    "    #model\n",
    "    #mod=linear_model.LinearRegression(fit_intercept=False)\\ \n",
    "    #mod=linear_model.Lasso(alpha=100,fit_intercept=False)\\\n",
    "    #mod = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0, loss='ls')\\ \n",
    "    #mod= RandomForestRegressor(max_features = 1.0/3.0,n_estimators = 100)\\\n",
    "    #mod= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100)\\\n",
    "    mod = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01,max_depth=2, random_state=0, loss='ls')\\\n",
    "    .fit(predictors.as_matrix()[train,4:], output.as_matrix()[train,5]) \n",
    "\n",
    "    y_pred =  mod.predict(predictors.as_matrix()[test,4:])\n",
    "    new_top.append(metric(y_pred,output.as_matrix()[test,5]))\n",
    "\n",
    "print 1.0*sum(new_top)/len(new_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt at boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print list(predictors.columns.values)\n",
    "K = 5\n",
    "\n",
    "for n_est in [1000,5000,10000,50000]:\n",
    "    for l_rate in [0.001,0.01,0.1]:\n",
    "        for m_depth in range(1,5):\n",
    "\n",
    "            np.random.seed(1)\n",
    "            kf = KFold(len(predictors), n_folds=K)\n",
    "            old_top = []\n",
    "            new_top = []\n",
    "            for train, test in kf:\n",
    "                #model\n",
    "                mod = GradientBoostingRegressor(n_estimators=n_est, learning_rate=l_rate,max_depth=m_depth, random_state=0, loss='ls').fit(predictors.as_matrix()[train,4:], output.as_matrix()[train,5]) \n",
    "                y_pred =  mod.predict(predictors.as_matrix()[test,4:])\n",
    "                new_top.append(metric(y_pred,output.as_matrix()[test,5]))\n",
    "\n",
    "            print \"Performance with \\tB=\"+str(n_est)+\"\\tl=\"+str(l_rate)+\"\\td=\"+ str(m_depth)+\" is: \\t\"+str(1.0*sum(new_top)/len(new_top))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt at SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "print list(predictors.columns.values)\n",
    "K = 5\n",
    "\n",
    "for ker in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "    for deg in range(5):\n",
    "        for coef in [0,1,2]:\n",
    "            for Cost in [0.001,0.01,0.1,1,10]:\n",
    "                for epsi in [0.001,0.01,0.1,1,10]:\n",
    "\n",
    "                    np.random.seed(1)\n",
    "                    kf = KFold(len(predictors), n_folds=K)\n",
    "                    old_top = []\n",
    "                    new_top = []\n",
    "                    for train, test in kf:\n",
    "                        #model\n",
    "                        #mod = SVR(kernel=ker, degree=deg,  coef0=coef, C=Cost, epsilon=epsi).fit(predictors.as_matrix()[train,4:], output.as_matrix()[train,5]) \n",
    "                        mod = SVR(kernel=ker, degree=deg,  coef0=coef, C=Cost, epsilon=epsi,max_iter=1000).fit(predictors.as_matrix()[train,4:], output.as_matrix()[train,5]) \n",
    "                        y_pred =  mod.predict(predictors.as_matrix()[test,4:])\n",
    "                        new_top.append(metric(y_pred,output.as_matrix()[test,5]))\n",
    "\n",
    "                    print \"Performance with \\tKernel=\"+ker+\"\\tdegree=\"+str(deg)+\"\\tcoef0=\"+ str(coef)+\"\\tCost=\"+str(Cost)+\"\\tepsilon=\"+ str(epsi)+\" is: \\t\"+str(1.0*sum(new_top)/len(new_top))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# CV on the size of the Classification top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print list(predictors.columns.values) \n",
    "\n",
    "K = 5\n",
    "\n",
    "cv_top_10 = []\n",
    "cv_top_1  = []\n",
    "\n",
    "for top in range(1,31):\n",
    "np.random.seed(1)\n",
    "    top_percent_classif = top\n",
    "\n",
    "    kf = KFold(len(predictors), n_folds=K)\n",
    "    old_top = []\n",
    "    new_top = []\n",
    "    top_10 = []\n",
    "    new_top_select = [] \n",
    "    for train, test in kf:\n",
    "        #base model\n",
    "        old_mod=linear_model.LinearRegression(fit_intercept=False).fit(predictors.as_matrix()[train,4:12], output.as_matrix()[train,5])\n",
    "        old_y_pred =  old_mod.predict(predictors.as_matrix()[test,4:12])\n",
    "        old_top.append(metric(old_y_pred,output.as_matrix()[test,5]))\n",
    "\n",
    "        #model to determine the top 10%   (CLASSIFICATION)\n",
    "        mod_class10= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100).fit(predictors.as_matrix()[train,4:], output.as_matrix()[train,5])\n",
    "\n",
    "        y_pred =  mod_class10.predict(predictors.as_matrix()[test,4:])\n",
    "\n",
    "        #estimate of the top 10% of the test set\n",
    "        estimate_class10 = predictors.iloc[test].copy()\n",
    "        estimate_class10[\"firstEstimate\"] = y_pred\n",
    "        #estimate_class10 = estimate_class10.sort_values(by= \"firstEstimate\",ascending = False).iloc[1:int(1.0*top_percent_classif/100.0*len(estimate_class10))]\n",
    "        estimate_class10 = estimate_class10.drop(\"firstEstimate\",1)\n",
    "        estimate_class10 = estimate_class10.sort_values(by= \"daily_avg\",ascending = False).iloc[1:int(1.0*top_percent_classif/100.0*len(estimate_class10))]\n",
    "\n",
    "\n",
    "        #top 10% of the trainning set\n",
    "        output_train_top_10_precent = output.iloc[train].copy().sort_values(by= 'cumulative_downloads_2016-02',ascending = False).iloc[1:int(1.0*top_percent_classif/100.0*len(output.iloc[train]))].drop('Unnamed: 0',1)\n",
    "        predictor_train_top_10_precent = output_train_top_10_precent.merge(predictors, how='left', on=[\"id\",\"name\",\"category\",\"device\"]).copy()\n",
    "        predictor_train_top_10_precent = predictor_train_top_10_precent.drop('cumulative_downloads_2016-02',1)\n",
    "        #predictor_train_top_10_precent = predictor_train_top_10_precent.drop('firstEstimate',1)\n",
    "\n",
    "        #This is the actual top 1% of the test set\n",
    "        output_test_top_1_precent = output.iloc[test].sort_values(by= 'cumulative_downloads_2016-02',ascending = False).iloc[1:int(0.01*len(output.iloc[test]))].copy()\n",
    "\n",
    "\n",
    "        #second model -> Regression on the top obtainned by regression\n",
    "        #mod_top1= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100).fit(predictor_train_top_10_precent.as_matrix()[:,4:], output_train_top_10_precent[\"cumulative_downloads_2016-02\"].as_matrix())\n",
    "        mod_top1= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100).fit(predictor_train_top_10_precent.as_matrix()[:,4:], np.log(output_train_top_10_precent[\"cumulative_downloads_2016-02\"].as_matrix()))\n",
    "        #mod_top1=linear_model.Lasso(alpha=100,fit_intercept=False).fit(predictor_train_top_10_precent.as_matrix()[:,4:], np.log(output_train_top_10_precent[\"cumulative_downloads_2016-02\"].as_matrix()))\n",
    "        #mod_top1=GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0, loss='ls').fit(predictor_train_top_10_precent.as_matrix()[:,4:], (output_train_top_10_precent[\"cumulative_downloads_2016-02\"].as_matrix()))\n",
    "\n",
    "        y_pred_2 =  mod_top1.predict(estimate_class10.as_matrix()[:,4:])\n",
    "\n",
    "        estimate_top1 = estimate_class10.copy()\n",
    "        estimate_top1[\"secondEstimate\"] = y_pred_2\n",
    "\n",
    "        estimate_top1 = estimate_top1.sort_values(by= \"secondEstimate\",ascending = False).iloc[1:int(0.01*len(output.iloc[test]))]\n",
    "\n",
    "        estimation_error = len(estimate_top1.merge(output_test_top_1_precent, how='inner', on=[\"id\",\"name\",\"category\",\"device\"]))*100.0/len(output_test_top_1_precent)\n",
    "        new_top.append(estimation_error)\n",
    "        top_10.append(len(estimate_class10.merge(output_test_top_1_precent, how='inner', on=[\"id\",\"name\",\"category\",\"device\"]))*100.0/len(output_test_top_1_precent))\n",
    "\n",
    "\n",
    "    cv_top_10.append(1.0*sum(top_10)/len(top_10))\n",
    "    cv_top_1.append(1.0*sum(new_top)/len(new_top))\n",
    "\n",
    "plt.plot(range(1,31),cv_top_10,label = 'top 10 estimate')\n",
    "plt.plot(range(1,31),cv_top_1,label = 'top 1 estimate')\n",
    "plt.legend(loc = 'best')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# CV on the size of the predictor set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(1)\n",
    "K = 5\n",
    "\n",
    "\n",
    "for nb_pred_to_keep in range(5,100,5):\n",
    "\n",
    "    predictors = pd.read_csv('predictors.csv').drop('Unnamed: 0', 1)\n",
    "    predictors = predictors.fillna(0)\n",
    "\n",
    "    predictors_to_drop = [i[0] for i in sorted(zip(predictors.columns[4:],ranking),key=lambda x: x[1])][nb_pred_to_keep:]\n",
    "    for col in predictors_to_drop:\n",
    "        predictors = predictors.drop(col,1)\n",
    "\n",
    "    kf = KFold(len(predictors), n_folds=K)\n",
    "    #print list(predictors.columns.values) \n",
    "    new_top_noClassif = [] \n",
    "    for train, test in kf:\n",
    "\n",
    "        output_test_top_1_precent = output.iloc[test].sort_values(by= 'cumulative_downloads_2016-02',ascending = False).iloc[1:int(0.01*len(output.iloc[test]))].copy()\n",
    "\n",
    "\n",
    "        #No Classification model\n",
    "        mod_noClassif= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100).fit(predictors.as_matrix()[train,4:], output.as_matrix()[train,5])\n",
    "        y_pred_noClassif =  mod_noClassif.predict(predictors.as_matrix()[test,4:])\n",
    "\n",
    "        estimate_top1_noClassif = predictors.iloc[test].copy()\n",
    "        estimate_top1_noClassif[\"noClassifEstimate\"] = y_pred_noClassif\n",
    "        estimate_top1_noClassif = estimate_top1_noClassif.sort_values(by= \"noClassifEstimate\",ascending = False).iloc[1:int(0.01*len(output.iloc[test]))]\n",
    "\n",
    "        new_top_noClassif.append(len(estimate_top1_noClassif.merge(output_test_top_1_precent, how='inner', on=[\"id\",\"name\",\"category\",\"device\"]))*100.0/len(output_test_top_1_precent))\n",
    "        \n",
    "\n",
    "    print \"Top1% no classif with \"+str(nb_pred_to_keep)+\" kept predictors : \" + str(1.0*sum(new_top_noClassif)/len(new_top_noClassif))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "RForest pred selection and CV on the number of predictors<br/>['id', 'name', 'category', 'device', 'week_1', 'week_2', 'week_3', 'week_4', 'week_5', 'week_6', 'week_7', 'week_8', 'daily_avg', 'coef_0', 'coef_1', 'coef_2', 'coef_3', 'maxStep', 'minStep', 'std', 'nb_missing', 'iphone', 'ipad', 'Productivity', 'Entertainment', 'Travel', 'Sports', 'Music', 'Shopping', 'Finance', 'Business', 'Navigation', 'Food and Drink', 'Utilities', 'Newsstand', 'Health and Fitness', 'News', 'Lifestyle', 'Medical', 'Weather', 'Games', 'Catalogs', 'Social Networking', 'Photo and Video', 'Reference', 'Books', 'Education', 'chinese', 'Unnamed: 49', 'japanese', 'other', 'english', 'korean', 'cumulative_downloads_2015-02', 'avg_review', 'var_review', 'star1', 'star2', 'star3', 'star4', 'star5', 'num_ratings', 'reviewPerDailyDownloads', 'num_versions', 'num_review', 'days_since_release', 'downloads_per_day_before', 'positive', 'negative', 'm1_min', 'm1_mean', 'm1_std', 'm1_coef_0', 'm1_coef_1', 'm1_coef_2', 'm2_max', 'm2_min', 'm2_coef_0', 'm2_coef_1', 'm2_coef_2', 'm3_min', 'm3_mean', 'm3_std', 'm3_coef_0', 'm3_coef_1', 'm3_coef_2', 'm4_max', 'm4_min', 'm4_mean', 'm4_std', 'm4_coef_0', 'm4_coef_1', 'm4_coef_2', 'rev_coef_0', 'rev_max', 'rev_min', 'rev_std', 'rev_coef_1', 'rev_coef_2'] <br/>\n",
    "Top1% no classif with 5 kept predictors : 62.5396825397    <br/>\n",
    "Top1% no classif with 10 kept predictors : 65.0793650794<br/>\n",
    "Top1% no classif with 15 kept predictors : 65.3968253968<br/>\n",
    "Top1% no classif with 20 kept predictors : 64.7619047619<br/>\n",
    "Top1% no classif with 25 kept predictors : 64.7619047619<br/>\n",
    "Top1% no classif with 30 kept predictors : 65.3968253968<br/>\n",
    "Top1% no classif with 35 kept predictors : 66.0317460317<br/>\n",
    "Top1% no classif with 40 kept predictors : 65.7142857143<br/>\n",
    "Top1% no classif with 45 kept predictors : 65.0793650794<br/>\n",
    "Top1% no classif with 50 kept predictors : 65.3968253968<br/>\n",
    "Top1% no classif with 55 kept predictors : 65.0793650794<br/>\n",
    "Top1% no classif with 60 kept predictors : 65.3968253968<br/>\n",
    "Top1% no classif with 65 kept predictors : 64.4444444444<br/>\n",
    "Top1% no classif with 70 kept predictors : 65.3968253968<br/>\n",
    "Top1% no classif with 75 kept predictors : 63.8095238095<br/>\n",
    "Top1% no classif with 80 kept predictors : 63.4920634921<br/>\n",
    "Top1% no classif with 85 kept predictors : 64.4444444444<br/>\n",
    "Top1% no classif with 90 kept predictors : 61.9047619048<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old_top = []\n",
    "new_top = []\n",
    "\n",
    "test_frac = 0.31  #Fraction of test points\n",
    "N = 20   #number of iterations\n",
    "np.random.seed(1)\n",
    "for i in range(1,N):\n",
    "    r = np.random.randint(1,429496729)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors.as_matrix()[:,4:12], output.as_matrix()[:,5], test_size=test_frac, random_state=r)\n",
    "    X_test = X_test[0:10000]\n",
    "    y_test = y_test[0:10000]\n",
    "    old_mod=linear_model.LinearRegression(fit_intercept=False).fit(X_train,y_train)\n",
    "    old_y_pred =  old_mod.predict(X_test)\n",
    "    old_top.append(metric(old_y_pred,y_test))\n",
    "    \n",
    "np.random.seed(1)\n",
    "for i in range(1,N):\n",
    "    r = np.random.randint(1,429496729)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors.as_matrix()[:,4:], output.as_matrix()[:,5], test_size=test_frac, random_state=r)\n",
    "    X_test = X_test[0:10000]\n",
    "    y_test = y_test[0:10000]\n",
    "    #mod=linear_model.LinearRegression(fit_intercept=False).fit(X_train,y_train)\n",
    "    #mod=linear_model.Lasso(alpha=100,fit_intercept=False).fit(X_train,y_train)\n",
    "    #mod= RandomForestRegressor(max_features = 1.0/3.0,n_estimators = 100).fit(X_train,y_train)\n",
    "    mod= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100).fit(X_train,y_train)\n",
    "    y_pred =  mod.predict(X_test)\n",
    "    new_top.append(metric(y_pred,y_test))\n",
    "if (N<300):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(range(1,N),old_top,label=\"old\")\n",
    "    plt.plot(range(1,N),new_top,label=\"performance\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "print 1.0*sum(old_top)/len(old_top)\n",
    "print list(predictors.columns.values)\n",
    "print 1.0*sum(new_top)/len(new_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "54.8947368421  \n",
    "55.0526315789  #with #missing and Lasso <br/>\n",
    "54.7894736842  #including raw ratings Lasso :( <br/>\n",
    "55.0526315789  #with #missing weightedSumRatings and Lasso <br/>\n",
    "55.1052631579  #adding the categories <br/>\n",
    "55.3684210526  #adding average sentiment score and positive/negative label <br/>\n",
    "55.5263157895  #adding coeficients of usages\n",
    "\n",
    "\n",
    "With 10000 only:\n",
    "54.8947368421 all\n",
    "\n",
    "56.5263157895 on Lasso all but 'rev_coef_i'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso All alpha = 100\n",
    "['id', 'name', 'category', 'device', 'week_1', 'week_2', 'week_3', 'week_4', 'week_5', 'week_6', 'week_7', 'week_8', 'daily_avg', 'coef_0', 'coef_1', 'coef_2', 'coef_3', 'maxStep', 'minStep', 'std', 'nb_missing', 'iphone', 'ipad', 'Productivity', 'Entertainment', 'Travel', 'Sports', 'Music', 'Shopping', 'Finance', 'Business', 'Navigation', 'Food and Drink', 'Utilities', 'Newsstand', 'Health and Fitness', 'News', 'Lifestyle', 'Medical', 'Weather', 'Games', 'Catalogs', 'Social Networking', 'Photo and Video', 'Reference', 'Books', 'Education', 'avg_review', 'var_review', 'star1', 'star2', 'star3', 'star4', 'star5', 'positive', 'negative', 'm1_coef_0', 'm1_coef_1', 'm1_coef_2', 'm2_coef_0', 'm2_coef_1', 'm2_coef_2', 'm3_coef_0', 'm3_coef_1', 'm3_coef_2', 'm4_coef_0', 'm4_coef_1', 'm4_coef_2']\n",
    "57.1052631579"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictor selection, pick the top 50 features selected in the forward recursive selection with Lasso and then do random forest\n",
    "\n",
    "54.6842105263\n",
    "['id', 'name', 'category', 'device', 'week_1', 'week_2', 'week_3', 'week_4', 'week_5', 'week_6', 'week_7', 'week_8', 'daily_avg', 'coef_0', 'coef_1', 'coef_2', 'coef_3', 'maxStep', 'minStep', 'std', 'nb_missing', 'iphone', 'ipad', 'Productivity', 'Entertainment', 'Sports', 'Music', 'Shopping', 'Finance', 'Business', 'Navigation', 'Food and Drink', 'Utilities', 'News', 'Lifestyle', 'Medical', 'Weather', 'Games', 'Catalogs', 'Social Networking', 'Photo and Video', 'Reference', 'Education', 'avg_review', 'var_review', 'star2', 'star3', 'star4', 'positive', 'negative', 'm2_coef_0', 'm3_coef_0', 'm3_coef_1', 'm4_coef_2']\n",
    "57.3157894737"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Estimation of alpha for the Lasso regression \"cros validation\" like approach\n",
    "for a in np.arange(90,110,5):\n",
    "\n",
    "    old_top = []\n",
    "    new_top = []\n",
    "\n",
    "    test_frac = 0.31  #Fraction of test points\n",
    "    N = 20   #number of iterations\n",
    "\n",
    "\n",
    "    np.random.seed(1)\n",
    "    for i in range(1,N):\n",
    "        r = np.random.randint(1,429496729)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(predictors.as_matrix()[:,4:], output.as_matrix()[:,5], test_size=test_frac, random_state=r)\n",
    "        X_test = X_test[0:10000]\n",
    "        y_test = y_test[0:10000]\n",
    "        mod=linear_model.Lasso(alpha=a,fit_intercept=False).fit(X_train,y_train) \n",
    "        y_pred =  mod.predict(X_test)\n",
    "        new_top.append(metric(y_pred,y_test))\n",
    "\n",
    "\n",
    "    print a\n",
    "    print 1.0*sum(new_top)/len(new_top)\n",
    "    print \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Classification on top 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = pd.read_csv('predictors.csv').drop('Unnamed: 0', 1)\n",
    "predictors = predictors.fillna(0)\n",
    "\n",
    "thr = predictors.sort_values(by= 'week_8',ascending = False).iloc[int(0.1*len(predictors))][\"week_8\"]\n",
    "predictors  = predictors[predictors.week_8 >= thr]\n",
    "\n",
    "\n",
    "len(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_classification_10 = output\n",
    "output_classification_50 = output\n",
    "output_classification_100 = output\n",
    "#Is top ?\n",
    "def generator_istop(inp,threshold):\n",
    "    if (inp[5] >= threshold):\n",
    "        return 'True'\n",
    "    else:\n",
    "        return 'False'\n",
    "\n",
    "threshold = output.sort_values('cumulative_downloads_2016-02',ascending = False).iloc[len(output)/10]['cumulative_downloads_2016-02']\n",
    "output_classification_10[\"is_top\"] = output.apply(generator_istop,axis = 1,args=(threshold,))\n",
    "\n",
    "threshold = output.sort_values('cumulative_downloads_2016-02',ascending = False).iloc[len(output)/50]['cumulative_downloads_2016-02']\n",
    "output_classification_50[\"is_top\"] = output.apply(generator_istop,axis = 1,args=(threshold,))\n",
    "\n",
    "threshold = output.sort_values('cumulative_downloads_2016-02',ascending = False).iloc[len(output)/100]['cumulative_downloads_2016-02']\n",
    "output_classification_100[\"is_top\"] = output.apply(generator_istop,axis = 1,args=(threshold,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_classification_10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "K=10\n",
    "kf = KFold(32339, n_folds=K)\n",
    "old_top = []\n",
    "new_top = []\n",
    "for train, test in kf:\n",
    "    #model\n",
    "    mod = SVC(C=10.0, kernel='poly', degree=3, gamma='auto', coef0=2.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=10, decision_function_shape=None, random_state=None).fit(predictors.as_matrix()[train,4:],output_classification_10.as_matrix()[train,6])\n",
    "    #mod = linear_model.LogisticRegression().fit(predictors.as_matrix()[train,4:], output_classification_10.as_matrix()[train,6])\n",
    "    y_pred =  mod.predict(predictors.as_matrix()[test,4:])\n",
    "    new_top.append(metric_classification(y_pred,output_classification_10.as_matrix()[test,6]))\n",
    "print list(predictors.columns.values)\n",
    "print 1.0*sum(new_top)/len(new_top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###This metric is not very good, put True everywhere and get 100%\n",
    "def metric_classification(y_pred,y_test):\n",
    "    nb_top = 0\n",
    "    for i in range(len(y_test)):\n",
    "        if (y_pred[i]=='True' and y_test[i]==\"False\"):\n",
    "            nb_top+=1\n",
    "    return 100.0*nb_top/len(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(y_pred).count(\"False\")*1.0/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print list(y_pred).count('True')\n",
    "print list(output_classification_10.as_matrix()[test,6]).count('True')\n",
    "print len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Question, what if instead of using the 'cumulative_downloads_2016-02' we used exp('cumulative_downloads_2016-02') to try to shrunk the lowest points?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_scaled = output\n",
    "#Is top ?\n",
    "def generator_outputScaler(inp):\n",
    "    #return math.exp(inp[5]*1.0/12329752*100)   #28%\n",
    "    #return 1.0/(20000000-inp[5])   #0.618429189858\n",
    "    #return inp[5]**2       #53.8045828797\n",
    "    #return (inp[5]*1.0/20000000)**2*inp[5]     #46.3834326015\n",
    "    #return (inp[5]*1.0/1500000)**0.5   #  56.5875142341\n",
    "    #return 100.0/(1+math.exp(-(inp[5]-1500000)*1.0/100000))    #54.4230120696\n",
    "    #return 200.0/(1+math.exp(-(inp[5]-1500000)*1.0/1000000)) - 100    #55.9691806875\n",
    "    #return math.log(inp[5])      #58.7522076851\n",
    "    return inp[5]               #58.4429930902\n",
    "    #return 200.0/(1+math.exp(-(inp[5]*1.0/1500000-1)*10)) - 100       #55.3506558544\n",
    "    #return math.exp(1.0/(1+math.exp(-(inp[5]*1.0/1500000-1)*10)))     #\n",
    "\n",
    "\n",
    "output_scaled[\"scaled_downloads\"] = output.apply(generator_outputScaler,axis = 1)\n",
    "\n",
    "#output_scaled[\"scaled_downloads\"] =(output['cumulative_downloads_2016-02']-output['cumulative_downloads_2016-02'].mean())/output['cumulative_downloads_2016-02'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_scaled.sort_values(by= 'cumulative_downloads_2016-02',ascending = False).head(320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(list(output_scaled.sort_values(by= 'cumulative_downloads_2016-02',ascending = False)[\"scaled_downloads\"]))\n",
    "plt.axis([0, 300,min(list(output_scaled.sort_values(by= 'cumulative_downloads_2016-02',ascending = False)[\"scaled_downloads\"])),max(list(output_scaled.sort_values(by= 'cumulative_downloads_2016-02',ascending = False)[\"scaled_downloads\"]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 10\n",
    "\n",
    "kf = KFold(32339, n_folds=K)\n",
    "old_top = []\n",
    "new_top = []\n",
    "for train, test in kf:\n",
    "    #base model\n",
    "    old_mod=linear_model.LinearRegression(fit_intercept=False).fit(predictors.as_matrix()[train,4:12], output.as_matrix()[train,5])\n",
    "    old_y_pred =  old_mod.predict(predictors.as_matrix()[test,4:12])\n",
    "    old_top.append(metric(old_y_pred,output.as_matrix()[test,5]))\n",
    "    #model\n",
    "    #mod=linear_model.LinearRegression(fit_intercept=False).fit(predictors.as_matrix()[train,4:], output_scaled.as_matrix()[train,6])\n",
    "    #mod=linear_model.Lasso(alpha=100,fit_intercept=False).fit(predictors.as_matrix()[train,4:], output_scaled.as_matrix()[train,6])\n",
    "    #mod = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0, loss='ls').fit(predictors.as_matrix()[train,4:], output_scaled.as_matrix()[train,6]) \n",
    "    #mod= RandomForestRegressor(max_features = 1.0/3.0,n_estimators = 100).fit(predictors.as_matrix()[train,4:], output_scaled.as_matrix()[train,6])\n",
    "    mod= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100).fit(predictors.as_matrix()[train,4:], output_scaled.as_matrix()[train,6])\n",
    "    y_pred =  mod.predict(predictors.as_matrix()[test,4:])\n",
    "    new_top.append(metric(y_pred,output_scaled.as_matrix()[test,6]))\n",
    "print 1.0*sum(old_top)/len(old_top)\n",
    "print list(predictors.columns.values)\n",
    "print 1.0*sum(new_top)/len(new_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(list(output.sort_values(by= 'cumulative_downloads_2016-02',ascending = False)[\"cumulative_downloads_2016-02\"]))\n",
    "plt.axis([0, 3200, 0, 13000000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(list(output_scaled.sort_values(by= 'cumulative_downloads_2016-02',ascending = False)[\"scaled_downloads\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(set(lang_series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors.sort_values(by= 'week_8',ascending = False)[0:3233].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output.sort_values(by= 'cumulative_downloads_2016-02',ascending = False)[0:323].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(predictors.sort_values(by= 'week_8',ascending = False)[0:3233].merge(output.sort_values(by= 'cumulative_downloads_2016-02',ascending = False)[0:323], how='inner', on=[\"id\",\"name\",\"category\",\"device\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = predictors[:30]\n",
    "downloads = downloads[:30]\n",
    "reviews = reviews[:30]\n",
    "ratings = ratings[:30]\n",
    "usages = usages[:30]\n",
    "revenues = revenues[:30]\n",
    "output =  output[:30]\n",
    "dateRange = dateRange[:30]\n",
    "prev_downloads = prev_downloads[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = reviews[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "estimate_top1 = estimate_class10.copy()\n",
    "estimate_top1[\"secondEstimate\"] = np.exp(y_pred_2).astype(int)\n",
    "estimate_top1 = estimate_top1.merge(output, how='left', on=[\"id\",\"name\",\"category\",\"device\"]).copy()\n",
    "estimate_top1 = estimate_top1.drop(\"Unnamed: 0\",1)\n",
    "estimate_top1.sort_values(by= 'cumulative_downloads_2016-02',ascending = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
