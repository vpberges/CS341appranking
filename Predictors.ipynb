{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this markdown to generate the predictors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imported libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import langdetect\n",
    "import datetime\n",
    "%matplotlib inline  \n",
    "from sklearn.svm import SVR\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "#Creating of the input data\n",
    "downloads = pd.read_csv('train_app_downloads.csv')\n",
    "reviews = pd.read_csv('train_app_review.csv')\n",
    "ratings = pd.read_csv('train_app_rating.csv')\n",
    "usages = pd.read_csv('train_usage.csv')\n",
    "revenues = pd.read_csv('train_revenue.csv')\n",
    "output = pd.read_csv('train_final_downloads.csv')\n",
    "prev_downloads = pd.read_csv('train_cumulative_downloads_2015-02.csv').drop('Unnamed: 0', 1)\n",
    "dateRange = pd.date_range('2015-03-01', periods=56).format(formatter=lambda x: x.strftime('%Y-%m-%d'))\n",
    "sentiment = pd.read_csv('sentiment.csv',header=-1).ix[:,0]\n",
    "sentiment.values[sentiment.values==0.0] = 0.5\n",
    "reviews[\"sentiment_score\"] = sentiment.values \n",
    "\n",
    "#We map -1 to 0 in the downloads (there are no 0 in the initial data)\n",
    "replacementValue=0\n",
    "#downloads = downloads.replace(-1,replacementValue)\n",
    "imputer = Imputer(missing_values=-1, strategy='median', axis=0)\n",
    "name = downloads.columns\n",
    "downloads = pd.concat([downloads.ix[:,0:5],pd.DataFrame(imputer.fit_transform(downloads.ix[:,5:]))],axis = 1)\n",
    "downloads.columns = name \n",
    "\n",
    "name = usages.columns\n",
    "usages = pd.concat([usages.ix[:,0:6],pd.DataFrame(imputer.fit_transform(usages.ix[:,6:]))],axis = 1)\n",
    "usages.columns = name \n",
    "\n",
    "name = revenues.columns\n",
    "revenues = pd.concat([revenues.ix[:,0:5],pd.DataFrame(imputer.fit_transform(revenues.ix[:,5:]))],axis = 1)\n",
    "revenues.columns = name \n",
    "\n",
    "\n",
    "\n",
    "#Minor corrections\n",
    "ratings = ratings.rename(columns={'start1': 'star1'})\n",
    "ratings = pd.merge(downloads.drop(dateRange,1), ratings.drop('Unnamed: 0', 1), how='left',\n",
    "                   on=[\"id\",\"name\",\"category\"]).replace(\"NaN\",replacementValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the predictors matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = pd.concat([downloads[\"id\"],downloads[\"name\"],downloads[\"category\"],downloads[\"device\"]],\n",
    "                       axis=1,keys=[\"id\",\"name\",\"category\",\"device\"])\n",
    "predictors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the train_app_downloads.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate the logWeekly average\n",
    "def generator_weekAvg(inp,w):\n",
    "    if (np.count_nonzero(inp[5+w*7:12+w*7] - replacementValue*np.ones(len(inp[5+w*7:12+w*7]))) == 0):\n",
    "        return 0\n",
    "    return  math.log(1.0*sum(inp[5+w*7:12+w*7])/np.count_nonzero(inp[5+w*7:12+w*7] \n",
    "                                                                 - replacementValue*np.ones(len(inp[5+w*7:12+w*7]))))\n",
    "\n",
    "for w in range(8):\n",
    "    predictors[\"week_\"+str(w+1)] = downloads.apply(generator_weekAvg,axis=1,args=(w,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate the daily average\n",
    "def generator_dailyAvg(inp):\n",
    "    if (np.count_nonzero(inp[5:] - replacementValue*np.ones(len(inp[5:]))) == 0):\n",
    "        return 0\n",
    "    return  (1.0*sum(inp[5:])/np.count_nonzero(inp[5:] - replacementValue*np.ones(len(inp[5:]))))\n",
    "    #return  math.log(1.0*sum(inp[5:])/len(inp[5:]))\n",
    "\n",
    "predictors[\"daily_avg\"] = downloads.apply(generator_dailyAvg,axis=1)\n",
    "predictors[\"download_sum\"] = downloads.ix[:,5:].sum(axis=1)\n",
    "#This one is bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate the polynomial coefficients\n",
    "def generator_coef(inp, coef):\n",
    "    return  np.polyfit(range(56),np.cumsum(inp[5:]),coef)[0]\n",
    "#Redo by ignoring the -1 / 0 ? w = [1110011]     w = np.not_equal(inp[5:],np.ones(len(inp[5:]))*replacementValue\n",
    "\n",
    "for c in range(4):\n",
    "    predictors[\"coef_\"+str(c)] = downloads.apply(generator_coef,axis=1,args=(c,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate the step max and min (we ignore the values of 0)\n",
    "def generator_maxStep(inp,maximum):\n",
    "    if (np.count_nonzero(inp[5:] - replacementValue*np.ones(len(inp[5:]))) == 0):\n",
    "        return 0\n",
    "    m = 0\n",
    "    for d in range(1,56):\n",
    "        if (inp[5+d]!=replacementValue and inp[4+d]!=replacementValue):\n",
    "            c = (inp[5+d]-inp[4+d])\n",
    "            if (maximum and m < c):\n",
    "                m = c\n",
    "            if ( not maximum and m > c):\n",
    "                m = c\n",
    "    return m\n",
    "                \n",
    "predictors[\"maxStep\"] = downloads.apply(generator_maxStep,axis=1,args=(True,))\n",
    "predictors[\"minStep\"] = downloads.apply(generator_maxStep,axis=1,args=(False,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Standard Deviation\n",
    "def generator_std(inp):\n",
    "    if (np.count_nonzero(inp[5:] - replacementValue*np.ones(len(inp[5:]))) == 0):\n",
    "        return 0\n",
    "    return np.std(inp[5:])\n",
    "\n",
    "predictors[\"std\"] = downloads.apply(generator_std,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Number of missing Values\n",
    "def generator_missing(inp):\n",
    "    return list(inp[5:]).count(replacementValue)\n",
    "    #return np.count_zero(inp[5:] -replacementValue*np.ones(len(inp[5:])))\n",
    "\n",
    "predictors[\"nb_missing\"] = downloads.apply(generator_missing,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#device\n",
    "def generator_iphone(inp):\n",
    "    if (inp[4] == \"iphone\"):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def generator_ipad(inp):\n",
    "    if(inp[4] == \"ipad\"):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "predictors[\"iphone\"] = downloads.apply(generator_iphone,axis = 1)\n",
    "predictors[\"ipad\"] = downloads.apply(generator_ipad,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Categories\n",
    "def generator_categories(inp,cat):\n",
    "    if (inp[3] == cat):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "for cat in list(set(downloads[\"category\"])):\n",
    "    predictors[cat] = downloads.apply(generator_categories,axis = 1,args=(cat,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def detect_language(x):\n",
    "    try:\n",
    "        detected = langdetect.detect(x.decode('utf8','ignore'))\n",
    "        if detected in ['ja']:\n",
    "            return 'japanese'\n",
    "        elif detected in ['zh-cn']:\n",
    "            return 'chinese'\n",
    "        elif detected in ['ko']:\n",
    "            return 'korean'\n",
    "        elif detected in ['en']:\n",
    "            return 'english'\n",
    "        return 'other'\n",
    "    except:\n",
    "        return 'other'\n",
    "        #return None\n",
    "\n",
    "def set_lang_categories(x, cat):\n",
    "    if x == cat:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "lang_series = predictors['name'].apply(detect_language)\n",
    "for cat in list(set(lang_series)):\n",
    "    predictors[cat] = lang_series.apply(set_lang_categories, args=(cat,))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the train_cumulative_downloads_2015-02.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#prev_downloads = pd.read_csv('train_cumulative_downloads_2015-02.csv').drop('Unnamed: 0', 1)\n",
    "predictors = pd.merge(predictors, prev_downloads, how='left',\n",
    "                  on=[\"id\",\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors['dl_projection'] = predictors['cumulative_downloads_2015-02'] + 7 * predictors['download_sum']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the train_app_rating.csv file and reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_reviews = reviews.groupby('id').agg('mean')\n",
    "avg_reviews['rating']\n",
    "predictors['avg_review'] = predictors.join(avg_reviews['rating'],on='id')['rating'].replace(\"NaN\",0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_reviews = reviews.groupby('id').agg('var')\n",
    "avg_reviews['rating']\n",
    "predictors['var_review'] = predictors.join(avg_reviews['rating'],on='id')['rating'].replace(\"NaN\",0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratings['num_ratings'] = ratings.ix[:,['star1','star2','star3','star4','star5']].sum(axis=1)\n",
    "#Scaling the ratings\n",
    "for i in range(1,6):\n",
    "    ratings['star'+str(i)]=ratings['star'+str(i)].divide(ratings['num_ratings']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#raw ratings\n",
    "predictors = pd.merge(predictors, ratings.drop('Unnamed: 0', 1), how='left',\n",
    "                   on=[\"id\",\"name\",\"category\",\"device\"]).replace(\"NaN\",replacementValue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reviews per daily downloads\n",
    "#predictors['reviewPerDailyDownloads'] = predictors['num_ratings'].divide(predictors['daily_avg']+1)\n",
    "predictors['ratings_per_daily_downloads'] = predictors['num_ratings'].divide(predictors['cumulative_downloads_2015-02']+1)\n",
    "\n",
    "#Drop either this or num_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_versions = reviews.groupby('id').version.nunique()\n",
    "num_versions.name = 'num_versions'\n",
    "predictors = predictors.join(num_versions, how='left', on='id')\n",
    "predictors['num_versions'] = predictors['num_versions'].replace('NaN',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_review = reviews.groupby('id').rating.count()\n",
    "num_review.name = 'num_review'\n",
    "predictors = predictors.join(num_review, how='left', on='id')\n",
    "predictors['num_review_per_daily_downloads'] = predictors['num_review'].replace('NaN',0).divide(predictors['cumulative_downloads_2015-02']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''reviews_withLang = reviews.copy()\n",
    "reviews_lang = reviews['text'].apply(detect_language)\n",
    "for cat in list(set(reviews_lang)):\n",
    "    reviews_withLang[cat] = reviews_lang.apply(set_lang_categories, args=(cat,))\n",
    "\n",
    "reviews_withLang = reviews_withLang.groupby('id')[list(set(reviews_lang))].sum().reset_index()\n",
    "\n",
    "def gini_impurity(inp):\n",
    "    tot = sum(inp[1:])\n",
    "    return sum([1.0*x / tot * (1 - 1.0*x / tot) for x in inp[1:]])\n",
    "\n",
    "reviews_withLang[\"gini_reviews\"] = reviews_withLang.apply(gini_impurity,axis=1)\n",
    "predictors[\"gini_reviews\"] = predictors.merge(reviews_withLang[[\"id\",\"gini_reviews\"]], how='left', on=\"id\")[\"gini_reviews\"]\\\n",
    ".replace(\"NaN\",replacementValue)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the train_release_date.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_days_since_release(x):\n",
    "    return (datetime.datetime.strptime('03/01/2015', '%m/%d/%Y').date() - datetime.datetime.strptime(x, '%Y-%m-%d').date()).days\n",
    "\n",
    "release_date = pd.read_csv('train_release_date.csv').drop('Unnamed: 0', 1)\n",
    "release_date['days_since_release'] = release_date['release_date'].apply(generate_days_since_release)\n",
    "predictors = pd.merge(predictors, release_date.drop('release_date', 1), how='left',\n",
    "                  on=[\"id\",\"name\"])\n",
    "\n",
    "# replacing any missing values with 0; not a good idea but placeholder for now\n",
    "predictors['days_since_release'] = predictors['days_since_release'].replace('NaN', 0)\n",
    "\n",
    "# The number of downloads divided by time\n",
    "predictors['downloads_per_day_before'] = predictors['cumulative_downloads_2015-02'].divide(predictors['days_since_release']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "#Generate the polynomial coefficients\n",
    "def generator_coef(inp, coef):\n",
    "    tmp = list(inp.values)\n",
    "    #print tmp[60:61]\n",
    "    #print int(tmp[59:60][0])\n",
    "    Y = np.array(tmp[60:61]*(int(tmp[59:60][0])/7)+tmp[4:12])\n",
    "    X = range(Y.shape[0])\n",
    "    return  np.polyfit(X,Y,coef)[0]\n",
    "#Redo by ignoring the -1 / 0 ? w = [1110011]     w = np.not_equal(inp[5:],np.ones(len(inp[5:]))*replacementValue\n",
    "\n",
    "for c in range(4):\n",
    "    predictors[\"coef_\"+str(c)] = predictors.apply(generator_coef,axis=1,args=(c,))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Later we can compute the weighted average of sentiment scores based on reviewers.\n",
    "#add positive and negative columns to indicate the app's popularity\n",
    "\n",
    "#print reviews.ix[id==predictors.id[0],:][\"sentiment_score\"]\n",
    "#print reviews.ix[id==predictors.id[1],:][\"sentiment_score\"]\n",
    "\n",
    "avg_score = [0.5]*predictors.shape[0]\n",
    "predictors[\"positive\"] = [0]*predictors.shape[0]\n",
    "predictors[\"negative\"] = [0]*predictors.shape[0]\n",
    "\n",
    "reviewers = {}\n",
    "for i in range(reviews.shape[0]):\n",
    "    user = reviews['reviewer'].values[i]\n",
    "    item_id = reviews['id'].values[i]\n",
    "    if user in reviewers:\n",
    "        reviewers[user] = reviewers[user] + predictors[\"week_8\"][predictors.id==item_id].values.sum()\n",
    "    else:\n",
    "        reviewers[user] = predictors[\"week_8\"][predictors.id==item_id].values.sum()\n",
    "        \n",
    "for i in range(predictors.shape[0]):\n",
    "    score_list = list(reviews.ix[reviews[\"id\"]==predictors.id[i],:]['sentiment_score'].values)\n",
    "    reviewer_list =  list(reviews.ix[reviews[\"id\"]==predictors.id[i],:]['reviewer'].values)\n",
    "    s = 0\n",
    "    rs = 0 \n",
    "    if len(score_list)==0: continue\n",
    "    for j in range(len(score_list)):\n",
    "        s = s + score_list[j]*reviewers[reviewer_list[j]]\n",
    "        rs = rs + reviewers[reviewer_list[j]]\n",
    "    avg_score[i] = s/rs\n",
    "    #avg_score[i] = reviews.ix[reviews[\"id\"]==predictors.id[i],:][\"sentiment_score\"].mean()\n",
    "    if avg_score[i]>0.55: \n",
    "        predictors[\"positive\"].values[i] = 1\n",
    "    elif avg_score[i]<0.45: predictors[\"negative\"].values[i] = 1\n",
    "predictors[\"avg_sentiment_score\"] = avg_score\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use coeficients of metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    predictors[\"m\"+str(i+1)+\"_max\"] = np.zeros(predictors.shape[0]) \n",
    "    #predictors[\"m\"+str(i+1)+\"_min\"] = np.zeros(predictors.shape[0])\n",
    "    predictors[\"m\"+str(i+1)+\"_mean\"] = np.zeros(predictors.shape[0])\n",
    "    #predictors[\"m\"+str(i+1)+\"_std\"] = np.zeros(predictors.shape[0])\n",
    "    for j in range(3):\n",
    "        predictors[\"m\"+str(i+1)+\"_coef_\"+str(j)] = np.zeros(predictors.shape[0])\n",
    "\n",
    "for i in range(predictors.shape[0]):\n",
    "    if predictors[\"id\"].values[i] not in usages[\"id\"].values: continue\n",
    "    for j in range(4):\n",
    "        tmp = usages.ix[usages[\"id\"] == predictors[\"id\"].values[i],:]\n",
    "        time_series = np.array(tmp.ix[tmp[\"metric\"] == j+1,6:14])[0]   \n",
    "        if -1 in time_series: continue\n",
    "        predictors[\"m\"+str(j+1)+\"_max\"].values[i] = time_series.max()\n",
    "        predictors[\"m\"+str(j+1)+\"_mean\"].values[i] = time_series.mean()\n",
    "        X = np.array(range(8))\n",
    "        fit = np.polyfit(X,time_series,2)\n",
    "        for k in range(3):\n",
    "            predictors[\"m\"+str(j+1)+\"_coef_\"+str(k)].values[i] = fit[2-k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use coeficients of revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for j in range(3):\n",
    "    predictors[\"rev_coef_\"+str(j)] = np.zeros(predictors.shape[0])\n",
    "    predictors[\"rev_max\"] = np.zeros(predictors.shape[0])\n",
    "    #predictors[\"rev_min\"] = np.zeros(predictors.shape[0])\n",
    "    predictors[\"rev_mean\"] = np.zeros(predictors.shape[0])\n",
    "    #predictors[\"rev_std\"] = np.zeros(predictors.shape[0])\n",
    "for i in range(predictors.shape[0]): \n",
    "    if predictors[\"id\"].values[i] in revenues[\"id\"].values: \n",
    "        curr_rev  = revenues.ix[revenues[\"id\"]== predictors[\"id\"].values[i],:]\n",
    "        if predictors[\"device\"].values[i] in curr_rev[\"device\"].values:\n",
    "            curr_rev = curr_rev.ix[curr_rev[\"device\"] == predictors[\"device\"].values[i],:]\n",
    "            time_series = np.array(curr_rev.ix[:,5:61])[0]\n",
    "            if -1 in time_series: continue \n",
    "            predictors[\"rev_max\"].values[i] = time_series.max()\n",
    "            predictors[\"rev_mean\"].values[i] = time_series.mean()\n",
    "            X = np.array(range(56))\n",
    "            fit = np.polyfit(X,time_series,2)\n",
    "            for k in range(3):  predictors[\"rev_coef_\"+str(k)].values[i] = fit[2-k]            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Smart Standardization accross the categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat_mean = predictors.groupby('category').agg('mean').reset_index()\n",
    "cat_var = predictors.groupby('category').agg('var').reset_index()\n",
    "cat_std = cat_mean.merge(cat_var,on = \"category\",suffixes=('_mean', '_var'))\n",
    "\n",
    "def smart_standardize(inp):\n",
    "    temp = predictors[[\"id\",\"name\",\"category\",\"device\",inp]].merge(cat_std[[\"category\",inp+\"_mean\",inp+\"_var\"]],how=\"left\",on=\"category\")\n",
    "    return 1.0*(predictors[inp] - temp[inp+\"_mean\"])/(temp[inp+\"_var\"]+1)\n",
    "\n",
    "\n",
    "'''\n",
    "for i in range(1,6):\n",
    "    predictors[\"star\"+str(i)] = smart_standardize(\"star\"+str(i))\n",
    "    \n",
    "for j in range(3):\n",
    "    predictors[\"rev_coef_\"+str(j)]=  smart_standardize(\"rev_coef_\"+str(j))\n",
    "for i in range(4):\n",
    "    predictors[\"m\"+str(i+1)+\"_max\"]  = smart_standardize(\"m\"+str(i+1)+\"_max\") \n",
    "    predictors[\"m\"+str(i+1)+\"_min\"]  = smart_standardize(\"m\"+str(i+1)+\"_min\")\n",
    "    predictors[\"m\"+str(i+1)+\"_mean\"]  = smart_standardize(\"m\"+str(i+1)+\"_mean\")\n",
    "    predictors[\"m\"+str(i+1)+\"_std\"]   = smart_standardize(\"m\"+str(i+1)+\"_std\")\n",
    "for j in range(3):\n",
    "        predictors[\"m\"+str(i+1)+\"_coef_\"+str(j)]  = smart_standardize(\"m\"+str(i+1)+\"_coef_\"+str(j))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## To csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors.to_csv(\"predictors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = pd.read_csv('predictors.csv').drop('Unnamed: 0', 1)\n",
    "predictors = predictors.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>device</th>\n",
       "      <th>week_1</th>\n",
       "      <th>week_2</th>\n",
       "      <th>week_3</th>\n",
       "      <th>week_4</th>\n",
       "      <th>week_5</th>\n",
       "      <th>week_6</th>\n",
       "      <th>week_7</th>\n",
       "      <th>week_8</th>\n",
       "      <th>daily_avg</th>\n",
       "      <th>download_sum</th>\n",
       "      <th>coef_0</th>\n",
       "      <th>coef_1</th>\n",
       "      <th>coef_2</th>\n",
       "      <th>coef_3</th>\n",
       "      <th>maxStep</th>\n",
       "      <th>minStep</th>\n",
       "      <th>std</th>\n",
       "      <th>nb_missing</th>\n",
       "      <th>iphone</th>\n",
       "      <th>ipad</th>\n",
       "      <th>Productivity</th>\n",
       "      <th>Entertainment</th>\n",
       "      <th>Travel</th>\n",
       "      <th>Sports</th>\n",
       "      <th>Music</th>\n",
       "      <th>Shopping</th>\n",
       "      <th>Finance</th>\n",
       "      <th>Business</th>\n",
       "      <th>Navigation</th>\n",
       "      <th>Food and Drink</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>Newsstand</th>\n",
       "      <th>Health and Fitness</th>\n",
       "      <th>News</th>\n",
       "      <th>Lifestyle</th>\n",
       "      <th>Medical</th>\n",
       "      <th>Weather</th>\n",
       "      <th>Games</th>\n",
       "      <th>Catalogs</th>\n",
       "      <th>Social Networking</th>\n",
       "      <th>Photo and Video</th>\n",
       "      <th>Reference</th>\n",
       "      <th>Books</th>\n",
       "      <th>Education</th>\n",
       "      <th>korean</th>\n",
       "      <th>other</th>\n",
       "      <th>japanese</th>\n",
       "      <th>chinese</th>\n",
       "      <th>english</th>\n",
       "      <th>cumulative_downloads_2015-02</th>\n",
       "      <th>dl_projection</th>\n",
       "      <th>avg_review</th>\n",
       "      <th>var_review</th>\n",
       "      <th>star1</th>\n",
       "      <th>star2</th>\n",
       "      <th>star3</th>\n",
       "      <th>star4</th>\n",
       "      <th>star5</th>\n",
       "      <th>num_ratings</th>\n",
       "      <th>ratings_per_daily_downloads</th>\n",
       "      <th>num_versions</th>\n",
       "      <th>num_review</th>\n",
       "      <th>num_review_per_daily_downloads</th>\n",
       "      <th>days_since_release</th>\n",
       "      <th>downloads_per_day_before</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>m1_max</th>\n",
       "      <th>m1_mean</th>\n",
       "      <th>m1_coef_0</th>\n",
       "      <th>m1_coef_1</th>\n",
       "      <th>m1_coef_2</th>\n",
       "      <th>m2_max</th>\n",
       "      <th>m2_mean</th>\n",
       "      <th>m2_coef_0</th>\n",
       "      <th>m2_coef_1</th>\n",
       "      <th>m2_coef_2</th>\n",
       "      <th>m3_max</th>\n",
       "      <th>m3_mean</th>\n",
       "      <th>m3_coef_0</th>\n",
       "      <th>m3_coef_1</th>\n",
       "      <th>m3_coef_2</th>\n",
       "      <th>m4_max</th>\n",
       "      <th>m4_mean</th>\n",
       "      <th>m4_coef_0</th>\n",
       "      <th>m4_coef_1</th>\n",
       "      <th>m4_coef_2</th>\n",
       "      <th>rev_coef_0</th>\n",
       "      <th>rev_max</th>\n",
       "      <th>rev_mean</th>\n",
       "      <th>rev_coef_1</th>\n",
       "      <th>rev_coef_2</th>\n",
       "      <th>ratio_latest_ver</th>\n",
       "      <th>avg_sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>281704574</td>\n",
       "      <td>AIM: Chat, Free Text, Photo Share, Voice Message</td>\n",
       "      <td>Social Networking</td>\n",
       "      <td>iphone</td>\n",
       "      <td>6.239161</td>\n",
       "      <td>6.127804</td>\n",
       "      <td>6.101279</td>\n",
       "      <td>6.183265</td>\n",
       "      <td>6.078625</td>\n",
       "      <td>5.926926</td>\n",
       "      <td>5.792578</td>\n",
       "      <td>6.059791</td>\n",
       "      <td>433.678571</td>\n",
       "      <td>24286</td>\n",
       "      <td>433.678571</td>\n",
       "      <td>-2.585304</td>\n",
       "      <td>0.035204</td>\n",
       "      <td>0.003661</td>\n",
       "      <td>308</td>\n",
       "      <td>-233</td>\n",
       "      <td>91.952066</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>617121</td>\n",
       "      <td>787123</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>2.684058</td>\n",
       "      <td>0.189152</td>\n",
       "      <td>0.080590</td>\n",
       "      <td>0.145992</td>\n",
       "      <td>0.139242</td>\n",
       "      <td>0.445023</td>\n",
       "      <td>469040</td>\n",
       "      <td>0.760044</td>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>2424</td>\n",
       "      <td>254.482887</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.035065</td>\n",
       "      <td>0.032192</td>\n",
       "      <td>0.03240</td>\n",
       "      <td>-0.001056</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.216173</td>\n",
       "      <td>0.212028</td>\n",
       "      <td>0.214854</td>\n",
       "      <td>-0.000385</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>16.729371</td>\n",
       "      <td>15.190247</td>\n",
       "      <td>15.083152</td>\n",
       "      <td>-0.472173</td>\n",
       "      <td>0.100554</td>\n",
       "      <td>96931</td>\n",
       "      <td>88379.625</td>\n",
       "      <td>88130.375000</td>\n",
       "      <td>-2675.898810</td>\n",
       "      <td>549.422619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.285714</td>\n",
       "      <td>0.295056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>281922769</td>\n",
       "      <td>Mobile MIM</td>\n",
       "      <td>Medical</td>\n",
       "      <td>ipad</td>\n",
       "      <td>4.075113</td>\n",
       "      <td>4.084775</td>\n",
       "      <td>3.964886</td>\n",
       "      <td>4.198275</td>\n",
       "      <td>4.089571</td>\n",
       "      <td>4.027899</td>\n",
       "      <td>4.124828</td>\n",
       "      <td>3.991626</td>\n",
       "      <td>58.678571</td>\n",
       "      <td>3286</td>\n",
       "      <td>58.678571</td>\n",
       "      <td>-0.028161</td>\n",
       "      <td>-0.005040</td>\n",
       "      <td>-0.000418</td>\n",
       "      <td>28</td>\n",
       "      <td>-34</td>\n",
       "      <td>8.860239</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>83281</td>\n",
       "      <td>106283</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.256757</td>\n",
       "      <td>0.131532</td>\n",
       "      <td>0.172072</td>\n",
       "      <td>0.142342</td>\n",
       "      <td>0.296396</td>\n",
       "      <td>1109</td>\n",
       "      <td>0.013316</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>2424</td>\n",
       "      <td>34.342680</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.253146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>281922769</td>\n",
       "      <td>Mobile MIM</td>\n",
       "      <td>Medical</td>\n",
       "      <td>iphone</td>\n",
       "      <td>4.576183</td>\n",
       "      <td>4.532599</td>\n",
       "      <td>4.487030</td>\n",
       "      <td>4.507715</td>\n",
       "      <td>4.480578</td>\n",
       "      <td>4.464265</td>\n",
       "      <td>4.439284</td>\n",
       "      <td>4.610868</td>\n",
       "      <td>91.267857</td>\n",
       "      <td>5111</td>\n",
       "      <td>91.267857</td>\n",
       "      <td>-0.048223</td>\n",
       "      <td>0.014281</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>51</td>\n",
       "      <td>-48</td>\n",
       "      <td>12.693490</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99000</td>\n",
       "      <td>134777</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.256757</td>\n",
       "      <td>0.131532</td>\n",
       "      <td>0.172072</td>\n",
       "      <td>0.142342</td>\n",
       "      <td>0.296396</td>\n",
       "      <td>1109</td>\n",
       "      <td>0.011202</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>2424</td>\n",
       "      <td>40.824742</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.253146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>281935788</td>\n",
       "      <td>Epocrates References &amp; Tools for Healthcare Pr...</td>\n",
       "      <td>Medical</td>\n",
       "      <td>ipad</td>\n",
       "      <td>5.617810</td>\n",
       "      <td>5.557379</td>\n",
       "      <td>5.520317</td>\n",
       "      <td>5.371302</td>\n",
       "      <td>5.493649</td>\n",
       "      <td>5.418637</td>\n",
       "      <td>5.434969</td>\n",
       "      <td>5.393628</td>\n",
       "      <td>239.660714</td>\n",
       "      <td>13421</td>\n",
       "      <td>239.660714</td>\n",
       "      <td>-0.934279</td>\n",
       "      <td>0.019362</td>\n",
       "      <td>-0.000439</td>\n",
       "      <td>120</td>\n",
       "      <td>-73</td>\n",
       "      <td>31.992341</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>291496</td>\n",
       "      <td>385443</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>2.941176</td>\n",
       "      <td>0.281447</td>\n",
       "      <td>0.103090</td>\n",
       "      <td>0.140426</td>\n",
       "      <td>0.140326</td>\n",
       "      <td>0.334690</td>\n",
       "      <td>50353</td>\n",
       "      <td>0.172739</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>2424</td>\n",
       "      <td>120.204536</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.133475</td>\n",
       "      <td>0.126261</td>\n",
       "      <td>0.12898</td>\n",
       "      <td>-0.000505</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>0.642443</td>\n",
       "      <td>0.637267</td>\n",
       "      <td>0.630600</td>\n",
       "      <td>0.004964</td>\n",
       "      <td>-0.000612</td>\n",
       "      <td>20.860287</td>\n",
       "      <td>19.811508</td>\n",
       "      <td>20.453041</td>\n",
       "      <td>-0.239506</td>\n",
       "      <td>0.011242</td>\n",
       "      <td>363825</td>\n",
       "      <td>346531.750</td>\n",
       "      <td>350810.583333</td>\n",
       "      <td>-516.928571</td>\n",
       "      <td>-141.119048</td>\n",
       "      <td>415.328688</td>\n",
       "      <td>1708</td>\n",
       "      <td>359.142857</td>\n",
       "      <td>-0.732351</td>\n",
       "      <td>-0.035426</td>\n",
       "      <td>61.111111</td>\n",
       "      <td>0.238110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>281935788</td>\n",
       "      <td>Epocrates References &amp; Tools for Healthcare Pr...</td>\n",
       "      <td>Medical</td>\n",
       "      <td>iphone</td>\n",
       "      <td>7.038282</td>\n",
       "      <td>7.073875</td>\n",
       "      <td>7.173192</td>\n",
       "      <td>7.016097</td>\n",
       "      <td>7.020318</td>\n",
       "      <td>7.022613</td>\n",
       "      <td>7.028075</td>\n",
       "      <td>7.001246</td>\n",
       "      <td>1150.660714</td>\n",
       "      <td>64437</td>\n",
       "      <td>1150.660714</td>\n",
       "      <td>-1.639337</td>\n",
       "      <td>-0.069742</td>\n",
       "      <td>0.006644</td>\n",
       "      <td>394</td>\n",
       "      <td>-276</td>\n",
       "      <td>154.055797</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>969059</td>\n",
       "      <td>1420118</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>2.941176</td>\n",
       "      <td>0.281447</td>\n",
       "      <td>0.103090</td>\n",
       "      <td>0.140426</td>\n",
       "      <td>0.140326</td>\n",
       "      <td>0.334690</td>\n",
       "      <td>50353</td>\n",
       "      <td>0.051961</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>2424</td>\n",
       "      <td>399.611959</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.133475</td>\n",
       "      <td>0.126261</td>\n",
       "      <td>0.12898</td>\n",
       "      <td>-0.000505</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>0.642443</td>\n",
       "      <td>0.637267</td>\n",
       "      <td>0.630600</td>\n",
       "      <td>0.004964</td>\n",
       "      <td>-0.000612</td>\n",
       "      <td>20.860287</td>\n",
       "      <td>19.811508</td>\n",
       "      <td>20.453041</td>\n",
       "      <td>-0.239506</td>\n",
       "      <td>0.011242</td>\n",
       "      <td>363825</td>\n",
       "      <td>346531.750</td>\n",
       "      <td>350810.583333</td>\n",
       "      <td>-516.928571</td>\n",
       "      <td>-141.119048</td>\n",
       "      <td>4691.119653</td>\n",
       "      <td>10011</td>\n",
       "      <td>5959.821429</td>\n",
       "      <td>195.635228</td>\n",
       "      <td>-4.040557</td>\n",
       "      <td>61.111111</td>\n",
       "      <td>0.238110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               name  \\\n",
       "0  281704574   AIM: Chat, Free Text, Photo Share, Voice Message   \n",
       "1  281922769                                         Mobile MIM   \n",
       "2  281922769                                         Mobile MIM   \n",
       "3  281935788  Epocrates References & Tools for Healthcare Pr...   \n",
       "4  281935788  Epocrates References & Tools for Healthcare Pr...   \n",
       "\n",
       "            category  device    week_1    week_2    week_3    week_4  \\\n",
       "0  Social Networking  iphone  6.239161  6.127804  6.101279  6.183265   \n",
       "1            Medical    ipad  4.075113  4.084775  3.964886  4.198275   \n",
       "2            Medical  iphone  4.576183  4.532599  4.487030  4.507715   \n",
       "3            Medical    ipad  5.617810  5.557379  5.520317  5.371302   \n",
       "4            Medical  iphone  7.038282  7.073875  7.173192  7.016097   \n",
       "\n",
       "     week_5    week_6    week_7    week_8    daily_avg  download_sum  \\\n",
       "0  6.078625  5.926926  5.792578  6.059791   433.678571         24286   \n",
       "1  4.089571  4.027899  4.124828  3.991626    58.678571          3286   \n",
       "2  4.480578  4.464265  4.439284  4.610868    91.267857          5111   \n",
       "3  5.493649  5.418637  5.434969  5.393628   239.660714         13421   \n",
       "4  7.020318  7.022613  7.028075  7.001246  1150.660714         64437   \n",
       "\n",
       "        coef_0    coef_1    coef_2    coef_3  maxStep  minStep         std  \\\n",
       "0   433.678571 -2.585304  0.035204  0.003661      308     -233   91.952066   \n",
       "1    58.678571 -0.028161 -0.005040 -0.000418       28      -34    8.860239   \n",
       "2    91.267857 -0.048223  0.014281  0.000401       51      -48   12.693490   \n",
       "3   239.660714 -0.934279  0.019362 -0.000439      120      -73   31.992341   \n",
       "4  1150.660714 -1.639337 -0.069742  0.006644      394     -276  154.055797   \n",
       "\n",
       "   nb_missing  iphone  ipad  Productivity  Entertainment  Travel  Sports  \\\n",
       "0           0       1     0             0              0       0       0   \n",
       "1           0       0     1             0              0       0       0   \n",
       "2           0       1     0             0              0       0       0   \n",
       "3           0       0     1             0              0       0       0   \n",
       "4           0       1     0             0              0       0       0   \n",
       "\n",
       "   Music  Shopping  Finance  Business  Navigation  Food and Drink  Utilities  \\\n",
       "0      0         0        0         0           0               0          0   \n",
       "1      0         0        0         0           0               0          0   \n",
       "2      0         0        0         0           0               0          0   \n",
       "3      0         0        0         0           0               0          0   \n",
       "4      0         0        0         0           0               0          0   \n",
       "\n",
       "   Newsstand  Health and Fitness  News  Lifestyle  Medical  Weather  Games  \\\n",
       "0          0                   0     0          0        0        0      0   \n",
       "1          0                   0     0          0        1        0      0   \n",
       "2          0                   0     0          0        1        0      0   \n",
       "3          0                   0     0          0        1        0      0   \n",
       "4          0                   0     0          0        1        0      0   \n",
       "\n",
       "   Catalogs  Social Networking  Photo and Video  Reference  Books  Education  \\\n",
       "0         0                  1                0          0      0          0   \n",
       "1         0                  0                0          0      0          0   \n",
       "2         0                  0                0          0      0          0   \n",
       "3         0                  0                0          0      0          0   \n",
       "4         0                  0                0          0      0          0   \n",
       "\n",
       "   korean  other  japanese  chinese  english  cumulative_downloads_2015-02  \\\n",
       "0       0      0         0        0        1                        617121   \n",
       "1       0      1         0        0        0                         83281   \n",
       "2       0      1         0        0        0                         99000   \n",
       "3       0      0         0        0        1                        291496   \n",
       "4       0      0         0        0        1                        969059   \n",
       "\n",
       "   dl_projection  avg_review  var_review     star1     star2     star3  \\\n",
       "0         787123    3.200000    2.684058  0.189152  0.080590  0.145992   \n",
       "1         106283    1.000000    0.000000  0.256757  0.131532  0.172072   \n",
       "2         134777    1.000000    0.000000  0.256757  0.131532  0.172072   \n",
       "3         385443    2.666667    2.941176  0.281447  0.103090  0.140426   \n",
       "4        1420118    2.666667    2.941176  0.281447  0.103090  0.140426   \n",
       "\n",
       "      star4     star5  num_ratings  ratings_per_daily_downloads  num_versions  \\\n",
       "0  0.139242  0.445023       469040                     0.760044             3   \n",
       "1  0.142342  0.296396         1109                     0.013316             1   \n",
       "2  0.142342  0.296396         1109                     0.011202             1   \n",
       "3  0.140326  0.334690        50353                     0.172739             3   \n",
       "4  0.140326  0.334690        50353                     0.051961             3   \n",
       "\n",
       "   num_review  num_review_per_daily_downloads  days_since_release  \\\n",
       "0          70                        0.000113                2424   \n",
       "1           1                        0.000012                2424   \n",
       "2           1                        0.000010                2424   \n",
       "3          18                        0.000062                2424   \n",
       "4          18                        0.000019                2424   \n",
       "\n",
       "   downloads_per_day_before  positive  negative    m1_max   m1_mean  \\\n",
       "0                254.482887         0         1  0.035065  0.032192   \n",
       "1                 34.342680         0         1  0.000000  0.000000   \n",
       "2                 40.824742         0         1  0.000000  0.000000   \n",
       "3                120.204536         0         1  0.133475  0.126261   \n",
       "4                399.611959         0         1  0.133475  0.126261   \n",
       "\n",
       "   m1_coef_0  m1_coef_1  m1_coef_2    m2_max   m2_mean  m2_coef_0  m2_coef_1  \\\n",
       "0    0.03240  -0.001056   0.000199  0.216173  0.212028   0.214854  -0.000385   \n",
       "1    0.00000   0.000000   0.000000  0.000000  0.000000   0.000000   0.000000   \n",
       "2    0.00000   0.000000   0.000000  0.000000  0.000000   0.000000   0.000000   \n",
       "3    0.12898  -0.000505  -0.000054  0.642443  0.637267   0.630600   0.004964   \n",
       "4    0.12898  -0.000505  -0.000054  0.642443  0.637267   0.630600   0.004964   \n",
       "\n",
       "   m2_coef_2     m3_max    m3_mean  m3_coef_0  m3_coef_1  m3_coef_2  m4_max  \\\n",
       "0  -0.000084  16.729371  15.190247  15.083152  -0.472173   0.100554   96931   \n",
       "1   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000       0   \n",
       "2   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000       0   \n",
       "3  -0.000612  20.860287  19.811508  20.453041  -0.239506   0.011242  363825   \n",
       "4  -0.000612  20.860287  19.811508  20.453041  -0.239506   0.011242  363825   \n",
       "\n",
       "      m4_mean      m4_coef_0    m4_coef_1   m4_coef_2   rev_coef_0  rev_max  \\\n",
       "0   88379.625   88130.375000 -2675.898810  549.422619     0.000000        0   \n",
       "1       0.000       0.000000     0.000000    0.000000     0.000000        0   \n",
       "2       0.000       0.000000     0.000000    0.000000     0.000000        0   \n",
       "3  346531.750  350810.583333  -516.928571 -141.119048   415.328688     1708   \n",
       "4  346531.750  350810.583333  -516.928571 -141.119048  4691.119653    10011   \n",
       "\n",
       "      rev_mean  rev_coef_1  rev_coef_2  ratio_latest_ver  avg_sentiment_score  \n",
       "0     0.000000    0.000000    0.000000         44.285714             0.295056  \n",
       "1     0.000000    0.000000    0.000000        100.000000             0.253146  \n",
       "2     0.000000    0.000000    0.000000        100.000000             0.253146  \n",
       "3   359.142857   -0.732351   -0.035426         61.111111             0.238110  \n",
       "4  5959.821429  195.635228   -4.040557         61.111111             0.238110  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How good did the predictor perform   --> Start running from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imported libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "%matplotlib inline  \n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "#Creating of the input data\n",
    "downloads = pd.read_csv('train_app_downloads.csv')\n",
    "reviews = pd.read_csv('train_app_review.csv')\n",
    "ratings = pd.read_csv('train_app_rating.csv')\n",
    "usages = pd.read_csv('train_usage.csv')\n",
    "revenues = pd.read_csv('train_revenue.csv')\n",
    "output = pd.read_csv('train_final_downloads.csv')\n",
    "dateRange = pd.date_range('2015-03-01', periods=56).format(formatter=lambda x: x.strftime('%Y-%m-%d'))\n",
    "\n",
    "sentiment = pd.read_csv('sentiment.csv',header=-1).ix[:,0]\n",
    "sentiment.values[sentiment.values==0.0] = 0.5\n",
    "reviews[\"sentiment_score\"] = sentiment.values \n",
    "\n",
    "#We map -1 to 0 in the downloads (there are no 0 in the initial data)\n",
    "replacementValue=0\n",
    "downloads = downloads.replace(-1,replacementValue)\n",
    "\n",
    "#Minor corrections\n",
    "ratings = ratings.rename(columns={'start1': 'star1'})\n",
    "ratings = pd.merge(downloads.drop(dateRange,1), ratings.drop('Unnamed: 0', 1), how='left',\n",
    "                   on=[\"id\",\"name\",\"category\"]).replace(\"NaN\",replacementValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This is the metric we use to determine our performance\n",
    "def metric(y_pred,y_test,percent=1):\n",
    "    top = int(len(y_pred)/100.0*percent)\n",
    "    return (len(set([i[0] for i in sorted(enumerate(y_pred), key=lambda x:x[1],reverse=True)][0:top])\n",
    "       .intersection([i[0] for i in sorted(enumerate(y_test), key=lambda x:x[1],reverse=True)][0:top])\n",
    "               ))/(percent/100.0)/len(y_pred)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = pd.read_csv('predictors.csv').drop('Unnamed: 0', 1)\n",
    "predictors = predictors.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor selection 2 methods. Cannot run both..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Predictor selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Use a model to do a forward recursive predictor selection\n",
    "#mod=linear_model.Lasso(alpha=100,fit_intercept=False)\n",
    "#mod=linear_model.LinearRegression(fit_intercept=False)\n",
    "#mod= RandomForestRegressor(max_features = 1.0/3.0,n_estimators = 100) \n",
    "#mod= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100) \n",
    "mod = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01,max_depth=2, random_state=0, loss='ls')\\\n",
    "\n",
    "rfe = RFE(estimator=mod, n_features_to_select=1, step=1)\n",
    "rfe.fit(predictors.as_matrix()[:,4:], output.as_matrix()[:,5])\n",
    "ranking = rfe.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for e in [(i[0],i[1]) for i in sorted(zip(predictors.columns[4:],ranking),key=lambda x: x[1])]:\n",
    "    print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#manually drop predictors \n",
    "#predictors_to_drop = []\n",
    "nb_pred_to_keep = 60\n",
    "predictors_to_drop = [i[0] for i in sorted(zip(predictors.columns[4:],ranking),key=lambda x: x[1])][nb_pred_to_keep:]\n",
    "for col in predictors_to_drop:\n",
    "    predictors = predictors.drop(col,1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other predictor selection method using Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Other method\n",
    "predictors = pd.read_csv('predictors.csv').drop('Unnamed: 0', 1)\n",
    "predictors = predictors.fillna(0)\n",
    "#var_select = linear_model.Lasso(alpha = 0.01).fit(predictor_train_top_10_precent.as_matrix()[:,4:],np.log(output_train_top_10_precent[\"cumulative_downloads_2016-02\"].as_matrix()))\n",
    "var_select = linear_model.Lasso(alpha = 0.1).fit(predictors.as_matrix()[:,4:],np.log(output[\"cumulative_downloads_2016-02\"].as_matrix()))\n",
    "\n",
    "importance_list = [i for i in range(len(var_select.coef_)) if abs(var_select.coef_[i])>0]\n",
    "important_predictors = [predictors.columns.values[i+4] for i in range(len(var_select.coef_)) if (i in importance_list)]\n",
    "print important_predictors\n",
    "for col in predictors.columns.values[4:]:\n",
    "    if col not in important_predictors:\n",
    "        predictors = predictors.drop(col,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Classification + Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print list(predictors.columns.values) \n",
    "np.random.seed(5)\n",
    "K = 5\n",
    "\n",
    "top_percent_classif = 20\n",
    "\n",
    "\n",
    "kf = KFold(len(predictors), n_folds=K)\n",
    "old_top = []\n",
    "new_top = []\n",
    "new_top_noClassif = []\n",
    "top_10 = []\n",
    "new_top_select = [] \n",
    "new_top_noClassif = [] \n",
    "for train, test in kf:\n",
    "    #base model\n",
    "    '''old_mod=linear_model.LinearRegression(fit_intercept=False).fit(predictors.as_matrix()[train,4:12], output.as_matrix()[train,5])\n",
    "    old_y_pred =  old_mod.predict(predictors.as_matrix()[test,4:12])\n",
    "    old_top.append(metric(old_y_pred,output.as_matrix()[test,5]))'''\n",
    "    \n",
    "    #model to determine the top 10%   (CLASSIFICATION)\n",
    "    #mod_class10=GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01,max_depth=2, random_state=0, loss='ls')\\\n",
    "    mod_class10= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100)\\\n",
    "    .fit(predictors.as_matrix()[train,4:], output.as_matrix()[train,5])\n",
    "    \n",
    "    y_pred =  mod_class10.predict(predictors.as_matrix()[test,4:])\n",
    "    \n",
    "    #estimate of the top 10% of the test set\n",
    "    estimate_class10 = predictors.iloc[test].copy()\n",
    "    estimate_class10[\"firstEstimate\"] = y_pred\n",
    "    estimate_class10 = estimate_class10.sort_values(by= \"firstEstimate\",ascending = False).iloc[0:int(1.0*top_percent_classif/100.0*len(estimate_class10))]\n",
    "    estimate_class10 = estimate_class10.drop(\"firstEstimate\",1)\n",
    "#     estimate_class10 = estimate_class10.sort_values(by= \"daily_avg\",ascending = False).iloc[0:int(1.0*top_percent_classif/100.0*len(estimate_class10))]\n",
    "\n",
    "    \n",
    "    #top 10% of the trainning set\n",
    "    output_train_top_10_precent = output.iloc[train].copy().sort_values(by= 'cumulative_downloads_2016-02',ascending = False).iloc[0:int(1.0*top_percent_classif/100.0*len(output.iloc[train]))].drop('Unnamed: 0',1)\n",
    "    predictor_train_top_10_precent = output_train_top_10_precent.merge(predictors, how='left', on=[\"id\",\"name\",\"category\",\"device\"]).copy()\n",
    "    predictor_train_top_10_precent = predictor_train_top_10_precent.drop('cumulative_downloads_2016-02',1)\n",
    "    #predictor_train_top_10_precent = predictor_train_top_10_precent.drop('firstEstimate',1)\n",
    "    \n",
    "    #This is the actual top 1% of the test set\n",
    "    output_test_top_1_precent = output.iloc[test].sort_values(by= 'cumulative_downloads_2016-02',ascending = False).iloc[0:int(0.01*len(output.iloc[test]))].copy()\n",
    "\n",
    "    \n",
    "    #second model -> Regression on the top obtainned by regression\n",
    "    #mod_top1= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100)\\\n",
    "    #mod_top1= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100)\\\n",
    "    #mod_top1=linear_model.Lasso(alpha=100,fit_intercept=False)\\\n",
    "    #mod_top1=GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0, loss='ls')\\\n",
    "    mod_top1 = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01,max_depth=2, random_state=0, loss='ls')\\\n",
    "    .fit(predictor_train_top_10_precent.as_matrix()[:,4:], np.log(output_train_top_10_precent[\"cumulative_downloads_2016-02\"].as_matrix()))\n",
    "\n",
    "    \n",
    "    y_pred_2 =  mod_top1.predict(estimate_class10.as_matrix()[:,4:])\n",
    "    \n",
    "    \n",
    "    '''#Andrew's regression with additionnal lasso predictor selection\n",
    "    #(1) feature selection : lasso or random forest\n",
    "    #var_select = linear_model.Lasso(alpha = 0.01).fit(predictor_train_top_10_precent.as_matrix()[:,4:],np.log(output_train_top_10_precent[\"cumulative_downloads_2016-02\"].as_matrix()))\n",
    "    var_select = RandomForestRegressor(max_features = \"sqrt\",n_estimators = 100).fit(predictor_train_top_10_precent.as_matrix()[:,4:], np.log(output_train_top_10_precent[\"cumulative_downloads_2016-02\"].as_matrix()))\n",
    "      \n",
    "    #importance_list = [i for i in  range(len(var_select.coef_)) if abs(var_select.coef_[i])>0]    \n",
    "    importance_list = list(reversed(np.argsort(var_select.feature_importances_)))[0:20]\n",
    "    \n",
    "    #new train and test set with the selected variables\n",
    "    X_rf_train = predictor_train_top_10_precent.as_matrix()[:,4:][:,importance_list]\n",
    "    y_rf_train = output_train_top_10_precent[\"cumulative_downloads_2016-02\"].as_matrix()\n",
    "    X_rf_test = estimate_class10.as_matrix()[:,4:][:,importance_list]  \n",
    "    \n",
    "    #(2) regression: random forest or boosting \n",
    "    #mod_top1_select= RandomForestRegressor(max_features = \"sqrt\",n_estimators = 100).fit(X_rf_train, y_rf_train)\n",
    "    params = {'n_estimators': 1000, 'max_depth': 2, 'learning_rate': 0.01, 'loss': 'ls'}\n",
    "    mod_top1_select= GradientBoostingRegressor(**params).fit(X_rf_train, y_rf_train)\n",
    "    #mod_top1_select = SVR(kernel = 'poly',degree = 3).fit(X_rf_train,y_rf_train)\n",
    "    y_pred_3 = mod_top1_select.predict(X_rf_test)\n",
    "    \n",
    "    #No Classification model\n",
    "    mod_noClassif= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100).fit(predictors.as_matrix()[train,4:], output.as_matrix()[train,5])\n",
    "    y_pred_noClassif =  mod_noClassif.predict(predictors.as_matrix()[test,4:])'''\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    estimate_top1 = estimate_class10.copy()\n",
    "    #estimate_top1_select = estimate_class10.copy()\n",
    "    #estimate_top1_noClassif = predictors.iloc[test].copy()\n",
    "    estimate_top1[\"secondEstimate\"] = y_pred_2\n",
    "    #estimate_top1_select[\"thirdEstimate\"] = y_pred_3\n",
    "    #estimate_top1_noClassif[\"noClassifEstimate\"] = y_pred_noClassif\n",
    "    estimate_top1 = estimate_top1.sort_values(by= \"secondEstimate\",ascending = False).iloc[:int(0.01*len(output.iloc[test]))]\n",
    "    \n",
    "    #estimate_top1_select = estimate_top1_select.sort_values(by= \"thirdEstimate\",ascending = False).iloc[0:int(0.01*len(output.iloc[test]))]\n",
    "    #estimate_top1_noClassif = estimate_top1_noClassif.sort_values(by= \"noClassifEstimate\",ascending = False).iloc[0:int(0.01*len(output.iloc[test]))]\n",
    "\n",
    "    estimation_error = len(estimate_top1.merge(output_test_top_1_precent, how='inner', on=[\"id\",\"name\",\"category\",\"device\"]))*100.0/len(output_test_top_1_precent)\n",
    "    new_top.append(estimation_error)\n",
    "    #new_top_select.append(len(estimate_top1_select.merge(output_test_top_1_precent, how='inner', on=[\"id\",\"name\",\"category\",\"device\"]))*100.0/len(output_test_top_1_precent))\n",
    "    #new_top_noClassif.append(len(estimate_top1_noClassif.merge(output_test_top_1_precent, how='inner', on=[\"id\",\"name\",\"category\",\"device\"]))*100.0/len(output_test_top_1_precent))\n",
    "    top_10.append(len(estimate_class10.merge(output_test_top_1_precent, how='inner', on=[\"id\",\"name\",\"category\",\"device\"]))*100.0/len(output_test_top_1_precent))\n",
    "       \n",
    "#print \"Old model           : \" + str(1.0*sum(old_top)/len(old_top))\n",
    "print \"Top10%              : \" + str(1.0*sum(top_10)/len(top_10))\n",
    "print \"Top1% with classif1 : \" + str(1.0*sum(new_top)/len(new_top))\n",
    "#print \"Top1% with classif2 : \" + str(1.0*sum(new_top_select)/len(new_top_select))\n",
    "#print \"Top1% no classif1   : \" + str(1.0*sum(new_top_noClassif)/len(new_top_noClassif))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var_select.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(predictor_train_top_10_precent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation k-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 5\n",
    "print list(predictors.columns.values)\n",
    "np.random.seed(1)\n",
    "kf = KFold(len(predictors), n_folds=K)\n",
    "new_top = []\n",
    "for train, test in kf:\n",
    "\n",
    "    #model\n",
    "    #mod=linear_model.LinearRegression(fit_intercept=False)\\ \n",
    "    #mod=linear_model.Lasso(alpha=100,fit_intercept=False)\\\n",
    "    #mod = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0, loss='ls')\\ \n",
    "    #mod= RandomForestRegressor(max_features = 1.0/3.0,n_estimators = 100)\\\n",
    "    #mod= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100)\\\n",
    "    mod = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01,max_depth=2, random_state=0, loss='ls')\\\n",
    "    .fit(predictors.as_matrix()[train,4:], output.as_matrix()[train,5]) \n",
    "\n",
    "    y_pred =  mod.predict(predictors.as_matrix()[test,4:])\n",
    "    new_top.append(metric(y_pred,output.as_matrix()[test,5]))\n",
    "\n",
    "print 1.0*sum(new_top)/len(new_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt at boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print list(predictors.columns.values)\n",
    "K = 5\n",
    "\n",
    "for n_est in [1000,5000,10000,50000]:\n",
    "    for l_rate in [0.001,0.01,0.1]:\n",
    "        for m_depth in range(1,5):\n",
    "\n",
    "            np.random.seed(1)\n",
    "            kf = KFold(len(predictors), n_folds=K)\n",
    "            old_top = []\n",
    "            new_top = []\n",
    "            for train, test in kf:\n",
    "                #model\n",
    "                mod = GradientBoostingRegressor(n_estimators=n_est, learning_rate=l_rate,max_depth=m_depth, random_state=0, loss='ls').fit(predictors.as_matrix()[train,4:], output.as_matrix()[train,5]) \n",
    "                y_pred =  mod.predict(predictors.as_matrix()[test,4:])\n",
    "                new_top.append(metric(y_pred,output.as_matrix()[test,5]))\n",
    "\n",
    "            print \"Performance with \\tB=\"+str(n_est)+\"\\tl=\"+str(l_rate)+\"\\td=\"+ str(m_depth)+\" is: \\t\"+str(1.0*sum(new_top)/len(new_top))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt at SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "print list(predictors.columns.values)\n",
    "K = 5\n",
    "\n",
    "for ker in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "    for deg in range(5):\n",
    "        for coef in [0,1,2]:\n",
    "            for Cost in [0.001,0.01,0.1,1,10]:\n",
    "                for epsi in [0.001,0.01,0.1,1,10]:\n",
    "\n",
    "                    np.random.seed(1)\n",
    "                    kf = KFold(len(predictors), n_folds=K)\n",
    "                    old_top = []\n",
    "                    new_top = []\n",
    "                    for train, test in kf:\n",
    "                        #model\n",
    "                        #mod = SVR(kernel=ker, degree=deg,  coef0=coef, C=Cost, epsilon=epsi).fit(predictors.as_matrix()[train,4:], output.as_matrix()[train,5]) \n",
    "                        mod = SVR(kernel=ker, degree=deg,  coef0=coef, C=Cost, epsilon=epsi,max_iter=1000).fit(predictors.as_matrix()[train,4:], output.as_matrix()[train,5]) \n",
    "                        y_pred =  mod.predict(predictors.as_matrix()[test,4:])\n",
    "                        new_top.append(metric(y_pred,output.as_matrix()[test,5]))\n",
    "\n",
    "                    print \"Performance with \\tKernel=\"+ker+\"\\tdegree=\"+str(deg)+\"\\tcoef0=\"+ str(coef)+\"\\tCost=\"+str(Cost)+\"\\tepsilon=\"+ str(epsi)+\" is: \\t\"+str(1.0*sum(new_top)/len(new_top))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# CV on the size of the Classification top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print list(predictors.columns.values) \n",
    "\n",
    "K = 5\n",
    "\n",
    "cv_top_10 = []\n",
    "cv_top_1  = []\n",
    "\n",
    "for top in range(1,31):\n",
    "np.random.seed(1)\n",
    "    top_percent_classif = top\n",
    "\n",
    "    kf = KFold(len(predictors), n_folds=K)\n",
    "    old_top = []\n",
    "    new_top = []\n",
    "    top_10 = []\n",
    "    new_top_select = [] \n",
    "    for train, test in kf:\n",
    "        #base model\n",
    "        old_mod=linear_model.LinearRegression(fit_intercept=False).fit(predictors.as_matrix()[train,4:12], output.as_matrix()[train,5])\n",
    "        old_y_pred =  old_mod.predict(predictors.as_matrix()[test,4:12])\n",
    "        old_top.append(metric(old_y_pred,output.as_matrix()[test,5]))\n",
    "\n",
    "        #model to determine the top 10%   (CLASSIFICATION)\n",
    "        mod_class10= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100).fit(predictors.as_matrix()[train,4:], output.as_matrix()[train,5])\n",
    "\n",
    "        y_pred =  mod_class10.predict(predictors.as_matrix()[test,4:])\n",
    "\n",
    "        #estimate of the top 10% of the test set\n",
    "        estimate_class10 = predictors.iloc[test].copy()\n",
    "        estimate_class10[\"firstEstimate\"] = y_pred\n",
    "        #estimate_class10 = estimate_class10.sort_values(by= \"firstEstimate\",ascending = False).iloc[1:int(1.0*top_percent_classif/100.0*len(estimate_class10))]\n",
    "        estimate_class10 = estimate_class10.drop(\"firstEstimate\",1)\n",
    "        estimate_class10 = estimate_class10.sort_values(by= \"daily_avg\",ascending = False).iloc[1:int(1.0*top_percent_classif/100.0*len(estimate_class10))]\n",
    "\n",
    "\n",
    "        #top 10% of the trainning set\n",
    "        output_train_top_10_precent = output.iloc[train].copy().sort_values(by= 'cumulative_downloads_2016-02',ascending = False).iloc[1:int(1.0*top_percent_classif/100.0*len(output.iloc[train]))].drop('Unnamed: 0',1)\n",
    "        predictor_train_top_10_precent = output_train_top_10_precent.merge(predictors, how='left', on=[\"id\",\"name\",\"category\",\"device\"]).copy()\n",
    "        predictor_train_top_10_precent = predictor_train_top_10_precent.drop('cumulative_downloads_2016-02',1)\n",
    "        #predictor_train_top_10_precent = predictor_train_top_10_precent.drop('firstEstimate',1)\n",
    "\n",
    "        #This is the actual top 1% of the test set\n",
    "        output_test_top_1_precent = output.iloc[test].sort_values(by= 'cumulative_downloads_2016-02',ascending = False).iloc[1:int(0.01*len(output.iloc[test]))].copy()\n",
    "\n",
    "\n",
    "        #second model -> Regression on the top obtainned by regression\n",
    "        #mod_top1= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100).fit(predictor_train_top_10_precent.as_matrix()[:,4:], output_train_top_10_precent[\"cumulative_downloads_2016-02\"].as_matrix())\n",
    "        mod_top1= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100).fit(predictor_train_top_10_precent.as_matrix()[:,4:], np.log(output_train_top_10_precent[\"cumulative_downloads_2016-02\"].as_matrix()))\n",
    "        #mod_top1=linear_model.Lasso(alpha=100,fit_intercept=False).fit(predictor_train_top_10_precent.as_matrix()[:,4:], np.log(output_train_top_10_precent[\"cumulative_downloads_2016-02\"].as_matrix()))\n",
    "        #mod_top1=GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0, loss='ls').fit(predictor_train_top_10_precent.as_matrix()[:,4:], (output_train_top_10_precent[\"cumulative_downloads_2016-02\"].as_matrix()))\n",
    "\n",
    "        y_pred_2 =  mod_top1.predict(estimate_class10.as_matrix()[:,4:])\n",
    "\n",
    "        estimate_top1 = estimate_class10.copy()\n",
    "        estimate_top1[\"secondEstimate\"] = y_pred_2\n",
    "\n",
    "        estimate_top1 = estimate_top1.sort_values(by= \"secondEstimate\",ascending = False).iloc[1:int(0.01*len(output.iloc[test]))]\n",
    "\n",
    "        estimation_error = len(estimate_top1.merge(output_test_top_1_precent, how='inner', on=[\"id\",\"name\",\"category\",\"device\"]))*100.0/len(output_test_top_1_precent)\n",
    "        new_top.append(estimation_error)\n",
    "        top_10.append(len(estimate_class10.merge(output_test_top_1_precent, how='inner', on=[\"id\",\"name\",\"category\",\"device\"]))*100.0/len(output_test_top_1_precent))\n",
    "\n",
    "\n",
    "    cv_top_10.append(1.0*sum(top_10)/len(top_10))\n",
    "    cv_top_1.append(1.0*sum(new_top)/len(new_top))\n",
    "\n",
    "plt.plot(range(1,31),cv_top_10,label = 'top 10 estimate')\n",
    "plt.plot(range(1,31),cv_top_1,label = 'top 1 estimate')\n",
    "plt.legend(loc = 'best')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# CV on the size of the predictor set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(1)\n",
    "K = 5\n",
    "\n",
    "\n",
    "for nb_pred_to_keep in range(5,100,5):\n",
    "\n",
    "    predictors = pd.read_csv('predictors.csv').drop('Unnamed: 0', 1)\n",
    "    predictors = predictors.fillna(0)\n",
    "\n",
    "    predictors_to_drop = [i[0] for i in sorted(zip(predictors.columns[4:],ranking),key=lambda x: x[1])][nb_pred_to_keep:]\n",
    "    for col in predictors_to_drop:\n",
    "        predictors = predictors.drop(col,1)\n",
    "\n",
    "    kf = KFold(len(predictors), n_folds=K)\n",
    "    #print list(predictors.columns.values) \n",
    "    new_top_noClassif = [] \n",
    "    for train, test in kf:\n",
    "\n",
    "        output_test_top_1_precent = output.iloc[test].sort_values(by= 'cumulative_downloads_2016-02',ascending = False).iloc[1:int(0.01*len(output.iloc[test]))].copy()\n",
    "\n",
    "\n",
    "        #No Classification model\n",
    "        mod_noClassif= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100).fit(predictors.as_matrix()[train,4:], output.as_matrix()[train,5])\n",
    "        y_pred_noClassif =  mod_noClassif.predict(predictors.as_matrix()[test,4:])\n",
    "\n",
    "        estimate_top1_noClassif = predictors.iloc[test].copy()\n",
    "        estimate_top1_noClassif[\"noClassifEstimate\"] = y_pred_noClassif\n",
    "        estimate_top1_noClassif = estimate_top1_noClassif.sort_values(by= \"noClassifEstimate\",ascending = False).iloc[1:int(0.01*len(output.iloc[test]))]\n",
    "\n",
    "        new_top_noClassif.append(len(estimate_top1_noClassif.merge(output_test_top_1_precent, how='inner', on=[\"id\",\"name\",\"category\",\"device\"]))*100.0/len(output_test_top_1_precent))\n",
    "        \n",
    "\n",
    "    print \"Top1% no classif with \"+str(nb_pred_to_keep)+\" kept predictors : \" + str(1.0*sum(new_top_noClassif)/len(new_top_noClassif))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "RForest pred selection and CV on the number of predictors<br/>['id', 'name', 'category', 'device', 'week_1', 'week_2', 'week_3', 'week_4', 'week_5', 'week_6', 'week_7', 'week_8', 'daily_avg', 'coef_0', 'coef_1', 'coef_2', 'coef_3', 'maxStep', 'minStep', 'std', 'nb_missing', 'iphone', 'ipad', 'Productivity', 'Entertainment', 'Travel', 'Sports', 'Music', 'Shopping', 'Finance', 'Business', 'Navigation', 'Food and Drink', 'Utilities', 'Newsstand', 'Health and Fitness', 'News', 'Lifestyle', 'Medical', 'Weather', 'Games', 'Catalogs', 'Social Networking', 'Photo and Video', 'Reference', 'Books', 'Education', 'chinese', 'Unnamed: 49', 'japanese', 'other', 'english', 'korean', 'cumulative_downloads_2015-02', 'avg_review', 'var_review', 'star1', 'star2', 'star3', 'star4', 'star5', 'num_ratings', 'reviewPerDailyDownloads', 'num_versions', 'num_review', 'days_since_release', 'downloads_per_day_before', 'positive', 'negative', 'm1_min', 'm1_mean', 'm1_std', 'm1_coef_0', 'm1_coef_1', 'm1_coef_2', 'm2_max', 'm2_min', 'm2_coef_0', 'm2_coef_1', 'm2_coef_2', 'm3_min', 'm3_mean', 'm3_std', 'm3_coef_0', 'm3_coef_1', 'm3_coef_2', 'm4_max', 'm4_min', 'm4_mean', 'm4_std', 'm4_coef_0', 'm4_coef_1', 'm4_coef_2', 'rev_coef_0', 'rev_max', 'rev_min', 'rev_std', 'rev_coef_1', 'rev_coef_2'] <br/>\n",
    "Top1% no classif with 5 kept predictors : 62.5396825397    <br/>\n",
    "Top1% no classif with 10 kept predictors : 65.0793650794<br/>\n",
    "Top1% no classif with 15 kept predictors : 65.3968253968<br/>\n",
    "Top1% no classif with 20 kept predictors : 64.7619047619<br/>\n",
    "Top1% no classif with 25 kept predictors : 64.7619047619<br/>\n",
    "Top1% no classif with 30 kept predictors : 65.3968253968<br/>\n",
    "Top1% no classif with 35 kept predictors : 66.0317460317<br/>\n",
    "Top1% no classif with 40 kept predictors : 65.7142857143<br/>\n",
    "Top1% no classif with 45 kept predictors : 65.0793650794<br/>\n",
    "Top1% no classif with 50 kept predictors : 65.3968253968<br/>\n",
    "Top1% no classif with 55 kept predictors : 65.0793650794<br/>\n",
    "Top1% no classif with 60 kept predictors : 65.3968253968<br/>\n",
    "Top1% no classif with 65 kept predictors : 64.4444444444<br/>\n",
    "Top1% no classif with 70 kept predictors : 65.3968253968<br/>\n",
    "Top1% no classif with 75 kept predictors : 63.8095238095<br/>\n",
    "Top1% no classif with 80 kept predictors : 63.4920634921<br/>\n",
    "Top1% no classif with 85 kept predictors : 64.4444444444<br/>\n",
    "Top1% no classif with 90 kept predictors : 61.9047619048<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old_top = []\n",
    "new_top = []\n",
    "\n",
    "test_frac = 0.31  #Fraction of test points\n",
    "N = 20   #number of iterations\n",
    "np.random.seed(1)\n",
    "for i in range(1,N):\n",
    "    r = np.random.randint(1,429496729)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors.as_matrix()[:,4:12], output.as_matrix()[:,5], test_size=test_frac, random_state=r)\n",
    "    X_test = X_test[0:10000]\n",
    "    y_test = y_test[0:10000]\n",
    "    old_mod=linear_model.LinearRegression(fit_intercept=False).fit(X_train,y_train)\n",
    "    old_y_pred =  old_mod.predict(X_test)\n",
    "    old_top.append(metric(old_y_pred,y_test))\n",
    "    \n",
    "np.random.seed(1)\n",
    "for i in range(1,N):\n",
    "    r = np.random.randint(1,429496729)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors.as_matrix()[:,4:], output.as_matrix()[:,5], test_size=test_frac, random_state=r)\n",
    "    X_test = X_test[0:10000]\n",
    "    y_test = y_test[0:10000]\n",
    "    #mod=linear_model.LinearRegression(fit_intercept=False).fit(X_train,y_train)\n",
    "    #mod=linear_model.Lasso(alpha=100,fit_intercept=False).fit(X_train,y_train)\n",
    "    #mod= RandomForestRegressor(max_features = 1.0/3.0,n_estimators = 100).fit(X_train,y_train)\n",
    "    mod= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100).fit(X_train,y_train)\n",
    "    y_pred =  mod.predict(X_test)\n",
    "    new_top.append(metric(y_pred,y_test))\n",
    "if (N<300):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(range(1,N),old_top,label=\"old\")\n",
    "    plt.plot(range(1,N),new_top,label=\"performance\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "print 1.0*sum(old_top)/len(old_top)\n",
    "print list(predictors.columns.values)\n",
    "print 1.0*sum(new_top)/len(new_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "54.8947368421  \n",
    "55.0526315789  #with #missing and Lasso <br/>\n",
    "54.7894736842  #including raw ratings Lasso :( <br/>\n",
    "55.0526315789  #with #missing weightedSumRatings and Lasso <br/>\n",
    "55.1052631579  #adding the categories <br/>\n",
    "55.3684210526  #adding average sentiment score and positive/negative label <br/>\n",
    "55.5263157895  #adding coeficients of usages\n",
    "\n",
    "\n",
    "With 10000 only:\n",
    "54.8947368421 all\n",
    "\n",
    "56.5263157895 on Lasso all but 'rev_coef_i'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso All alpha = 100\n",
    "['id', 'name', 'category', 'device', 'week_1', 'week_2', 'week_3', 'week_4', 'week_5', 'week_6', 'week_7', 'week_8', 'daily_avg', 'coef_0', 'coef_1', 'coef_2', 'coef_3', 'maxStep', 'minStep', 'std', 'nb_missing', 'iphone', 'ipad', 'Productivity', 'Entertainment', 'Travel', 'Sports', 'Music', 'Shopping', 'Finance', 'Business', 'Navigation', 'Food and Drink', 'Utilities', 'Newsstand', 'Health and Fitness', 'News', 'Lifestyle', 'Medical', 'Weather', 'Games', 'Catalogs', 'Social Networking', 'Photo and Video', 'Reference', 'Books', 'Education', 'avg_review', 'var_review', 'star1', 'star2', 'star3', 'star4', 'star5', 'positive', 'negative', 'm1_coef_0', 'm1_coef_1', 'm1_coef_2', 'm2_coef_0', 'm2_coef_1', 'm2_coef_2', 'm3_coef_0', 'm3_coef_1', 'm3_coef_2', 'm4_coef_0', 'm4_coef_1', 'm4_coef_2']\n",
    "57.1052631579"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictor selection, pick the top 50 features selected in the forward recursive selection with Lasso and then do random forest\n",
    "\n",
    "54.6842105263\n",
    "['id', 'name', 'category', 'device', 'week_1', 'week_2', 'week_3', 'week_4', 'week_5', 'week_6', 'week_7', 'week_8', 'daily_avg', 'coef_0', 'coef_1', 'coef_2', 'coef_3', 'maxStep', 'minStep', 'std', 'nb_missing', 'iphone', 'ipad', 'Productivity', 'Entertainment', 'Sports', 'Music', 'Shopping', 'Finance', 'Business', 'Navigation', 'Food and Drink', 'Utilities', 'News', 'Lifestyle', 'Medical', 'Weather', 'Games', 'Catalogs', 'Social Networking', 'Photo and Video', 'Reference', 'Education', 'avg_review', 'var_review', 'star2', 'star3', 'star4', 'positive', 'negative', 'm2_coef_0', 'm3_coef_0', 'm3_coef_1', 'm4_coef_2']\n",
    "57.3157894737"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Estimation of alpha for the Lasso regression \"cros validation\" like approach\n",
    "for a in np.arange(90,110,5):\n",
    "\n",
    "    old_top = []\n",
    "    new_top = []\n",
    "\n",
    "    test_frac = 0.31  #Fraction of test points\n",
    "    N = 20   #number of iterations\n",
    "\n",
    "\n",
    "    np.random.seed(1)\n",
    "    for i in range(1,N):\n",
    "        r = np.random.randint(1,429496729)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(predictors.as_matrix()[:,4:], output.as_matrix()[:,5], test_size=test_frac, random_state=r)\n",
    "        X_test = X_test[0:10000]\n",
    "        y_test = y_test[0:10000]\n",
    "        mod=linear_model.Lasso(alpha=a,fit_intercept=False).fit(X_train,y_train) \n",
    "        y_pred =  mod.predict(X_test)\n",
    "        new_top.append(metric(y_pred,y_test))\n",
    "\n",
    "\n",
    "    print a\n",
    "    print 1.0*sum(new_top)/len(new_top)\n",
    "    print \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Classification on top 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = pd.read_csv('predictors.csv').drop('Unnamed: 0', 1)\n",
    "predictors = predictors.fillna(0)\n",
    "\n",
    "thr = predictors.sort_values(by= 'week_8',ascending = False).iloc[int(0.1*len(predictors))][\"week_8\"]\n",
    "predictors  = predictors[predictors.week_8 >= thr]\n",
    "\n",
    "\n",
    "len(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_classification_10 = output\n",
    "output_classification_50 = output\n",
    "output_classification_100 = output\n",
    "#Is top ?\n",
    "def generator_istop(inp,threshold):\n",
    "    if (inp[5] >= threshold):\n",
    "        return 'True'\n",
    "    else:\n",
    "        return 'False'\n",
    "\n",
    "threshold = output.sort_values('cumulative_downloads_2016-02',ascending = False).iloc[len(output)/10]['cumulative_downloads_2016-02']\n",
    "output_classification_10[\"is_top\"] = output.apply(generator_istop,axis = 1,args=(threshold,))\n",
    "\n",
    "threshold = output.sort_values('cumulative_downloads_2016-02',ascending = False).iloc[len(output)/50]['cumulative_downloads_2016-02']\n",
    "output_classification_50[\"is_top\"] = output.apply(generator_istop,axis = 1,args=(threshold,))\n",
    "\n",
    "threshold = output.sort_values('cumulative_downloads_2016-02',ascending = False).iloc[len(output)/100]['cumulative_downloads_2016-02']\n",
    "output_classification_100[\"is_top\"] = output.apply(generator_istop,axis = 1,args=(threshold,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_classification_10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "K=10\n",
    "kf = KFold(32339, n_folds=K)\n",
    "old_top = []\n",
    "new_top = []\n",
    "for train, test in kf:\n",
    "    #model\n",
    "    mod = SVC(C=10.0, kernel='poly', degree=3, gamma='auto', coef0=2.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=10, decision_function_shape=None, random_state=None).fit(predictors.as_matrix()[train,4:],output_classification_10.as_matrix()[train,6])\n",
    "    #mod = linear_model.LogisticRegression().fit(predictors.as_matrix()[train,4:], output_classification_10.as_matrix()[train,6])\n",
    "    y_pred =  mod.predict(predictors.as_matrix()[test,4:])\n",
    "    new_top.append(metric_classification(y_pred,output_classification_10.as_matrix()[test,6]))\n",
    "print list(predictors.columns.values)\n",
    "print 1.0*sum(new_top)/len(new_top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###This metric is not very good, put True everywhere and get 100%\n",
    "def metric_classification(y_pred,y_test):\n",
    "    nb_top = 0\n",
    "    for i in range(len(y_test)):\n",
    "        if (y_pred[i]=='True' and y_test[i]==\"False\"):\n",
    "            nb_top+=1\n",
    "    return 100.0*nb_top/len(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(y_pred).count(\"False\")*1.0/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print list(y_pred).count('True')\n",
    "print list(output_classification_10.as_matrix()[test,6]).count('True')\n",
    "print len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Question, what if instead of using the 'cumulative_downloads_2016-02' we used exp('cumulative_downloads_2016-02') to try to shrunk the lowest points?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_scaled = output\n",
    "#Is top ?\n",
    "def generator_outputScaler(inp):\n",
    "    #return math.exp(inp[5]*1.0/12329752*100)   #28%\n",
    "    #return 1.0/(20000000-inp[5])   #0.618429189858\n",
    "    #return inp[5]**2       #53.8045828797\n",
    "    #return (inp[5]*1.0/20000000)**2*inp[5]     #46.3834326015\n",
    "    #return (inp[5]*1.0/1500000)**0.5   #  56.5875142341\n",
    "    #return 100.0/(1+math.exp(-(inp[5]-1500000)*1.0/100000))    #54.4230120696\n",
    "    #return 200.0/(1+math.exp(-(inp[5]-1500000)*1.0/1000000)) - 100    #55.9691806875\n",
    "    #return math.log(inp[5])      #58.7522076851\n",
    "    return inp[5]               #58.4429930902\n",
    "    #return 200.0/(1+math.exp(-(inp[5]*1.0/1500000-1)*10)) - 100       #55.3506558544\n",
    "    #return math.exp(1.0/(1+math.exp(-(inp[5]*1.0/1500000-1)*10)))     #\n",
    "\n",
    "\n",
    "output_scaled[\"scaled_downloads\"] = output.apply(generator_outputScaler,axis = 1)\n",
    "\n",
    "#output_scaled[\"scaled_downloads\"] =(output['cumulative_downloads_2016-02']-output['cumulative_downloads_2016-02'].mean())/output['cumulative_downloads_2016-02'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_scaled.sort_values(by= 'cumulative_downloads_2016-02',ascending = False).head(320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(list(output_scaled.sort_values(by= 'cumulative_downloads_2016-02',ascending = False)[\"scaled_downloads\"]))\n",
    "plt.axis([0, 300,min(list(output_scaled.sort_values(by= 'cumulative_downloads_2016-02',ascending = False)[\"scaled_downloads\"])),max(list(output_scaled.sort_values(by= 'cumulative_downloads_2016-02',ascending = False)[\"scaled_downloads\"]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 10\n",
    "\n",
    "kf = KFold(32339, n_folds=K)\n",
    "old_top = []\n",
    "new_top = []\n",
    "for train, test in kf:\n",
    "    #base model\n",
    "    old_mod=linear_model.LinearRegression(fit_intercept=False).fit(predictors.as_matrix()[train,4:12], output.as_matrix()[train,5])\n",
    "    old_y_pred =  old_mod.predict(predictors.as_matrix()[test,4:12])\n",
    "    old_top.append(metric(old_y_pred,output.as_matrix()[test,5]))\n",
    "    #model\n",
    "    #mod=linear_model.LinearRegression(fit_intercept=False).fit(predictors.as_matrix()[train,4:], output_scaled.as_matrix()[train,6])\n",
    "    #mod=linear_model.Lasso(alpha=100,fit_intercept=False).fit(predictors.as_matrix()[train,4:], output_scaled.as_matrix()[train,6])\n",
    "    #mod = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0, loss='ls').fit(predictors.as_matrix()[train,4:], output_scaled.as_matrix()[train,6]) \n",
    "    #mod= RandomForestRegressor(max_features = 1.0/3.0,n_estimators = 100).fit(predictors.as_matrix()[train,4:], output_scaled.as_matrix()[train,6])\n",
    "    mod= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100).fit(predictors.as_matrix()[train,4:], output_scaled.as_matrix()[train,6])\n",
    "    y_pred =  mod.predict(predictors.as_matrix()[test,4:])\n",
    "    new_top.append(metric(y_pred,output_scaled.as_matrix()[test,6]))\n",
    "print 1.0*sum(old_top)/len(old_top)\n",
    "print list(predictors.columns.values)\n",
    "print 1.0*sum(new_top)/len(new_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(list(output.sort_values(by= 'cumulative_downloads_2016-02',ascending = False)[\"cumulative_downloads_2016-02\"]))\n",
    "plt.axis([0, 3200, 0, 13000000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(list(output_scaled.sort_values(by= 'cumulative_downloads_2016-02',ascending = False)[\"scaled_downloads\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(set(lang_series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors.sort_values(by= 'week_8',ascending = False)[0:3233].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output.sort_values(by= 'cumulative_downloads_2016-02',ascending = False)[0:323].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(predictors.sort_values(by= 'week_8',ascending = False)[0:3233].merge(output.sort_values(by= 'cumulative_downloads_2016-02',ascending = False)[0:323], how='inner', on=[\"id\",\"name\",\"category\",\"device\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = predictors[:30]\n",
    "downloads = downloads[:30]\n",
    "reviews = reviews[:300]\n",
    "ratings = ratings[:30]\n",
    "usages = usages[:30]\n",
    "revenues = revenues[:30]\n",
    "output =  output[:30]\n",
    "dateRange = dateRange[:30]\n",
    "prev_downloads = prev_downloads[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def detect_language(x):\n",
    "    try:\n",
    "        detected = langdetect.detect(x.decode('utf8','ignore'))\n",
    "        if detected in ['ja']:\n",
    "            return 'japanese'\n",
    "        elif detected in ['zh-cn']:\n",
    "            return 'chinese'\n",
    "        elif detected in ['ko']:\n",
    "            return 'korean'\n",
    "        elif detected in ['en']:\n",
    "            return 'english'\n",
    "        return 'other'\n",
    "    except:\n",
    "        return 'other'\n",
    "        #return None\n",
    "\n",
    "def set_lang_categories(x, cat):\n",
    "    if x == cat:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "reviews_withLang = reviews.copy()\n",
    "reviews_lang = reviews['text'].apply(detect_language)\n",
    "for cat in list(set(reviews_lang)):\n",
    "    reviews_withLang[cat] = reviews_lang.apply(set_lang_categories, args=(cat,))\n",
    "    \n",
    "#lang_series = predictors['name'].apply(detect_language)\n",
    "#for cat in list(set(lang_series)):\n",
    "#    predictors[cat] = lang_series.apply(set_lang_categories, args=(cat,))\n",
    "\n",
    "reviews_withLang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews_withLang = reviews.copy()\n",
    "reviews_lang = reviews['text'].apply(detect_language)\n",
    "for cat in list(set(reviews_lang)):\n",
    "    reviews_withLang[cat] = reviews_lang.apply(set_lang_categories, args=(cat,))\n",
    "\n",
    "reviews_withLang = reviews_withLang.groupby('id')[list(set(reviews_lang))].sum().reset_index()\n",
    "\n",
    "def gini_impurity(inp):\n",
    "    tot = sum(inp[1:])\n",
    "    return sum([1.0*x / tot * (1 - 1.0*x / tot) for x in inp[1:]])\n",
    "\n",
    "reviews_withLang[\"gini_reviews\"] = reviews_withLang.apply(gini_impurity,axis=1)\n",
    "predictors[\"gini_reviews\"] = predictors.join(reviews_withLang[\"gini_reviews\"],on='id')[\"gini_reviews\"].replace(\"NaN\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews_withLang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "estimate_top1 = estimate_class10.copy()\n",
    "estimate_top1[\"secondEstimate\"] = np.exp(y_pred_2).astype(int)\n",
    "estimate_top1 = estimate_top1.merge(output, how='left', on=[\"id\",\"name\",\"category\",\"device\"]).copy()\n",
    "estimate_top1 = estimate_top1.drop(\"Unnamed: 0\",1)\n",
    "estimate_top1.sort_values(by= 'cumulative_downloads_2016-02',ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_test_top_1_precent.join(estimate_class10.ix[:,['days_since_release','cumulative_downloads_2015-02','week_1','week_4','week_8','download_sum']]).sort_values('cumulative_downloads_2016-02',ascending=False)[~output_test_top_1_precent.index.isin(estimate_top1_select.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
