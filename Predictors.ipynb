{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this markdown to generate the predictors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imported libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "from sklearn import linear_model\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "#Creating of the input data\n",
    "downloads = pd.read_csv('train_app_downloads.csv')\n",
    "reviews = pd.read_csv('train_app_review.csv')\n",
    "ratings = pd.read_csv('train_app_rating.csv')\n",
    "usages = pd.read_csv('train_usage.csv')\n",
    "revenues = pd.read_csv('train_revenue.csv')\n",
    "output = pd.read_csv('train_final_downloads.csv')\n",
    "dateRange = pd.date_range('2015-03-01', periods=56).format(formatter=lambda x: x.strftime('%Y-%m-%d'))\n",
    "\n",
    "sentiment = pd.read_csv('sentiment.csv',header=-1).ix[:,0]\n",
    "sentiment.values[sentiment.values==0.0] = 0.5\n",
    "reviews[\"sentiment_score\"] = sentiment.values \n",
    "\n",
    "#We map -1 to 0 in the downloads (there are no 0 in the initial data)\n",
    "replacementValue=0\n",
    "downloads = downloads.replace(-1,replacementValue)\n",
    "\n",
    "#Minor corrections\n",
    "ratings = ratings.rename(columns={'start1': 'star1'})\n",
    "ratings = pd.merge(downloads.drop(dateRange,1), ratings.drop('Unnamed: 0', 1), how='left',\n",
    "                   on=[\"id\",\"name\",\"category\"]).replace(\"NaN\",replacementValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the predictors matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = pd.concat([downloads[\"id\"],downloads[\"name\"],downloads[\"category\"],downloads[\"device\"]],\n",
    "                       axis=1,keys=[\"id\",\"name\",\"category\",\"device\"])\n",
    "predictors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the train_app_downloads.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate the logWeekly average\n",
    "def generator_weekAvg(inp,w):\n",
    "    if (np.count_nonzero(inp[5+w*7:12+w*7] - replacementValue*np.ones(len(inp[5+w*7:12+w*7]))) == 0):\n",
    "        return 0\n",
    "    return  math.log(1.0*sum(inp[5+w*7:12+w*7])/np.count_nonzero(inp[5+w*7:12+w*7] \n",
    "                                                                 - replacementValue*np.ones(len(inp[5+w*7:12+w*7]))))\n",
    "\n",
    "for w in range(8):\n",
    "    predictors[\"week_\"+str(w+1)] = downloads.apply(generator_weekAvg,axis=1,args=(w,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate the daily average\n",
    "def generator_dailyAvg(inp):\n",
    "    if (np.count_nonzero(inp[5:] - replacementValue*np.ones(len(inp[5:]))) == 0):\n",
    "        return 0\n",
    "    return  (1.0*sum(inp[5:])/np.count_nonzero(inp[5:] - replacementValue*np.ones(len(inp[5:]))))\n",
    "    #return  math.log(1.0*sum(inp[5:])/len(inp[5:]))\n",
    "\n",
    "predictors[\"daily_avg\"] = downloads.apply(generator_dailyAvg,axis=1)\n",
    "#This one is bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate the polynomial coefficients\n",
    "def generator_coef(inp, coef):\n",
    "    return  np.polyfit(range(56),inp[5:],coef)[0]\n",
    "#Redo by ignoring the -1 / 0 ? w = [1110011]     w = np.not_equal(inp[5:],np.ones(len(inp[5:]))*replacementValue\n",
    "\n",
    "for c in range(4):\n",
    "    predictors[\"coef_\"+str(c)] = downloads.apply(generator_coef,axis=1,args=(c,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate the step max and min (we ignore the values of 0)\n",
    "def generator_maxStep(inp,maximum):\n",
    "    if (np.count_nonzero(inp[5:] - replacementValue*np.ones(len(inp[5:]))) == 0):\n",
    "        return 0\n",
    "    m = 0\n",
    "    for d in range(1,56):\n",
    "        if (inp[5+d]!=replacementValue and inp[4+d]!=replacementValue):\n",
    "            c = (inp[5+d]-inp[4+d])\n",
    "            if (maximum and m < c):\n",
    "                m = c\n",
    "            if ( not maximum and m > c):\n",
    "                m = c\n",
    "    return m\n",
    "                \n",
    "predictors[\"maxStep\"] = downloads.apply(generator_maxStep,axis=1,args=(True,))\n",
    "predictors[\"minStep\"] = downloads.apply(generator_maxStep,axis=1,args=(False,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Standard Deviation\n",
    "def generator_std(inp):\n",
    "    if (np.count_nonzero(inp[5:] - replacementValue*np.ones(len(inp[5:]))) == 0):\n",
    "        return 0\n",
    "    return np.std(inp[5:])\n",
    "\n",
    "predictors[\"std\"] = downloads.apply(generator_std,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Number of missing Values\n",
    "def generator_missing(inp):\n",
    "    return list(inp[5:]).count(replacementValue)\n",
    "    #return np.count_zero(inp[5:] -replacementValue*np.ones(len(inp[5:])))\n",
    "\n",
    "predictors[\"nb_missing\"] = downloads.apply(generator_missing,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#device\n",
    "def generator_iphone(inp):\n",
    "    if (inp[4] == \"iphone\"):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def generator_ipad(inp):\n",
    "    if(inp[4] == \"ipad\"):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "predictors[\"iphone\"] = downloads.apply(generator_iphone,axis = 1)\n",
    "predictors[\"ipad\"] = downloads.apply(generator_ipad,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Categories\n",
    "def generator_categories(inp,cat):\n",
    "    if (inp[3] == cat):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "for cat in list(set(downloads[\"category\"])):\n",
    "    predictors[cat] = downloads.apply(generator_categories,axis = 1,args=(cat,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the train_app_rating.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_reviews = reviews.groupby('id').agg('mean')\n",
    "avg_reviews['rating']\n",
    "predictors['avg_review'] = predictors.join(avg_reviews['rating'],on='id')['rating'].replace(\"NaN\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_reviews = reviews.groupby('id').agg('var')\n",
    "avg_reviews['rating']\n",
    "predictors['var_review'] = predictors.join(avg_reviews['rating'],on='id')['rating'].replace(\"NaN\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#raw ratings\n",
    "predictors = pd.merge(predictors, ratings.drop('Unnamed: 0', 1), how='left',\n",
    "                   on=[\"id\",\"name\",\"category\",\"device\"]).replace(\"NaN\",replacementValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Later we can compute the weighted average of sentiment scores based on reviewers.\n",
    "#add positive and negative columns to indicate the app's popularity\n",
    "print reviews.ix[id==predictors.id[0],:][\"sentiment_score\"]\n",
    "print reviews.ix[id==predictors.id[1],:][\"sentiment_score\"]\n",
    "avg_score = [0]*predictors.shape[0]\n",
    "predictors[\"positive\"] = [0]*predictors.shape[0]\n",
    "predictors[\"negative\"] = [0]*predictors.shape[0]\n",
    "for i in range(predictors.shape[0]):\n",
    "    avg_score[i] = reviews.ix[reviews[\"id\"]==predictors.id[i],:][\"sentiment_score\"].mean()\n",
    "    if avg_score[i]>0.55: \n",
    "        predictors[\"positive\"].values[i] = 1\n",
    "    elif avg_score[i]<0.45: predictors[\"negative\"].values[i] = 1\n",
    "#predictors[\"avg_sentiment_score\"] = avg_score\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use coeficients of metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    predictors[\"m\"+str(i+1)+\"_max\"] = np.zeros(predictors.shape[0]) \n",
    "    predictors[\"m\"+str(i+1)+\"_min\"] = np.zeros(predictors.shape[0])\n",
    "    predictors[\"m\"+str(i+1)+\"_mean\"] = np.zeros(predictors.shape[0])\n",
    "    predictors[\"m\"+str(i+1)+\"_std\"] = np.zeros(predictors.shape[0])\n",
    "    for j in range(3):\n",
    "        predictors[\"m\"+str(i+1)+\"_coef_\"+str(j)] = np.zeros(predictors.shape[0])\n",
    "\n",
    "for i in range(predictors.shape[0]):\n",
    "    if predictors[\"id\"].values[i] not in usages[\"id\"].values: continue\n",
    "    for j in range(4):\n",
    "        tmp = usages.ix[usages[\"id\"] == predictors[\"id\"].values[i],:]\n",
    "        time_series = np.array(tmp.ix[tmp[\"metric\"] == j+1,6:14])[0]   \n",
    "        if -1 in time_series: continue\n",
    "        X = np.array(range(8))\n",
    "        fit = np.polyfit(X,time_series,2)\n",
    "        for k in range(3):\n",
    "            predictors[\"m\"+str(j+1)+\"_coef_\"+str(k)].values[i] = fit[2-k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print predictors.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use coeficients of revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for j in range(3):\n",
    "    predictors[\"rev_coef_\"+str(j)] = np.zeros(predictors.shape[0])\n",
    "    predictors[\"rev_max\"] = np.zeros(predictors.shape[0])\n",
    "    predictors[\"rev_min\"] = np.zeros(predictors.shape[0])\n",
    "    predictors[\"rev_mean\"] = np.zeros(predictors.shape[0])\n",
    "    predictors[\"rev_std\"] = np.zeros(predictors.shape[0])\n",
    "for i in range(predictors.shape[0]):\n",
    "    \n",
    "    if predictors[\"id\"].values[i] in revenues[\"id\"].values: \n",
    "        curr_rev  = revenues.ix[revenues[\"id\"]== predictors[\"id\"].values[i],:]\n",
    "        if predictors[\"device\"].values[i] in curr_rev[\"device\"].values:\n",
    "            curr_rev = curr_rev.ix[curr_rev[\"device\"] == predictors[\"device\"].values[i],:]\n",
    "            time_series = np.array(curr_rev.ix[:,5:61])[0]\n",
    "            if -1 in time_series: continue \n",
    "            X = np.array(range(56))\n",
    "            fit = np.polyfit(X,time_series,2)\n",
    "            for k in range(3):  predictors[\"rev_coef_\"+str(k)].values[i] = fit[2-k]            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## To csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors.to_csv(\"predictors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = pd.read_csv('predictors.csv').drop('Unnamed: 0', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How good did the predictor perform   --> Start running from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imported libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "from sklearn import linear_model\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "#Creating of the input data\n",
    "downloads = pd.read_csv('train_app_downloads.csv')\n",
    "reviews = pd.read_csv('train_app_review.csv')\n",
    "ratings = pd.read_csv('train_app_rating.csv')\n",
    "usages = pd.read_csv('train_usage.csv')\n",
    "revenues = pd.read_csv('train_revenue.csv')\n",
    "output = pd.read_csv('train_final_downloads.csv')\n",
    "dateRange = pd.date_range('2015-03-01', periods=56).format(formatter=lambda x: x.strftime('%Y-%m-%d'))\n",
    "\n",
    "sentiment = pd.read_csv('sentiment.csv',header=-1).ix[:,0]\n",
    "sentiment.values[sentiment.values==0.0] = 0.5\n",
    "reviews[\"sentiment_score\"] = sentiment.values \n",
    "\n",
    "#We map -1 to 0 in the downloads (there are no 0 in the initial data)\n",
    "replacementValue=0\n",
    "downloads = downloads.replace(-1,replacementValue)\n",
    "\n",
    "#Minor corrections\n",
    "ratings = ratings.rename(columns={'start1': 'star1'})\n",
    "ratings = pd.merge(downloads.drop(dateRange,1), ratings.drop('Unnamed: 0', 1), how='left',\n",
    "                   on=[\"id\",\"name\",\"category\"]).replace(\"NaN\",replacementValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This is the metric we use to determine our performance\n",
    "def metric(y_pred,y_test,percent=1):\n",
    "    top = int(len(y_pred)/100*percent)\n",
    "    return (len(set([i[0] for i in sorted(enumerate(y_pred), key=lambda x:x[1],reverse=True)][0:top])\n",
    "       .intersection([i[0] for i in sorted(enumerate(y_test), key=lambda x:x[1],reverse=True)][0:top])\n",
    "               ))/(percent/100.0)/len(y_pred)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = pd.read_csv('predictors.csv').drop('Unnamed: 0', 1)\n",
    "predictors = predictors.fillna(0)\n",
    "\n",
    "#Standardize\n",
    "#for p in [ 'daily_avg', 'coef_0', 'coef_1', 'coef_2', 'coef_3', 'maxStep', 'minStep', 'std', 'nb_missing', 'ratingsWeightSum', 'star1', 'star2', 'star3', 'star4', 'star5']:\n",
    "#    predictors[p] = (predictors[p] - predictors[p].mean())/predictors[p].std() \n",
    "\n",
    "#drop daily\n",
    "#predictors = predictors.drop(\"week1\",1)\n",
    "#predictors = predictors.drop(\"week2\",1)\n",
    "#predictors = predictors.drop(\"week3\",1)\n",
    "#predictors = predictors.drop(\"week4\",1)\n",
    "#predictors = predictors.drop(\"week5\",1)\n",
    "#predictors = predictors.drop(\"week6\",1)\n",
    "#predictors = predictors.drop(\"week7\",1)\n",
    "#predictors = predictors.drop(\"week8\",1)\n",
    "# predictors = predictors.drop(\"daily_avg\",1)\n",
    "# predictors = predictors.drop(\"star1\",1)\n",
    "# predictors = predictors.drop(\"star2\",1)\n",
    "# predictors = predictors.drop(\"star3\",1)\n",
    "# predictors = predictors.drop(\"star4\",1)\n",
    "# predictors = predictors.drop(\"star5\",1)\n",
    "# predictors = predictors.drop(list(set(downloads[\"category\"])),1)\n",
    "# predictors = predictors.drop(\"coef_0\",1)\n",
    "# predictors = predictors.drop(\"coef_1\",1)\n",
    "# predictors = predictors.drop(\"coef_2\",1)\n",
    "# predictors = predictors.drop(\"coef_3\",1)\n",
    "# predictors = predictors.drop(\"maxStep\",1)\n",
    "# predictors = predictors.drop(\"minStep\",1)\n",
    "# predictors = predictors.drop(\"std\",1)\n",
    "# predictors = predictors.drop(\"nb_missing\",1)\n",
    "#predictors = predictors.drop(\"iphone\",1)\n",
    "# predictors = predictors.drop(\"ipad\",1)\n",
    "\n",
    "# predictors = predictors.drop('avg_sentiment_score',1)\n",
    "# predictors = predictors.drop('positive',1)\n",
    "# predictors = predictors.drop('negative',1)\n",
    "# predictors = predictors.drop('m1_coef_0',1)\n",
    "# predictors = predictors.drop('m1_coef_1',1)\n",
    "# predictors = predictors.drop('m1_coef_2',1)\n",
    "# predictors = predictors.drop('m2_coef_0',1)\n",
    "# predictors = predictors.drop('m2_coef_1',1)\n",
    "# predictors = predictors.drop('m2_coef_2',1)\n",
    "# predictors = predictors.drop('m3_coef_0',1)\n",
    "# predictors = predictors.drop('m3_coef_1',1)\n",
    "# predictors = predictors.drop('m3_coef_2',1)\n",
    "# predictors = predictors.drop('m4_coef_0',1)\n",
    "# predictors = predictors.drop('m4_coef_1',1)\n",
    "# predictors = predictors.drop('m4_coef_2',1)\n",
    "predictors = predictors.drop('rev_coef_0',1)\n",
    "predictors = predictors.drop('rev_coef_1',1)\n",
    "predictors = predictors.drop('rev_coef_2',1)\n",
    "\n",
    "\n",
    "old_top = []\n",
    "new_top = []\n",
    "\n",
    "test_frac = 0.31  #Fraction of test points\n",
    "N = 20   #number of iterations\n",
    "np.random.seed(1)\n",
    "for i in range(1,N):\n",
    "    r = np.random.randint(1,429496729)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors.as_matrix()[:,4:12], output.as_matrix()[:,5], test_size=test_frac, random_state=r)\n",
    "    X_test = X_test[0:10000]\n",
    "    y_test = y_test[0:10000]\n",
    "    old_mod=linear_model.LinearRegression(fit_intercept=False).fit(X_train,y_train)\n",
    "    old_y_pred =  old_mod.predict(X_test)\n",
    "    old_top.append(metric(old_y_pred,y_test))\n",
    "    \n",
    "np.random.seed(1)\n",
    "for i in range(1,N):\n",
    "    r = np.random.randint(1,429496729)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors.as_matrix()[:,4:], output.as_matrix()[:,5], test_size=test_frac, random_state=r)\n",
    "    X_test = X_test[0:10000]\n",
    "    y_test = y_test[0:10000]\n",
    "    #mod=linear_model.LinearRegression(fit_intercept=False).fit(X_train,y_train)\n",
    "    mod=linear_model.Lasso(alpha=100,fit_intercept=False).fit(X_train,y_train)\n",
    "    #mod= RandomForestRegressor(max_features = 1.0/3.0,n_estimators = 100) \n",
    "    y_pred =  mod.predict(X_test)\n",
    "    new_top.append(metric(y_pred,y_test))\n",
    "if (N<300):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(range(1,N),old_top,label=\"old\")\n",
    "    plt.plot(range(1,N),new_top,label=\"performance\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "print 1.0*sum(old_top)/len(old_top)\n",
    "print list(predictors.columns.values)\n",
    "print 1.0*sum(new_top)/len(new_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "54.8947368421  \n",
    "55.0526315789  #with #missing and Lasso <br/>\n",
    "54.7894736842  #including raw ratings Lasso :( <br/>\n",
    "55.0526315789  #with #missing weightedSumRatings and Lasso <br/>\n",
    "55.1052631579  #adding the categories <br/>\n",
    "55.3684210526  #adding average sentiment score and positive/negative label <br/>\n",
    "55.5263157895  #adding coeficients of usages\n",
    "\n",
    "\n",
    "With 10000 only:\n",
    "54.8947368421 all\n",
    "\n",
    "56.5263157895 on Lasso all but 'rev_coef_i'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso All alpha = 100\n",
    "['id', 'name', 'category', 'device', 'week_1', 'week_2', 'week_3', 'week_4', 'week_5', 'week_6', 'week_7', 'week_8', 'daily_avg', 'coef_0', 'coef_1', 'coef_2', 'coef_3', 'maxStep', 'minStep', 'std', 'nb_missing', 'iphone', 'ipad', 'Productivity', 'Entertainment', 'Travel', 'Sports', 'Music', 'Shopping', 'Finance', 'Business', 'Navigation', 'Food and Drink', 'Utilities', 'Newsstand', 'Health and Fitness', 'News', 'Lifestyle', 'Medical', 'Weather', 'Games', 'Catalogs', 'Social Networking', 'Photo and Video', 'Reference', 'Books', 'Education', 'avg_review', 'var_review', 'star1', 'star2', 'star3', 'star4', 'star5', 'positive', 'negative', 'm1_coef_0', 'm1_coef_1', 'm1_coef_2', 'm2_coef_0', 'm2_coef_1', 'm2_coef_2', 'm3_coef_0', 'm3_coef_1', 'm3_coef_2', 'm4_coef_0', 'm4_coef_1', 'm4_coef_2']\n",
    "57.1052631579"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Estimation of alpha for the Lasso regression \"cros validation\" like approach\n",
    "\n",
    "for a in np.arange(90,110,5):\n",
    "\n",
    "    old_top = []\n",
    "    new_top = []\n",
    "\n",
    "    test_frac = 0.31  #Fraction of test points\n",
    "    N = 20   #number of iterations\n",
    "\n",
    "\n",
    "    np.random.seed(1)\n",
    "    for i in range(1,N):\n",
    "        r = np.random.randint(1,429496729)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(predictors.as_matrix()[:,4:], output.as_matrix()[:,5], test_size=test_frac, random_state=r)\n",
    "        X_test = X_test[0:10000]\n",
    "        y_test = y_test[0:10000]\n",
    "        mod=linear_model.Lasso(alpha=a,fit_intercept=False).fit(X_train,y_train) \n",
    "        y_pred =  mod.predict(X_test)\n",
    "        new_top.append(metric(y_pred,y_test))\n",
    "\n",
    "\n",
    "    print a\n",
    "    print 1.0*sum(new_top)/len(new_top)\n",
    "    print \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation k-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 32\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "predictors = pd.read_csv('predictors.csv').drop('Unnamed: 0', 1)\n",
    "predictors = predictors.fillna(0)\n",
    "\n",
    "#Standardize\n",
    "#for p in [ 'daily_avg', 'coef_0', 'coef_1', 'coef_2', 'coef_3', 'maxStep', 'minStep', 'std', 'nb_missing', 'ratingsWeightSum', 'star1', 'star2', 'star3', 'star4', 'star5']:\n",
    "#    predictors[p] = (predictors[p] - predictors[p].mean())/predictors[p].std() \n",
    "\n",
    "#drop daily\n",
    "#predictors = predictors.drop(\"week1\",1)\n",
    "#predictors = predictors.drop(\"week2\",1)\n",
    "#predictors = predictors.drop(\"week3\",1)\n",
    "#predictors = predictors.drop(\"week4\",1)\n",
    "#predictors = predictors.drop(\"week5\",1)\n",
    "#predictors = predictors.drop(\"week6\",1)\n",
    "#predictors = predictors.drop(\"week7\",1)\n",
    "#predictors = predictors.drop(\"week8\",1)\n",
    "# predictors = predictors.drop(\"daily_avg\",1)\n",
    "# predictors = predictors.drop(\"star1\",1)\n",
    "# predictors = predictors.drop(\"star2\",1)\n",
    "# predictors = predictors.drop(\"star3\",1)\n",
    "# predictors = predictors.drop(\"star4\",1)\n",
    "# predictors = predictors.drop(\"star5\",1)\n",
    "# predictors = predictors.drop(list(set(downloads[\"category\"])),1)\n",
    "# predictors = predictors.drop(\"coef_0\",1)\n",
    "# predictors = predictors.drop(\"coef_1\",1)\n",
    "# predictors = predictors.drop(\"coef_2\",1)\n",
    "# predictors = predictors.drop(\"coef_3\",1)\n",
    "# predictors = predictors.drop(\"maxStep\",1)\n",
    "# predictors = predictors.drop(\"minStep\",1)\n",
    "# predictors = predictors.drop(\"std\",1)\n",
    "# predictors = predictors.drop(\"nb_missing\",1)\n",
    "#predictors = predictors.drop(\"iphone\",1)\n",
    "# predictors = predictors.drop(\"ipad\",1)\n",
    "\n",
    "# predictors = predictors.drop('avg_sentiment_score',1)\n",
    "# predictors = predictors.drop('positive',1)\n",
    "# predictors = predictors.drop('negative',1)\n",
    "# predictors = predictors.drop('m1_coef_0',1)\n",
    "# predictors = predictors.drop('m1_coef_1',1)\n",
    "# predictors = predictors.drop('m1_coef_2',1)\n",
    "# predictors = predictors.drop('m2_coef_0',1)\n",
    "# predictors = predictors.drop('m2_coef_1',1)\n",
    "# predictors = predictors.drop('m2_coef_2',1)\n",
    "# predictors = predictors.drop('m3_coef_0',1)\n",
    "# predictors = predictors.drop('m3_coef_1',1)\n",
    "# predictors = predictors.drop('m3_coef_2',1)\n",
    "# predictors = predictors.drop('m4_coef_0',1)\n",
    "# predictors = predictors.drop('m4_coef_1',1)\n",
    "# predictors = predictors.drop('m4_coef_2',1)\n",
    "predictors = predictors.drop('rev_coef_0',1)\n",
    "predictors = predictors.drop('rev_coef_1',1)\n",
    "predictors = predictors.drop('rev_coef_2',1)\n",
    "\n",
    "kf = KFold(32339, n_folds=K)\n",
    "old_top = []\n",
    "new_top = []\n",
    "for train, test in kf:\n",
    "    #base model\n",
    "    old_mod=linear_model.LinearRegression(fit_intercept=False).fit(predictors.as_matrix()[train,4:12], output.as_matrix()[train,5])\n",
    "    old_y_pred =  old_mod.predict(predictors.as_matrix()[test,4:12])\n",
    "    old_top.append(metric(old_y_pred,output.as_matrix()[test,5]))\n",
    "    #model\n",
    "    #mod=linear_model.LinearRegression(fit_intercept=False).fit(predictors.as_matrix()[train,4:], output.as_matrix()[train,5]) \n",
    "    mod=linear_model.Lasso(alpha=100,fit_intercept=False).fit(predictors.as_matrix()[train,4:], output.as_matrix()[train,5]) \n",
    "    #mod = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0, loss='ls').fit(predictors.as_matrix()[train,4:], output.as_matrix()[train,5]) \n",
    "    #mod= RandomForestRegressor(max_features = 1.0/3.0,n_estimators = 100).fit(predictors.as_matrix()[train,4:], output.as_matrix()[train,5])\n",
    "    y_pred =  mod.predict(predictors.as_matrix()[test,4:])\n",
    "    new_top.append(metric(y_pred,output.as_matrix()[test,5]))\n",
    "print 1.0*sum(old_top)/len(old_top)\n",
    "print list(predictors.columns.values)\n",
    "print 1.0*sum(new_top)/len(new_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso all\n",
    "53.4375\n",
    "['id', 'name', 'category', 'device', 'week_1', 'week_2', 'week_3', 'week_4', 'week_5', 'week_6', 'week_7', 'week_8', 'daily_avg', 'coef_0', 'coef_1', 'coef_2', 'coef_3', 'maxStep', 'minStep', 'std', 'nb_missing', 'iphone', 'ipad', 'Productivity', 'Entertainment', 'Travel', 'Sports', 'Music', 'Shopping', 'Finance', 'Business', 'Navigation', 'Food and Drink', 'Utilities', 'Newsstand', 'Health and Fitness', 'News', 'Lifestyle', 'Medical', 'Weather', 'Games', 'Catalogs', 'Social Networking', 'Photo and Video', 'Reference', 'Books', 'Education', 'avg_review', 'var_review', 'star1', 'star2', 'star3', 'star4', 'star5', 'positive', 'negative', 'm1_coef_0', 'm1_coef_1', 'm1_coef_2', 'm2_coef_0', 'm2_coef_1', 'm2_coef_2', 'm3_coef_0', 'm3_coef_1', 'm3_coef_2', 'm4_coef_0', 'm4_coef_1', 'm4_coef_2']\n",
    "54.0625"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "#mod=linear_model.Lasso(alpha=100,fit_intercept=False)\n",
    "mod=linear_model.LinearRegression(fit_intercept=False)\n",
    "#mod= RandomForestRegressor(max_features = 1.0/3.0,n_estimators = 100) \n",
    "rfe = RFE(estimator=mod, n_features_to_select=1, step=1)\n",
    "rfe.fit(predictors.as_matrix()[train,4:], output.as_matrix()[train,5])\n",
    "ranking = rfe.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for e in sorted(zip(predictors.columns[4:],ranking),key=lambda x: x[1]):\n",
    "    print e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt at boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Boosting\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "new_top = []\n",
    "\n",
    "test_frac = 0.31  #Fraction of test points\n",
    "N = 20   #number of iterations\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "for i in range(1,N):\n",
    "    r = np.random.randint(1,429496729)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors.as_matrix()[:,4:], output.as_matrix()[:,5], test_size=test_frac, random_state=r)\n",
    "    X_test = X_test[0:10000]\n",
    "    y_test = y_test[0:10000]\n",
    "    #mod=linear_model.LinearRegression(fit_intercept=False).fit(X_train,y_train)\n",
    "    #mod=linear_model.Lasso(alpha=a,fit_intercept=False).fit(X_train,y_train) \n",
    "    mod = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0, loss='ls').fit(X_train,y_train)\n",
    "    mod= RandomForestRegressor(max_features = 1.0/3.0,n_estimators = 100) \n",
    "    y_pred =  mod.predict(X_test)\n",
    "    new_top.append(metric(y_pred,y_test))\n",
    "\n",
    "print 1.0*sum(new_top)/len(new_top)\n",
    "print \" \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output.sort_values(by= 'cumulative_downloads_2016-02',ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
