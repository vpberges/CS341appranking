{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this markdown to generate the predictors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imported libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import langdetect\n",
    "%matplotlib inline  \n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "#Creating of the input data\n",
    "downloads = pd.read_csv('train_app_downloads.csv')\n",
    "reviews = pd.read_csv('train_app_review.csv')\n",
    "ratings = pd.read_csv('train_app_rating.csv')\n",
    "usages = pd.read_csv('train_usage.csv')\n",
    "revenues = pd.read_csv('train_revenue.csv')\n",
    "output = pd.read_csv('train_final_downloads.csv')\n",
    "dateRange = pd.date_range('2015-03-01', periods=56).format(formatter=lambda x: x.strftime('%Y-%m-%d'))\n",
    "\n",
    "sentiment = pd.read_csv('sentiment.csv',header=-1).ix[:,0]\n",
    "sentiment.values[sentiment.values==0.0] = 0.5\n",
    "reviews[\"sentiment_score\"] = sentiment.values \n",
    "\n",
    "#We map -1 to 0 in the downloads (there are no 0 in the initial data)\n",
    "replacementValue=0\n",
    "downloads = downloads.replace(-1,replacementValue)\n",
    "\n",
    "#Minor corrections\n",
    "ratings = ratings.rename(columns={'start1': 'star1'})\n",
    "ratings = pd.merge(downloads.drop(dateRange,1), ratings.drop('Unnamed: 0', 1), how='left',\n",
    "                   on=[\"id\",\"name\",\"category\"]).replace(\"NaN\",replacementValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the predictors matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = pd.concat([downloads[\"id\"],downloads[\"name\"],downloads[\"category\"],downloads[\"device\"]],\n",
    "                       axis=1,keys=[\"id\",\"name\",\"category\",\"device\"])\n",
    "predictors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the train_app_downloads.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate the logWeekly average\n",
    "def generator_weekAvg(inp,w):\n",
    "    if (np.count_nonzero(inp[5+w*7:12+w*7] - replacementValue*np.ones(len(inp[5+w*7:12+w*7]))) == 0):\n",
    "        return 0\n",
    "    return  math.log(1.0*sum(inp[5+w*7:12+w*7])/np.count_nonzero(inp[5+w*7:12+w*7] \n",
    "                                                                 - replacementValue*np.ones(len(inp[5+w*7:12+w*7]))))\n",
    "\n",
    "for w in range(8):\n",
    "    predictors[\"week_\"+str(w+1)] = downloads.apply(generator_weekAvg,axis=1,args=(w,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate the daily average\n",
    "def generator_dailyAvg(inp):\n",
    "    if (np.count_nonzero(inp[5:] - replacementValue*np.ones(len(inp[5:]))) == 0):\n",
    "        return 0\n",
    "    return  (1.0*sum(inp[5:])/np.count_nonzero(inp[5:] - replacementValue*np.ones(len(inp[5:]))))\n",
    "    #return  math.log(1.0*sum(inp[5:])/len(inp[5:]))\n",
    "\n",
    "predictors[\"daily_avg\"] = downloads.apply(generator_dailyAvg,axis=1)\n",
    "#This one is bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate the polynomial coefficients\n",
    "def generator_coef(inp, coef):\n",
    "    return  np.polyfit(range(56),inp[5:],coef)[0]\n",
    "#Redo by ignoring the -1 / 0 ? w = [1110011]     w = np.not_equal(inp[5:],np.ones(len(inp[5:]))*replacementValue\n",
    "\n",
    "for c in range(4):\n",
    "    predictors[\"coef_\"+str(c)] = downloads.apply(generator_coef,axis=1,args=(c,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate the step max and min (we ignore the values of 0)\n",
    "def generator_maxStep(inp,maximum):\n",
    "    if (np.count_nonzero(inp[5:] - replacementValue*np.ones(len(inp[5:]))) == 0):\n",
    "        return 0\n",
    "    m = 0\n",
    "    for d in range(1,56):\n",
    "        if (inp[5+d]!=replacementValue and inp[4+d]!=replacementValue):\n",
    "            c = (inp[5+d]-inp[4+d])\n",
    "            if (maximum and m < c):\n",
    "                m = c\n",
    "            if ( not maximum and m > c):\n",
    "                m = c\n",
    "    return m\n",
    "                \n",
    "predictors[\"maxStep\"] = downloads.apply(generator_maxStep,axis=1,args=(True,))\n",
    "predictors[\"minStep\"] = downloads.apply(generator_maxStep,axis=1,args=(False,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Standard Deviation\n",
    "def generator_std(inp):\n",
    "    if (np.count_nonzero(inp[5:] - replacementValue*np.ones(len(inp[5:]))) == 0):\n",
    "        return 0\n",
    "    return np.std(inp[5:])\n",
    "\n",
    "predictors[\"std\"] = downloads.apply(generator_std,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Number of missing Values\n",
    "def generator_missing(inp):\n",
    "    return list(inp[5:]).count(replacementValue)\n",
    "    #return np.count_zero(inp[5:] -replacementValue*np.ones(len(inp[5:])))\n",
    "\n",
    "predictors[\"nb_missing\"] = downloads.apply(generator_missing,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#device\n",
    "def generator_iphone(inp):\n",
    "    if (inp[4] == \"iphone\"):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def generator_ipad(inp):\n",
    "    if(inp[4] == \"ipad\"):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "predictors[\"iphone\"] = downloads.apply(generator_iphone,axis = 1)\n",
    "predictors[\"ipad\"] = downloads.apply(generator_ipad,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Categories\n",
    "def generator_categories(inp,cat):\n",
    "    if (inp[3] == cat):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "for cat in list(set(downloads[\"category\"])):\n",
    "    predictors[cat] = downloads.apply(generator_categories,axis = 1,args=(cat,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def detect_language(x):\n",
    "    try:\n",
    "        detected = langdetect.detect(x.decode('utf8','ignore'))\n",
    "        if detected in ['ja']:\n",
    "            return 'japanese'\n",
    "        elif detected in ['zh-cn']:\n",
    "            return 'chinese'\n",
    "        elif detected in ['ko']:\n",
    "            return 'korean'\n",
    "        return 'other'\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def set_lang_categories(x, cat):\n",
    "    if x == cat:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "lang_series = predictors['name'].apply(detect_language)\n",
    "for cat in list(set(lang_series)):\n",
    "    predictors[cat] = lang_series.apply(set_lang_categories, args=(cat,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the train_app_rating.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_reviews = reviews.groupby('id').agg('mean')\n",
    "avg_reviews['rating']\n",
    "predictors['avg_review'] = predictors.join(avg_reviews['rating'],on='id')['rating'].replace(\"NaN\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_reviews = reviews.groupby('id').agg('var')\n",
    "avg_reviews['rating']\n",
    "predictors['var_review'] = predictors.join(avg_reviews['rating'],on='id')['rating'].replace(\"NaN\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings['num_ratings'] = ratings.ix[:,['star1','star2','star3','star4','star5']].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#raw ratings\n",
    "predictors = pd.merge(predictors, ratings.drop('Unnamed: 0', 1), how='left',\n",
    "                   on=[\"id\",\"name\",\"category\",\"device\"]).replace(\"NaN\",replacementValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_versions = reviews.groupby('id').version.nunique()\n",
    "num_versions.name = 'num_versions'\n",
    "predictors = predictors.join(num_versions, how='left', on='id')\n",
    "predictors['num_versions'] = predictors['num_versions'].replace('NaN',0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Later we can compute the weighted average of sentiment scores based on reviewers.\n",
    "#add positive and negative columns to indicate the app's popularity\n",
    "print reviews.ix[id==predictors.id[0],:][\"sentiment_score\"]\n",
    "print reviews.ix[id==predictors.id[1],:][\"sentiment_score\"]\n",
    "avg_score = [0]*predictors.shape[0]\n",
    "predictors[\"positive\"] = [0]*predictors.shape[0]\n",
    "predictors[\"negative\"] = [0]*predictors.shape[0]\n",
    "for i in range(predictors.shape[0]):\n",
    "    avg_score[i] = reviews.ix[reviews[\"id\"]==predictors.id[i],:][\"sentiment_score\"].mean()\n",
    "    if avg_score[i]>0.55: \n",
    "        predictors[\"positive\"].values[i] = 1\n",
    "    elif avg_score[i]<0.45: predictors[\"negative\"].values[i] = 1\n",
    "#predictors[\"avg_sentiment_score\"] = avg_score\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use coeficients of metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    predictors[\"m\"+str(i+1)+\"_max\"] = np.zeros(predictors.shape[0]) \n",
    "    predictors[\"m\"+str(i+1)+\"_min\"] = np.zeros(predictors.shape[0])\n",
    "    predictors[\"m\"+str(i+1)+\"_mean\"] = np.zeros(predictors.shape[0])\n",
    "    predictors[\"m\"+str(i+1)+\"_std\"] = np.zeros(predictors.shape[0])\n",
    "    for j in range(3):\n",
    "        predictors[\"m\"+str(i+1)+\"_coef_\"+str(j)] = np.zeros(predictors.shape[0])\n",
    "\n",
    "for i in range(predictors.shape[0]):\n",
    "    if predictors[\"id\"].values[i] not in usages[\"id\"].values: continue\n",
    "    for j in range(4):\n",
    "        tmp = usages.ix[usages[\"id\"] == predictors[\"id\"].values[i],:]\n",
    "        time_series = np.array(tmp.ix[tmp[\"metric\"] == j+1,6:14])[0]   \n",
    "        if -1 in time_series: continue\n",
    "        X = np.array(range(8))\n",
    "        fit = np.polyfit(X,time_series,2)\n",
    "        for k in range(3):\n",
    "            predictors[\"m\"+str(j+1)+\"_coef_\"+str(k)].values[i] = fit[2-k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print predictors.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use coeficients of revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for j in range(3):\n",
    "    predictors[\"rev_coef_\"+str(j)] = np.zeros(predictors.shape[0])\n",
    "    predictors[\"rev_max\"] = np.zeros(predictors.shape[0])\n",
    "    predictors[\"rev_min\"] = np.zeros(predictors.shape[0])\n",
    "    predictors[\"rev_mean\"] = np.zeros(predictors.shape[0])\n",
    "    predictors[\"rev_std\"] = np.zeros(predictors.shape[0])\n",
    "for i in range(predictors.shape[0]):\n",
    "    \n",
    "    if predictors[\"id\"].values[i] in revenues[\"id\"].values: \n",
    "        curr_rev  = revenues.ix[revenues[\"id\"]== predictors[\"id\"].values[i],:]\n",
    "        if predictors[\"device\"].values[i] in curr_rev[\"device\"].values:\n",
    "            curr_rev = curr_rev.ix[curr_rev[\"device\"] == predictors[\"device\"].values[i],:]\n",
    "            time_series = np.array(curr_rev.ix[:,5:61])[0]\n",
    "            if -1 in time_series: continue \n",
    "            X = np.array(range(56))\n",
    "            fit = np.polyfit(X,time_series,2)\n",
    "            for k in range(3):  predictors[\"rev_coef_\"+str(k)].values[i] = fit[2-k]            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## To csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors.to_csv(\"predictors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = pd.read_csv('predictors.csv').drop('Unnamed: 0', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How good did the predictor perform   --> Start running from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imported libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "#Creating of the input data\n",
    "downloads = pd.read_csv('train_app_downloads.csv')\n",
    "reviews = pd.read_csv('train_app_review.csv')\n",
    "ratings = pd.read_csv('train_app_rating.csv')\n",
    "usages = pd.read_csv('train_usage.csv')\n",
    "revenues = pd.read_csv('train_revenue.csv')\n",
    "output = pd.read_csv('train_final_downloads.csv')\n",
    "dateRange = pd.date_range('2015-03-01', periods=56).format(formatter=lambda x: x.strftime('%Y-%m-%d'))\n",
    "\n",
    "sentiment = pd.read_csv('sentiment.csv',header=-1).ix[:,0]\n",
    "sentiment.values[sentiment.values==0.0] = 0.5\n",
    "reviews[\"sentiment_score\"] = sentiment.values \n",
    "\n",
    "#We map -1 to 0 in the downloads (there are no 0 in the initial data)\n",
    "replacementValue=0\n",
    "downloads = downloads.replace(-1,replacementValue)\n",
    "\n",
    "#Minor corrections\n",
    "ratings = ratings.rename(columns={'start1': 'star1'})\n",
    "ratings = pd.merge(downloads.drop(dateRange,1), ratings.drop('Unnamed: 0', 1), how='left',\n",
    "                   on=[\"id\",\"name\",\"category\"]).replace(\"NaN\",replacementValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This is the metric we use to determine our performance\n",
    "def metric(y_pred,y_test,percent=1):\n",
    "    top = int(len(y_pred)/100.0*percent)\n",
    "    return (len(set([i[0] for i in sorted(enumerate(y_pred), key=lambda x:x[1],reverse=True)][0:top])\n",
    "       .intersection([i[0] for i in sorted(enumerate(y_test), key=lambda x:x[1],reverse=True)][0:top])\n",
    "               ))/(percent/100.0)/len(y_pred)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = pd.read_csv('predictors.csv').drop('Unnamed: 0', 1)\n",
    "predictors = predictors.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Use a model to do a forward recursive predictor selection\n",
    "mod=linear_model.Lasso(alpha=100,fit_intercept=False)\n",
    "#mod=linear_model.LinearRegression(fit_intercept=False)\n",
    "#mod= RandomForestRegressor(max_features = 1.0/3.0,n_estimators = 100) \n",
    "#mod= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100) \n",
    "rfe = RFE(estimator=mod, n_features_to_select=1, step=1)\n",
    "rfe.fit(predictors.as_matrix()[:,4:], output.as_matrix()[:,5])\n",
    "ranking = rfe.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for e in [(i[0],i[1]) for i in sorted(zip(predictors.columns[4:],ranking),key=lambda x: x[1])]:\n",
    "    print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#manually drop predictors \n",
    "#predictors_to_drop = []\n",
    "nb_pred_to_keep = 40\n",
    "predictors_to_drop = [i[0] for i in sorted(zip(predictors.columns[4:],ranking),key=lambda x: x[1])][nb_pred_to_keep:]\n",
    "for col in predictors_to_drop:\n",
    "    predictors = predictors.drop(col,1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Classification + Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print list(predictors.columns.values) \n",
    "np.random.seed(1)\n",
    "K = 5\n",
    "\n",
    "top_percent_classif = 20\n",
    "\n",
    "\n",
    "kf = KFold(len(predictors), n_folds=K)\n",
    "old_top = []\n",
    "new_top = []\n",
    "top_10 = []\n",
    "for train, test in kf:\n",
    "    #base model\n",
    "    old_mod=linear_model.LinearRegression(fit_intercept=False).fit(predictors.as_matrix()[train,4:12], output.as_matrix()[train,5])\n",
    "    old_y_pred =  old_mod.predict(predictors.as_matrix()[test,4:12])\n",
    "    old_top.append(metric(old_y_pred,output.as_matrix()[test,5]))\n",
    "    \n",
    "    #model to determine the top 10%\n",
    "    mod_class10= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100).fit(predictors.as_matrix()[train,4:], output.as_matrix()[train,5])\n",
    "    \n",
    "    y_pred =  mod_class10.predict(predictors.as_matrix()[test,4:])\n",
    "    \n",
    "    #estimate of the top 10% of the test set\n",
    "    estimate_class10 = predictors.iloc[test].copy()\n",
    "    estimate_class10[\"firstEstimate\"] = y_pred\n",
    "    estimate_class10 = estimate_class10.sort_values(by= \"firstEstimate\",ascending = False).iloc[1:int(1.0*top_percent_classif/100.0*len(estimate_class10))]\n",
    "    estimate_class10 = estimate_class10.drop(\"firstEstimate\",1)\n",
    "    #estimate_class10 = estimate_class10.sort_values(by= \"week_8\",ascending = False).iloc[1:int(1.0*top_percent_classif/100.0*len(estimate_class10))]\n",
    "\n",
    "    \n",
    "    #top 10% of the trainning set\n",
    "    output_train_top_10_precent = output.iloc[train].copy().sort_values(by= 'cumulative_downloads_2016-02',ascending = False).iloc[1:int(1.0*top_percent_classif/100.0*len(output.iloc[train]))].drop('Unnamed: 0',1)\n",
    "    predictor_train_top_10_precent = output_train_top_10_precent.merge(predictors, how='left', on=[\"id\",\"name\",\"category\",\"device\"]).copy()\n",
    "    predictor_train_top_10_precent = predictor_train_top_10_precent.drop('cumulative_downloads_2016-02',1)\n",
    "    #predictor_train_top_10_precent = predictor_train_top_10_precent.drop('firstEstimate',1)\n",
    "    \n",
    "\n",
    "    #This is the actual top 1% of the test set\n",
    "    output_test_top_1_precent = output.iloc[test].sort_values(by= 'cumulative_downloads_2016-02',ascending = False).iloc[1:int(0.01*len(output.iloc[test]))].copy()\n",
    "\n",
    "    #second model\n",
    "    #mod_top1= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100).fit(predictor_train_top_10_precent.as_matrix()[:,4:], output_train_top_10_precent[\"cumulative_downloads_2016-02\"].as_matrix())\n",
    "    mod_top1= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100).fit(predictor_train_top_10_precent.as_matrix()[:,4:], np.log(output_train_top_10_precent[\"cumulative_downloads_2016-02\"].as_matrix()))\n",
    "    #mod_top1=linear_model.Lasso(alpha=100,fit_intercept=False).fit(predictor_train_top_10_precent.as_matrix()[:,4:], np.log(output_train_top_10_precent[\"cumulative_downloads_2016-02\"].as_matrix()))\n",
    "    #mod_top1=GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0, loss='ls').fit(predictor_train_top_10_precent.as_matrix()[:,4:], (output_train_top_10_precent[\"cumulative_downloads_2016-02\"].as_matrix()))\n",
    "    \n",
    "    y_pred_2 =  mod_top1.predict(estimate_class10.as_matrix()[:,4:])\n",
    "    \n",
    "    estimate_top1 = estimate_class10.copy()\n",
    "    estimate_top1[\"secondEstimate\"] = y_pred_2\n",
    "    estimate_top1 = estimate_top1.sort_values(by= \"secondEstimate\",ascending = False).iloc[1:int(0.01*len(output.iloc[test]))]\n",
    "    \n",
    "    estimation_error = len(estimate_top1.merge(output_test_top_1_precent, how='inner', on=[\"id\",\"name\",\"category\",\"device\"]))*100.0/len(output_test_top_1_precent)\n",
    "    new_top.append(estimation_error)\n",
    "    top_10.append(len(estimate_class10.merge(output_test_top_1_precent, how='inner', on=[\"id\",\"name\",\"category\",\"device\"]))*100.0/len(output_test_top_1_precent))\n",
    "       \n",
    "print 1.0*sum(old_top)/len(old_top)\n",
    "print 1.0*sum(top_10)/len(top_10)\n",
    "print 1.0*sum(new_top)/len(new_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(predictor_train_top_10_precent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old_top = []\n",
    "new_top = []\n",
    "\n",
    "test_frac = 0.31  #Fraction of test points\n",
    "N = 20   #number of iterations\n",
    "np.random.seed(1)\n",
    "for i in range(1,N):\n",
    "    r = np.random.randint(1,429496729)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors.as_matrix()[:,4:12], output.as_matrix()[:,5], test_size=test_frac, random_state=r)\n",
    "    X_test = X_test[0:10000]\n",
    "    y_test = y_test[0:10000]\n",
    "    old_mod=linear_model.LinearRegression(fit_intercept=False).fit(X_train,y_train)\n",
    "    old_y_pred =  old_mod.predict(X_test)\n",
    "    old_top.append(metric(old_y_pred,y_test))\n",
    "    \n",
    "np.random.seed(1)\n",
    "for i in range(1,N):\n",
    "    r = np.random.randint(1,429496729)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors.as_matrix()[:,4:], output.as_matrix()[:,5], test_size=test_frac, random_state=r)\n",
    "    X_test = X_test[0:10000]\n",
    "    y_test = y_test[0:10000]\n",
    "    #mod=linear_model.LinearRegression(fit_intercept=False).fit(X_train,y_train)\n",
    "    #mod=linear_model.Lasso(alpha=100,fit_intercept=False).fit(X_train,y_train)\n",
    "    #mod= RandomForestRegressor(max_features = 1.0/3.0,n_estimators = 100).fit(X_train,y_train)\n",
    "    mod= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100).fit(X_train,y_train)\n",
    "    y_pred =  mod.predict(X_test)\n",
    "    new_top.append(metric(y_pred,y_test))\n",
    "if (N<300):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(range(1,N),old_top,label=\"old\")\n",
    "    plt.plot(range(1,N),new_top,label=\"performance\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "print 1.0*sum(old_top)/len(old_top)\n",
    "print list(predictors.columns.values)\n",
    "print 1.0*sum(new_top)/len(new_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "54.8947368421  \n",
    "55.0526315789  #with #missing and Lasso <br/>\n",
    "54.7894736842  #including raw ratings Lasso :( <br/>\n",
    "55.0526315789  #with #missing weightedSumRatings and Lasso <br/>\n",
    "55.1052631579  #adding the categories <br/>\n",
    "55.3684210526  #adding average sentiment score and positive/negative label <br/>\n",
    "55.5263157895  #adding coeficients of usages\n",
    "\n",
    "\n",
    "With 10000 only:\n",
    "54.8947368421 all\n",
    "\n",
    "56.5263157895 on Lasso all but 'rev_coef_i'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso All alpha = 100\n",
    "['id', 'name', 'category', 'device', 'week_1', 'week_2', 'week_3', 'week_4', 'week_5', 'week_6', 'week_7', 'week_8', 'daily_avg', 'coef_0', 'coef_1', 'coef_2', 'coef_3', 'maxStep', 'minStep', 'std', 'nb_missing', 'iphone', 'ipad', 'Productivity', 'Entertainment', 'Travel', 'Sports', 'Music', 'Shopping', 'Finance', 'Business', 'Navigation', 'Food and Drink', 'Utilities', 'Newsstand', 'Health and Fitness', 'News', 'Lifestyle', 'Medical', 'Weather', 'Games', 'Catalogs', 'Social Networking', 'Photo and Video', 'Reference', 'Books', 'Education', 'avg_review', 'var_review', 'star1', 'star2', 'star3', 'star4', 'star5', 'positive', 'negative', 'm1_coef_0', 'm1_coef_1', 'm1_coef_2', 'm2_coef_0', 'm2_coef_1', 'm2_coef_2', 'm3_coef_0', 'm3_coef_1', 'm3_coef_2', 'm4_coef_0', 'm4_coef_1', 'm4_coef_2']\n",
    "57.1052631579"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictor selection, pick the top 50 features selected in the forward recursive selection with Lasso and then do random forest\n",
    "\n",
    "54.6842105263\n",
    "['id', 'name', 'category', 'device', 'week_1', 'week_2', 'week_3', 'week_4', 'week_5', 'week_6', 'week_7', 'week_8', 'daily_avg', 'coef_0', 'coef_1', 'coef_2', 'coef_3', 'maxStep', 'minStep', 'std', 'nb_missing', 'iphone', 'ipad', 'Productivity', 'Entertainment', 'Sports', 'Music', 'Shopping', 'Finance', 'Business', 'Navigation', 'Food and Drink', 'Utilities', 'News', 'Lifestyle', 'Medical', 'Weather', 'Games', 'Catalogs', 'Social Networking', 'Photo and Video', 'Reference', 'Education', 'avg_review', 'var_review', 'star2', 'star3', 'star4', 'positive', 'negative', 'm2_coef_0', 'm3_coef_0', 'm3_coef_1', 'm4_coef_2']\n",
    "57.3157894737"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Estimation of alpha for the Lasso regression \"cros validation\" like approach\n",
    "for a in np.arange(90,110,5):\n",
    "\n",
    "    old_top = []\n",
    "    new_top = []\n",
    "\n",
    "    test_frac = 0.31  #Fraction of test points\n",
    "    N = 20   #number of iterations\n",
    "\n",
    "\n",
    "    np.random.seed(1)\n",
    "    for i in range(1,N):\n",
    "        r = np.random.randint(1,429496729)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(predictors.as_matrix()[:,4:], output.as_matrix()[:,5], test_size=test_frac, random_state=r)\n",
    "        X_test = X_test[0:10000]\n",
    "        y_test = y_test[0:10000]\n",
    "        mod=linear_model.Lasso(alpha=a,fit_intercept=False).fit(X_train,y_train) \n",
    "        y_pred =  mod.predict(X_test)\n",
    "        new_top.append(metric(y_pred,y_test))\n",
    "\n",
    "\n",
    "    print a\n",
    "    print 1.0*sum(new_top)/len(new_top)\n",
    "    print \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation k-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 10\n",
    "\n",
    "kf = KFold(len(predictors), n_folds=K)\n",
    "old_top = []\n",
    "new_top = []\n",
    "for train, test in kf:\n",
    "    #base model\n",
    "    old_mod=linear_model.LinearRegression(fit_intercept=False).fit(predictors.as_matrix()[train,4:12], output.as_matrix()[train,5])\n",
    "    old_y_pred =  old_mod.predict(predictors.as_matrix()[test,4:12])\n",
    "    old_top.append(metric(old_y_pred,output.as_matrix()[test,5]))\n",
    "    #model\n",
    "    #mod=linear_model.LinearRegression(fit_intercept=False).fit(predictors.as_matrix()[train,4:], output.as_matrix()[train,5]) \n",
    "    #mod=linear_model.Lasso(alpha=100,fit_intercept=False).fit(predictors.as_matrix()[train,4:], output.as_matrix()[train,5]) \n",
    "    #mod = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0, loss='ls').fit(predictors.as_matrix()[train,4:], output.as_matrix()[train,5]) \n",
    "    #mod= RandomForestRegressor(max_features = 1.0/3.0,n_estimators = 100).fit(predictors.as_matrix()[train,4:], output.as_matrix()[train,5])\n",
    "    mod= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100).fit(predictors.as_matrix()[train,4:], output.as_matrix()[train,5])\n",
    "    y_pred =  mod.predict(predictors.as_matrix()[test,4:])\n",
    "    new_top.append(metric(y_pred,output.as_matrix()[test,5]))\n",
    "print 1.0*sum(old_top)/len(old_top)\n",
    "print list(predictors.columns.values)\n",
    "print 1.0*sum(new_top)/len(new_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K=10 \n",
    "Random Forest sqrt  :mod= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100).fit(predictors.as_matrix()[train,4:], output.as_matrix()[train,5])\n",
    "PPredictors obtainned with top 50 on Lasso\n",
    "['id', 'name', 'category', 'device', 'week_1', 'week_2', 'week_3', 'week_4', 'week_5', 'week_6', 'week_7', 'week_8', 'coef_0', 'coef_1', 'coef_2', 'coef_3', 'minStep', 'std', 'nb_missing', 'iphone', 'ipad', 'Productivity', 'Entertainment', 'Travel', 'Sports', 'Music', 'Shopping', 'Finance', 'Business', 'Navigation', 'Utilities', 'News', 'Lifestyle', 'Medical', 'Weather', 'Games', 'Catalogs', 'Social Networking', 'Photo and Video', 'Reference', 'Books', 'Education', 'avg_review', 'var_review', 'star2', 'star3', 'star4', 'positive', 'negative', 'm2_coef_0', 'm3_coef_0', 'm3_coef_1', 'rev_coef_0', 'rev_coef_1']\n",
    "58.1337784952"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso all\n",
    "53.4375\n",
    "['id', 'name', 'category', 'device', 'week_1', 'week_2', 'week_3', 'week_4', 'week_5', 'week_6', 'week_7', 'week_8', 'daily_avg', 'coef_0', 'coef_1', 'coef_2', 'coef_3', 'maxStep', 'minStep', 'std', 'nb_missing', 'iphone', 'ipad', 'Productivity', 'Entertainment', 'Travel', 'Sports', 'Music', 'Shopping', 'Finance', 'Business', 'Navigation', 'Food and Drink', 'Utilities', 'Newsstand', 'Health and Fitness', 'News', 'Lifestyle', 'Medical', 'Weather', 'Games', 'Catalogs', 'Social Networking', 'Photo and Video', 'Reference', 'Books', 'Education', 'avg_review', 'var_review', 'star1', 'star2', 'star3', 'star4', 'star5', 'positive', 'negative', 'm1_coef_0', 'm1_coef_1', 'm1_coef_2', 'm2_coef_0', 'm2_coef_1', 'm2_coef_2', 'm3_coef_0', 'm3_coef_1', 'm3_coef_2', 'm4_coef_0', 'm4_coef_1', 'm4_coef_2']\n",
    "54.0625"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt at boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Boosting\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "new_top = []\n",
    "\n",
    "test_frac = 0.31  #Fraction of test points\n",
    "N = 20   #number of iterations\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "for i in range(1,N):\n",
    "    r = np.random.randint(1,429496729)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors.as_matrix()[:,4:], output.as_matrix()[:,5], test_size=test_frac, random_state=r)\n",
    "    X_test = X_test[0:10000]\n",
    "    y_test = y_test[0:10000]\n",
    "    #mod=linear_model.LinearRegression(fit_intercept=False).fit(X_train,y_train)\n",
    "    #mod=linear_model.Lasso(alpha=a,fit_intercept=False).fit(X_train,y_train) \n",
    "    mod = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0, loss='ls').fit(X_train,y_train)\n",
    "    mod= RandomForestRegressor(max_features = 1.0/3.0,n_estimators = 100) \n",
    "    y_pred =  mod.predict(X_test)\n",
    "    new_top.append(metric(y_pred,y_test))\n",
    "\n",
    "print 1.0*sum(new_top)/len(new_top)\n",
    "print \" \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_scaled.sort_values(by= 'cumulative_downloads_2016-02',ascending = False).head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Classification on top 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = pd.read_csv('predictors.csv').drop('Unnamed: 0', 1)\n",
    "predictors = predictors.fillna(0)\n",
    "\n",
    "thr = predictors.sort_values(by= 'week_8',ascending = False).iloc[int(0.1*len(predictors))][\"week_8\"]\n",
    "predictors  = predictors[predictors.week_8 >= thr]\n",
    "\n",
    "\n",
    "len(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_classification_10 = output\n",
    "output_classification_50 = output\n",
    "output_classification_100 = output\n",
    "#Is top ?\n",
    "def generator_istop(inp,threshold):\n",
    "    if (inp[5] >= threshold):\n",
    "        return 'True'\n",
    "    else:\n",
    "        return 'False'\n",
    "\n",
    "threshold = output.sort_values('cumulative_downloads_2016-02',ascending = False).iloc[len(output)/10]['cumulative_downloads_2016-02']\n",
    "output_classification_10[\"is_top\"] = output.apply(generator_istop,axis = 1,args=(threshold,))\n",
    "\n",
    "threshold = output.sort_values('cumulative_downloads_2016-02',ascending = False).iloc[len(output)/50]['cumulative_downloads_2016-02']\n",
    "output_classification_50[\"is_top\"] = output.apply(generator_istop,axis = 1,args=(threshold,))\n",
    "\n",
    "threshold = output.sort_values('cumulative_downloads_2016-02',ascending = False).iloc[len(output)/100]['cumulative_downloads_2016-02']\n",
    "output_classification_100[\"is_top\"] = output.apply(generator_istop,axis = 1,args=(threshold,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_classification_10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "K=10\n",
    "kf = KFold(32339, n_folds=K)\n",
    "old_top = []\n",
    "new_top = []\n",
    "for train, test in kf:\n",
    "    #model\n",
    "    mod = SVC(C=10.0, kernel='poly', degree=3, gamma='auto', coef0=2.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=10, decision_function_shape=None, random_state=None).fit(predictors.as_matrix()[train,4:],output_classification_10.as_matrix()[train,6])\n",
    "    #mod = linear_model.LogisticRegression().fit(predictors.as_matrix()[train,4:], output_classification_10.as_matrix()[train,6])\n",
    "    y_pred =  mod.predict(predictors.as_matrix()[test,4:])\n",
    "    new_top.append(metric_classification(y_pred,output_classification_10.as_matrix()[test,6]))\n",
    "print list(predictors.columns.values)\n",
    "print 1.0*sum(new_top)/len(new_top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###This metric is not very good, put True everywhere and get 100%\n",
    "def metric_classification(y_pred,y_test):\n",
    "    nb_top = 0\n",
    "    for i in range(len(y_test)):\n",
    "        if (y_pred[i]=='True' and y_test[i]==\"False\"):\n",
    "            nb_top+=1\n",
    "    return 100.0*nb_top/len(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(y_pred).count(\"False\")*1.0/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print list(y_pred).count('True')\n",
    "print list(output_classification_10.as_matrix()[test,6]).count('True')\n",
    "print len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Question, what if instead of using the 'cumulative_downloads_2016-02' we used exp('cumulative_downloads_2016-02') to try to shrunk the lowest points?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_scaled = output\n",
    "#Is top ?\n",
    "def generator_outputScaler(inp):\n",
    "    #return math.exp(inp[5]*1.0/12329752*100)   #28%\n",
    "    #return 1.0/(20000000-inp[5])   #0.618429189858\n",
    "    #return inp[5]**2       #53.8045828797\n",
    "    #return (inp[5]*1.0/20000000)**2*inp[5]     #46.3834326015\n",
    "    #return (inp[5]*1.0/1500000)**0.5   #  56.5875142341\n",
    "    #return 100.0/(1+math.exp(-(inp[5]-1500000)*1.0/100000))    #54.4230120696\n",
    "    #return 200.0/(1+math.exp(-(inp[5]-1500000)*1.0/1000000)) - 100    #55.9691806875\n",
    "    #return math.log(inp[5])      #58.7522076851\n",
    "    return inp[5]               #58.4429930902\n",
    "    #return 200.0/(1+math.exp(-(inp[5]*1.0/1500000-1)*10)) - 100       #55.3506558544\n",
    "    #return math.exp(1.0/(1+math.exp(-(inp[5]*1.0/1500000-1)*10)))     #\n",
    "\n",
    "\n",
    "output_scaled[\"scaled_downloads\"] = output.apply(generator_outputScaler,axis = 1)\n",
    "\n",
    "#output_scaled[\"scaled_downloads\"] =(output['cumulative_downloads_2016-02']-output['cumulative_downloads_2016-02'].mean())/output['cumulative_downloads_2016-02'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_scaled.sort_values(by= 'cumulative_downloads_2016-02',ascending = False).head(320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(list(output_scaled.sort_values(by= 'cumulative_downloads_2016-02',ascending = False)[\"scaled_downloads\"]))\n",
    "plt.axis([0, 300,min(list(output_scaled.sort_values(by= 'cumulative_downloads_2016-02',ascending = False)[\"scaled_downloads\"])),max(list(output_scaled.sort_values(by= 'cumulative_downloads_2016-02',ascending = False)[\"scaled_downloads\"]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 10\n",
    "\n",
    "kf = KFold(32339, n_folds=K)\n",
    "old_top = []\n",
    "new_top = []\n",
    "for train, test in kf:\n",
    "    #base model\n",
    "    old_mod=linear_model.LinearRegression(fit_intercept=False).fit(predictors.as_matrix()[train,4:12], output.as_matrix()[train,5])\n",
    "    old_y_pred =  old_mod.predict(predictors.as_matrix()[test,4:12])\n",
    "    old_top.append(metric(old_y_pred,output.as_matrix()[test,5]))\n",
    "    #model\n",
    "    #mod=linear_model.LinearRegression(fit_intercept=False).fit(predictors.as_matrix()[train,4:], output_scaled.as_matrix()[train,6])\n",
    "    #mod=linear_model.Lasso(alpha=100,fit_intercept=False).fit(predictors.as_matrix()[train,4:], output_scaled.as_matrix()[train,6])\n",
    "    #mod = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0, loss='ls').fit(predictors.as_matrix()[train,4:], output_scaled.as_matrix()[train,6]) \n",
    "    #mod= RandomForestRegressor(max_features = 1.0/3.0,n_estimators = 100).fit(predictors.as_matrix()[train,4:], output_scaled.as_matrix()[train,6])\n",
    "    mod= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100).fit(predictors.as_matrix()[train,4:], output_scaled.as_matrix()[train,6])\n",
    "    y_pred =  mod.predict(predictors.as_matrix()[test,4:])\n",
    "    new_top.append(metric(y_pred,output_scaled.as_matrix()[test,6]))\n",
    "print 1.0*sum(old_top)/len(old_top)\n",
    "print list(predictors.columns.values)\n",
    "print 1.0*sum(new_top)/len(new_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(list(output.sort_values(by= 'cumulative_downloads_2016-02',ascending = False)[\"cumulative_downloads_2016-02\"]))\n",
    "plt.axis([0, 3200, 0, 13000000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(list(output_scaled.sort_values(by= 'cumulative_downloads_2016-02',ascending = False)[\"scaled_downloads\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors.sort_values(by= 'week_8',ascending = False)[0:3233].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output.sort_values(by= 'cumulative_downloads_2016-02',ascending = False)[0:323].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(predictors.sort_values(by= 'week_8',ascending = False)[0:3233].merge(output.sort_values(by= 'cumulative_downloads_2016-02',ascending = False)[0:323], how='inner', on=[\"id\",\"name\",\"category\",\"device\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "313.0/323"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
