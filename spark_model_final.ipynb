{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imported libraries\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import boto\n",
    "import numpy as np\n",
    "from numpy import matlib\n",
    "import scipy as sp\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import langdetect\n",
    "import datetime\n",
    "import warnings\n",
    "%matplotlib inline  \n",
    "from sklearn.svm import SVR\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "import os\n",
    "import findspark; findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import Row\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors \n",
    "from pyspark.mllib.regression import LabeledPoint  \n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD \n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "  \"--packages com.databricks:spark-csv_2.11:1.4.0 pyspark-shell\"\n",
    ")\n",
    "\n",
    "#NEED TO ADD \"  SPARK_DRIVER_MEMORY=5G   \"  to ./conf/spark-env.sh \n",
    "\n",
    "try:\n",
    "    conf = SparkConf().set(\"spark.executor.memory\", \"3g\")\n",
    "    sc = pyspark.SparkContext(conf=conf)\n",
    "except Exception as e:\n",
    "    print \"SparkContext exists... Continuing on.\"\n",
    "    \n",
    "sqlCtx = pyspark.sql.SQLContext(sc)\n",
    "sc.setCheckpointDir('checkpoint/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    output = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true',inferSchema='true') \\\n",
    "    .load('s3n://cs341bucket1/Data/train_final_downloads.csv').drop('')\n",
    "    gdp_countries_pd = pd.read_csv('s3://cs341bucket1/Data/GDP.csv')\n",
    "except:\n",
    "    output = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true',inferSchema='true') \\\n",
    "    .load('train_final_downloads.csv').drop('')\n",
    "    gdp_countries_pd = pd.read_csv('GDP.csv')\n",
    "output = output.withColumnRenamed(\"cumulative_downloads_2016-02\",\"cumulative_downloads_2016_02\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testing(predictors):\n",
    "    # use this to print information about predictor at each step\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_log_week(*args):\n",
    "    args = [x if not (x<0) else 0 for x in list(args)]\n",
    "    nb_0 = args.count(0)\n",
    "    if nb_0 == 7:\n",
    "        return float(0)\n",
    "    return math.log(1.0*sum(args)/(7-nb_0))\n",
    "    #Y = [np.log(c) for c in args if c > 0]\n",
    "    #if len(Y) == 0 : return 0\n",
    "    #return float(1.0*sum(Y))/(7-nb_0)\n",
    "sqlCtx.registerFunction(\"get_log_week\", get_log_week,returnType=FloatType())\n",
    "def get_download_sum(*args):\n",
    "    args = [x if not (x==-1) else 0 for x in list(args)]\n",
    "    return (1.0*sum(args))\n",
    "sqlCtx.registerFunction(\"get_download_sum\", get_download_sum,returnType=FloatType())\n",
    "\n",
    "\n",
    "def get_coefficients(*args):\n",
    "    #The first element of the list is the degree of the coefficient\n",
    "    args = [x if not (x==-1) else 0 for x in list(args)]    \n",
    "    return  float(np.polyfit(range(56),np.cumsum(args[1:]),3)[3-args[0]])\n",
    "    #Y = [np.log(c) for c in args[1:] if c>0]\n",
    "    #if len(Y)<=1: return float(0)\n",
    "    #return  float(np.polyfit(range(len(Y)),np.cumsum(Y),args[0])[0])\n",
    "\n",
    "#Generate the step max and min \n",
    "def get_maxStep(maximum,*args):\n",
    "    args = [x if not (x==-1) else 0 for x in list(args)]\n",
    "    if (np.count_nonzero(args) == 0):\n",
    "        return float(0)\n",
    "    if maximum :\n",
    "        return max(args)\n",
    "    else:\n",
    "        return min(filter(lambda x: x>0,args))\n",
    "\n",
    "\n",
    "def get_maxStep_old(maximum,*args):\n",
    "    args = [x if not (x==-1) else 0 for x in list(args)]\n",
    "    m = 0\n",
    "    for d in range(1,56):\n",
    "        if (args[d]!=0 and args[d-1]!=0):\n",
    "            c = (args[d]-args[d-1])\n",
    "            #c = float(args[d])/args[d-1]\n",
    "            if (maximum and m < c):\n",
    "                m = c\n",
    "            if ( not maximum and m > c):\n",
    "                m = c\n",
    "    return m\n",
    "    #if m==0: return float(0)\n",
    "    #return m\n",
    "\n",
    "def get_std(*args):\n",
    "    args = [x if not (x==-1) else 0 for x in list(args)]\n",
    "    return float(np.std(list(args)))\n",
    "\n",
    "def get_nbMissing(*args):\n",
    "    return list(args).count(-1)\n",
    "\n",
    "#Generate the daily average\n",
    "\n",
    "def get_revenue_coefficients(*args):\n",
    "    #The first element of the list is the degree of the coefficient\n",
    "    args = list(args)\n",
    "    time_series = [np.log(c) for c in args[1:] if c>0]\n",
    "    if len(time_series) <=1: return float(0)\n",
    "    #return float(np.polyfit(np.array(range(56)),args[1:],args[0])[0])\n",
    "    return  float(np.polyfit(np.array(range(len(time_series))),time_series,args[0])[0])\n",
    "\n",
    "def get_usage_coefficients(*args):\n",
    "    #The first element of the list is the degree of the coefficient\n",
    "    args = list(args)\n",
    "    if -1 in args: return 0\n",
    "    return  float(np.polyfit(range(8),args[1:],args[0])[0])\n",
    "\n",
    "def get_usage_max(*args):\n",
    "    #The first element of the list is the degree of the coefficient\n",
    "    args = list(args)\n",
    "    args = [c for c in args[1:] if c!=-1]\n",
    "    if len(args)==0: return 0\n",
    "    return  float(np.max(args))\n",
    "\n",
    "def get_usage_mean(*args):\n",
    "    #The first element of the list is the degree of the coefficient\n",
    "    args = list(args)\n",
    "    args = [c for c in args[1:] if c!=-1]\n",
    "    if len(args)==0: return 0\n",
    "    return  float(np.mean(args))\n",
    "\n",
    "def get_revenue_max(*args):\n",
    "    #The first element of the list is the degree of the coefficient\n",
    "    args = list(args)\n",
    "    time_series = [np.log(c) for c in args[1:] if c>0]\n",
    "    if len(time_series) <=1: return float(0)\n",
    "    #args = [c for c in args[1:] if c!=-1]\n",
    "    #if len(args)==0: return 0\n",
    "    #return  float(np.max(args))\n",
    "    return float(np.max(time_series))\n",
    "\n",
    "def get_revenue_mean(*args):\n",
    "    #The first element of the list is the degree of the coefficient\n",
    "    args = list(args)\n",
    "    #args = [c for c in args[1:] if c!=-1]\n",
    "    #if len(args)==0: return 0\n",
    "    #return  float(np.mean(args))\n",
    "    time_series = [np.log(c) for c in args[1:] if c>0]\n",
    "    if len(time_series) <=1: return float(0)\n",
    "    return float(np.mean(time_series))\n",
    "\n",
    "def get_dailyAvg(*args):\n",
    "    args = [x if not (x==-1) else 0 for x in list(args)]\n",
    "    if (np.count_nonzero(args) == 0):\n",
    "        return float(0)\n",
    "    return  (1.0*sum(args)/np.count_nonzero(args))\n",
    "    #return float(sum([np.log(c) for c in args if c>0]))/np.count_nonzero(args)\n",
    "\n",
    "sqlCtx.registerFunction(\"get_nbMissing\", get_nbMissing,returnType=IntegerType())\n",
    "sqlCtx.registerFunction(\"get_std\", get_std,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_maxStep\", get_maxStep,returnType=IntegerType())\n",
    "sqlCtx.registerFunction(\"get_maxStep_old\", get_maxStep_old,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_coefficients\", get_coefficients,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"daily_avg\", get_dailyAvg,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_usage_coefficients\", get_usage_coefficients,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_revenue_coefficients\", get_revenue_coefficients,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_usage_max\", get_usage_max,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_usage_mean\", get_usage_mean,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_revenue_max\", get_revenue_max,returnType=FloatType())\n",
    "sqlCtx.registerFunction(\"get_revenue_mean\", get_revenue_mean,returnType=FloatType())\n",
    "\n",
    "lang = ['ja','zh-cn','ko','en','other']\n",
    "def get_language(x):\n",
    "    try:\n",
    "        detected = langdetect.detect_langs(unicode(x))[0]\n",
    "        if detected.prob < 0.7:\n",
    "            return \"other\"\n",
    "        elif  detected.lang in lang:\n",
    "            return detected.lang\n",
    "        else:\n",
    "            return \"other\"\n",
    "    except:\n",
    "        return \"other\"\n",
    "sqlCtx.registerFunction(\"get_language\", get_language,returnType=StringType())\n",
    "\n",
    "def get_days(date, id):\n",
    "    if (date == \"no_date\") or True:   #Attention here, only imputation\n",
    "        # return 0\n",
    "        id = 1.0*id/100000000\n",
    "        return int(5856.25394104 -1731.74798728*id+195.553086*id**2  -8.12861635*id**3)\n",
    "    else:\n",
    "        try:\n",
    "            return (datetime.datetime.strptime('03/01/2015', '%m/%d/%Y').date() \\\n",
    "                - datetime.datetime.strptime(date, '%m/%d/%Y').date()).days\n",
    "        except:\n",
    "            return (datetime.datetime.strptime('03/01/2015', '%m/%d/%Y').date() \\\n",
    "                - datetime.datetime.strptime(date, '%Y-%m-%d').date()).days\n",
    "sqlCtx.registerFunction(\"get_days\", get_days,returnType=IntegerType())\n",
    "\n",
    "#escape is used in case some asshole used - or [space] anywhere\n",
    "def escape(text):\n",
    "    return text.replace(\" \",\"_\").replace(\"-\",\"_\")\n",
    "# number of reviews\n",
    "def get_recentReviews(date):\n",
    "    return int((datetime.datetime.strptime('04/01/2015', '%m/%d/%Y').date() \\\n",
    "            - datetime.datetime.strptime(date, '%Y-%m-%d').date()).days >=0)\n",
    "\n",
    "def get_market(country,device):\n",
    "    #http://blog.nelso.com/2010/06/iphone-os-penetration-by-country.html\n",
    "    if device == \"iphone\":\n",
    "        return {\n",
    "            \"United_States\" : 10683403,\n",
    "            \"France\" : 2248817,\n",
    "            \"Japan\" : 1378903,\n",
    "            \"Spain\" : 377346,\n",
    "            \"United_Kingdom\" : 2551128,\n",
    "            \"Germany\" : 1117716,\n",
    "            \"Hong_Kong\" : 299720,\n",
    "            \"Switzerland\" : 399364,\n",
    "            \"Netherlands\" : 372539,\n",
    "            \"Australia\" : 1207428,\n",
    "            \"Norway\" : 154218,\n",
    "            \"Sweden\" : 281622,\n",
    "            \"China\" : 725358,\n",
    "            \"Canada\" : 919074,\n",
    "            \"Denmark\" : 151426,\n",
    "            \"Italy\" : 648718,\n",
    "            \"Taiwan\" : 174226,\n",
    "            \"Mexico\" : 215326,\n",
    "            \"Austria\" : 156322,\n",
    "            \"Brazil\" : 219339,\n",
    "            \"Poland\" : 72114,\n",
    "            \"Singapore\" : 402922,\n",
    "            \"Hungary\" : 33219,\n",
    "            \"Czech_Republic\" : 42753,\n",
    "            'South_Korea': 530235,\n",
    "            \"Russia\" :246421   \n",
    "    }.get(country, 15000) \n",
    "    else:\n",
    "        return{\n",
    "            \"United_States\" : 223269,\n",
    "            \"France\" : 2724,\n",
    "            \"Japan\" : 2293,\n",
    "            \"Spain\" : 1494,\n",
    "            \"United_Kingdom\" : 4197,\n",
    "            \"Germany\" : 3403,\n",
    "            \"Hong_Kong\" : 2306,\n",
    "            \"Switzerland\" : 1698,\n",
    "            \"Netherlands\" : 2554,\n",
    "            \"Australia\" : 1400,\n",
    "            \"Norway\" : 1333,\n",
    "            \"Sweden\" : 1188,\n",
    "            \"China\" : 12516,\n",
    "            \"Canada\" : 6275,\n",
    "            \"Denmark\" : 753,\n",
    "            \"Italy\" : 1370,\n",
    "            \"Taiwan\" : 1356,\n",
    "            \"Mexico\" : 3380,\n",
    "            \"Austria\" : 493,\n",
    "            \"Brazil\" : 2014,\n",
    "            \"Poland\" : 324,\n",
    "            \"Singapore\" : 1453,\n",
    "            \"Hungary\" : 211,\n",
    "            \"Czech_Republic\" : 203,\n",
    "            'South_Korea': 2416,\n",
    "            \"Russia\" :2183\n",
    "        }.get(country, 100) \n",
    "    return 1\n",
    "sqlCtx.registerFunction(\"get_market\", get_market,returnType=IntegerType())\n",
    "\n",
    "def get_gdp(country):\n",
    "    if country == \"no_country\":\n",
    "        return 0.0\n",
    "    try:\n",
    "        return float(gdp_countries_pd[gdp_countries_pd.Country == country][\"GDP2015\"])\n",
    "    \n",
    "    except:\n",
    "        return 0.0\n",
    "sqlCtx.registerFunction(\"get_gdp\", get_gdp,returnType=FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_predictors(down, rati, usag, reve, prev, rele, raco, revi ):\n",
    "\n",
    "\n",
    "\n",
    "    ###load files : \n",
    "    try:\n",
    "        sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", os.environ['AWS_ACCESS_KEY_ID'])\n",
    "        sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", os.environ['AWS_SECRET_ACCESS_KEY'])\n",
    "        downloads = sqlCtx.read \\\n",
    "            .format('com.databricks.spark.csv') \\\n",
    "            .options(header='true',inferSchema='true') \\\n",
    "            .load('s3n://cs341bucket1/Data/'+down).drop('')\n",
    "        ratings = sqlCtx.read \\\n",
    "            .format('com.databricks.spark.csv') \\\n",
    "            .options(header='true',inferSchema='true') \\\n",
    "            .load('s3n://cs341bucket1/Data/'+rati).drop('')\n",
    "        usages = sqlCtx.read \\\n",
    "            .format('com.databricks.spark.csv') \\\n",
    "            .options(header='true',inferSchema='true') \\\n",
    "            .load('s3n://cs341bucket1/Data/'+usag).drop('')\n",
    "        revenues = sqlCtx.read \\\n",
    "            .format('com.databricks.spark.csv') \\\n",
    "            .options(header='true',inferSchema='true') \\\n",
    "            .load('s3n://cs341bucket1/Data/'+reve).drop('')\n",
    "        prev_downloads = sqlCtx.read \\\n",
    "            .format('com.databricks.spark.csv') \\\n",
    "            .options(header='true',inferSchema='true') \\\n",
    "            .load('s3n://cs341bucket1/Data/'+prev).drop('')  \n",
    "        release_date = sqlCtx.read \\\n",
    "            .format('com.databricks.spark.csv') \\\n",
    "            .options(header='true',inferSchema='true') \\\n",
    "            .load('s3n://cs341bucket1/Data/'+rele).drop('')\n",
    "        rating_country = sqlCtx.read \\\n",
    "            .format('com.databricks.spark.csv') \\\n",
    "            .options(header='true',inferSchema='true') \\\n",
    "            .load('s3n://cs341bucket1/Data/'+raco).drop('')\n",
    "\n",
    "        reviews_schema = StructType([\n",
    "            StructField(\"id\",IntegerType(),True),\n",
    "            StructField(\"name\",StringType(),True),\n",
    "            StructField(\"country\",StringType(),True),\n",
    "            StructField(\"rating\",IntegerType(),True),\n",
    "            StructField(\"date\",StringType(),True),\n",
    "            StructField(\"title\",StringType(),True),\n",
    "            StructField(\"version\",StringType(),True),\n",
    "            StructField(\"text\",StringType(),True),\n",
    "            StructField(\"reviewer\",StringType(),True)\n",
    "        ])\n",
    "        reviews = pd.read_csv('s3://cs341bucket1/Data/'+revi)\n",
    "        reviews = sqlCtx.createDataFrame(reviews,reviews_schema)\n",
    "\n",
    "    except:\n",
    "        downloads = sqlCtx.read \\\n",
    "            .format('com.databricks.spark.csv') \\\n",
    "            .options(header='true',inferSchema='true') \\\n",
    "            .load(down).drop('')\n",
    "        ratings = sqlCtx.read \\\n",
    "            .format('com.databricks.spark.csv') \\\n",
    "            .options(header='true',inferSchema='true') \\\n",
    "            .load(rati).drop('')\n",
    "        usages = sqlCtx.read \\\n",
    "            .format('com.databricks.spark.csv') \\\n",
    "            .options(header='true',inferSchema='true') \\\n",
    "            .load(usag).drop('')\n",
    "        revenues = sqlCtx.read \\\n",
    "            .format('com.databricks.spark.csv') \\\n",
    "            .options(header='true',inferSchema='true') \\\n",
    "            .load(reve).drop('')\n",
    "        prev_downloads = sqlCtx.read \\\n",
    "            .format('com.databricks.spark.csv') \\\n",
    "            .options(header='true',inferSchema='true') \\\n",
    "            .load(prev).drop('')  \n",
    "        release_date = sqlCtx.read \\\n",
    "            .format('com.databricks.spark.csv') \\\n",
    "            .options(header='true',inferSchema='true') \\\n",
    "            .load(rele).drop('')\n",
    "\n",
    "        rating_country = sqlCtx.read \\\n",
    "            .format('com.databricks.spark.csv') \\\n",
    "            .options(header='true',inferSchema='true') \\\n",
    "            .load(raco).drop('')\n",
    "\n",
    "        reviews_schema = StructType([\n",
    "            StructField(\"id\",IntegerType(),True),\n",
    "            StructField(\"name\",StringType(),True),\n",
    "            StructField(\"country\",StringType(),True),\n",
    "            StructField(\"rating\",IntegerType(),True),\n",
    "            StructField(\"date\",StringType(),True),\n",
    "            StructField(\"title\",StringType(),True),\n",
    "            StructField(\"version\",StringType(),True),\n",
    "            StructField(\"text\",StringType(),True),\n",
    "            StructField(\"reviewer\",StringType(),True)\n",
    "        ])\n",
    "        reviews = pd.read_csv(revi)\n",
    "        reviews = sqlCtx.createDataFrame(reviews,reviews_schema)\n",
    "\n",
    "    \n",
    "    ###imputations\n",
    "    #usage imputation\n",
    "    imputer = Imputer(missing_values=-1, strategy='median', axis=0)\n",
    "    pd_usages = usages.toPandas()\n",
    "    category = list(set(pd_usages[\"category\"].values))\n",
    "    imp = pd.DataFrame(columns = pd_usages.columns)\n",
    "    for cat in category:\n",
    "        #for dev in [\"iphone\",\"ipad\"]:\n",
    "            for metric in range(1,5):\n",
    "                curr_df = pd_usages.ix[pd_usages[\"category\"]==cat,:]\n",
    "                #curr_df = curr_df.ix[curr_df[\"device\"]==dev,:]\n",
    "                curr_df = curr_df.ix[curr_df[\"metric\"]==metric,:]\n",
    "                try:\n",
    "                    name = curr_df.columns\n",
    "                    df1 = curr_df.ix[:,0:6]\n",
    "                    df2 = pd.DataFrame(imputer.fit_transform(curr_df.ix[:,6:]))\n",
    "                    df2.index = df1.index\n",
    "                    curr_df = pd.concat([df1,df2],axis = 1)\n",
    "                    curr_df.columns = name \n",
    "                except:\n",
    "                    pass\n",
    "                imp = pd.concat([imp,curr_df],axis = 0)\n",
    "    usages = sqlCtx.createDataFrame(imp).fillna(-1)\n",
    "\n",
    "    #revenue imputation\n",
    "    pd_revenues = revenues.toPandas()\n",
    "    imp = pd.DataFrame(columns = revenues.columns)\n",
    "    for cat in category:\n",
    "        for dev in [\"iphone\",\"ipad\"]:\n",
    "            #for metric in range(1,5):\n",
    "                curr_df = pd_revenues.ix[pd_revenues[\"category\"]==cat,:]\n",
    "                curr_df = curr_df.ix[curr_df[\"device\"]==dev,:]\n",
    "                #curr_df = curr_df.ix[curr_df[\"metric\"]==metric,:]\n",
    "                name = curr_df.columns\n",
    "                df1 = curr_df.ix[:,0:5]\n",
    "                df2 = pd.DataFrame(imputer.fit_transform(curr_df.ix[:,5:]))\n",
    "                df2.index = df1.index\n",
    "                curr_df = pd.concat([df1,df2],axis = 1)\n",
    "                curr_df.columns = name \n",
    "                imp = pd.concat([imp,curr_df],axis = 0)\n",
    "    revenues = sqlCtx.createDataFrame(imp).fillna(-1)\n",
    "\n",
    "\n",
    "\n",
    "    #warnings.filterwarnings('error')\n",
    "\n",
    "    ### Renaming\n",
    "    old_dateRange = pd.date_range('03/01/2015', periods=56).format(formatter=lambda x: x.strftime('%Y-%m-%d'))\n",
    "    dateRange = pd.date_range('03/01/2015', periods=56).format(formatter=lambda x: x.strftime('%m_%d_%Y'))\n",
    "    for d in range(56):\n",
    "        revenues = revenues.withColumnRenamed(old_dateRange[d],dateRange[d])\n",
    "        usages = usages.withColumnRenamed(old_dateRange[d],dateRange[d])\n",
    "        downloads = downloads.withColumnRenamed(old_dateRange[d],dateRange[d])  \n",
    "    prev_downloads = prev_downloads.withColumnRenamed(\"cumulative_downloads_2015-02\",\"cumulative_downloads_2015_02\")\n",
    "    try:\n",
    "        ratings = ratings.withColumnRenamed(\"start1\",\"star1\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    #Initialization\n",
    "    predictors = downloads['id','name','category','device']\n",
    "    testing(predictors)\n",
    "\n",
    "    # Generate the weekly downloads\n",
    "\n",
    "    sqlCtx.registerDataFrameAsTable(downloads, \"downloads\")\n",
    "    sqlCtx.registerDataFrameAsTable(usages, \"usages\")\n",
    "    sqlCtx.registerDataFrameAsTable(revenues, \"revenues\")\n",
    "\n",
    "\n",
    "\n",
    "    predictors = sqlCtx.sql(\"SELECT id, name, category, device, \\\n",
    "               get_log_week(\"+ \",\".join(dateRange[0:7])+\") AS week_1 \\\n",
    "                ,get_log_week(\"+\",\".join(dateRange[7:14])+\") AS week_2 \\\n",
    "                ,get_log_week(\"+ \",\".join(dateRange[14:21])+\") AS week_3 \\\n",
    "               ,get_log_week(\"+\",\".join(dateRange[21:28])+\") AS week_4 \\\n",
    "               ,get_log_week(\"+\",\".join(dateRange[28:35])+\") AS week_5 \\\n",
    "               ,get_log_week(\"+\",\".join(dateRange[35:42])+\") AS week_6 \\\n",
    "               ,get_log_week(\"+\",\".join(dateRange[42:49])+\") AS week_7 \\\n",
    "               ,get_log_week(\"+\",\".join(dateRange[49:56])+\") AS week_8\\\n",
    "               ,get_download_sum(\"+ \",\".join(dateRange)+\") AS download_sum \\\n",
    "               from downloads\")\n",
    "    sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "\n",
    "    ###Usages\n",
    "    m1 = sqlCtx.sql(\"SELECT * FROM usages WHERE metric = 1\")\n",
    "    m2 = sqlCtx.sql(\"SELECT * FROM usages WHERE metric = 2\")\n",
    "    m3 = sqlCtx.sql(\"SELECT * FROM usages WHERE metric = 3\")\n",
    "    m4 = sqlCtx.sql(\"SELECT * FROM usages WHERE metric = 4\")\n",
    "    sqlCtx.registerDataFrameAsTable(m1,\"m1\")\n",
    "    sqlCtx.registerDataFrameAsTable(m2,\"m2\")\n",
    "    sqlCtx.registerDataFrameAsTable(m3,\"m3\")\n",
    "    sqlCtx.registerDataFrameAsTable(m4,\"m4\")\n",
    "    #sqlCtx.registerDataFrameAsTable(avg_score,\"avg_score\")\n",
    "\n",
    "    ### Make coefficients\n",
    "\n",
    "    temp_downloads = sqlCtx.sql(\"SELECT id,name,category, device \\\n",
    "    , get_coefficients(0,\"+\",\".join(dateRange)+\") AS coef_0 \\\n",
    "    ,get_coefficients(1,\"+\",\".join(dateRange)+\") AS coef_1 \\\n",
    "    ,get_coefficients(2,\"+\",\".join(dateRange)+\") AS coef_2 \\\n",
    "    ,get_coefficients(3,\"+\",\".join(dateRange)+\") AS coef_3 \\\n",
    "    ,get_maxStep(True,\"+\",\".join(dateRange)+\") AS max_step \\\n",
    "    ,get_maxStep(False,\"+\",\".join(dateRange)+\") AS min_step \\\n",
    "    ,get_maxStep_old(True,\"+\",\".join(dateRange)+\") AS max_step_old \\\n",
    "    ,get_maxStep_old(False,\"+\",\".join(dateRange)+\") AS min_step_old \\\n",
    "    ,get_std(\"+\",\".join(dateRange)+\") AS downloads_std \\\n",
    "    ,get_nbMissing(\"+\",\".join(dateRange)+\") AS nb_missing \\\n",
    "    ,daily_avg(\" + \",\".join(dateRange[0:56]) + \") AS daily_avg \\\n",
    "     FROM downloads\")\n",
    "\n",
    "    predictors = predictors.join(temp_downloads,[\"id\",\"name\",\"category\",\"device\"],how='left_outer')\n",
    "    testing(predictors)\n",
    "\n",
    "    m_col = '03_01_2015,03_08_2015,03_15_2015,03_22_2015,03_29_2015,04_05_2015,04_12_2015,04_19_2015'\n",
    "    \n",
    "    temp_m1 = sqlCtx.sql(\"SELECT id, name, category, device, \\\n",
    "    get_usage_coefficients(0,\"+m_col+\") AS m1_coef_0, \\\n",
    "    get_usage_coefficients(1,\"+m_col+\") AS m1_coef_1, \\\n",
    "    get_usage_coefficients(2,\"+m_col+\") AS m1_coef_2, \\\n",
    "    get_usage_max(0,\"+m_col+\") AS m1_max, \\\n",
    "    get_usage_mean(0,\"+m_col+\") AS m1_mean FROM m1\")\n",
    "\n",
    "    \n",
    "    temp_m2 = sqlCtx.sql(\"SELECT id, name, category, device, \\\n",
    "    get_usage_coefficients(0,\"+m_col+\") AS m2_coef_0, \\\n",
    "    get_usage_coefficients(1,\"+m_col+\") AS m2_coef_1, \\\n",
    "    get_usage_coefficients(2,\"+m_col+\") AS m2_coef_2, \\\n",
    "    get_usage_max(0,\"+m_col+\") AS m2_max, \\\n",
    "    get_usage_mean(0,\"+m_col+\") AS m2_mean FROM m2\")\n",
    "\n",
    "    \n",
    "    temp_m3 = sqlCtx.sql(\"SELECT id, name, category, device, \\\n",
    "    get_usage_coefficients(0,\"+m_col+\") AS m3_coef_0, \\\n",
    "    get_usage_coefficients(1,\"+m_col+\") AS m3_coef_1, \\\n",
    "    get_usage_coefficients(2,\"+m_col+\") AS m3_coef_2, \\\n",
    "    get_usage_max(0,\"+m_col+\") AS m3_max, \\\n",
    "    get_usage_mean(0,\"+m_col+\") AS m3_mean FROM m3\")\n",
    "\n",
    "    \n",
    "    temp_m4 = sqlCtx.sql(\"SELECT id, name, category, device, \\\n",
    "    get_usage_coefficients(0,\"+m_col+\") AS m4_coef_0, \\\n",
    "    get_usage_coefficients(1,\"+m_col+\") AS m4_coef_1, \\\n",
    "    get_usage_coefficients(2,\"+m_col+\") AS m4_coef_2, \\\n",
    "    get_usage_max(0,\"+m_col+\") AS m4_max, \\\n",
    "    get_usage_mean(0,\"+m_col+\") AS m4_mean FROM m4\")\n",
    "\n",
    "\n",
    "    temp_revenues = sqlCtx.sql(\"SELECT id, name, category, device, \\\n",
    "    get_revenue_coefficients(0,\"+\",\".join(revenues.columns[4:])+\") AS rev_coef_0, \\\n",
    "    get_revenue_coefficients(1,\"+\",\".join(revenues.columns[4:])+\") AS rev_coef_1, \\\n",
    "    get_revenue_coefficients(2,\"+\",\".join(revenues.columns[4:])+\") AS rev_coef_2, \\\n",
    "    get_revenue_max(0,\"+\",\".join(revenues.columns[4:])+\") AS rev_max, \\\n",
    "    get_revenue_mean(0,\"+\",\".join(revenues.columns[4:])+\") AS rev_mean FROM revenues\")\n",
    "\n",
    "\n",
    "\n",
    "    predictors = predictors.join(temp_revenues,[\"id\", \"name\", \"category\",\"device\"],how='left_outer')\n",
    "    testing(predictors)\n",
    "    predictors = predictors.join(temp_m1,[\"id\", \"name\", \"category\",\"device\"],how='left_outer')\n",
    "    testing(predictors)\n",
    "    predictors = predictors.join(temp_m2,[\"id\", \"name\", \"category\",\"device\"],how='left_outer')\n",
    "    testing(predictors)\n",
    "    predictors = predictors.join(temp_m3,[\"id\", \"name\", \"category\",\"device\"],how='left_outer')\n",
    "    testing(predictors)\n",
    "    predictors = predictors.join(temp_m4,[\"id\", \"name\", \"category\",\"device\"],how='left_outer')\n",
    "    #predictors = predictors.join(avg_score,[\"id\", \"name\", \"category\",\"device\"],how='left_outer')\n",
    "    #predictors = predictors.join(temp_usages,[\"id\", \"name\", \"category\",\"device\"],how='left_outer')\n",
    "    #predictors = predictors.join(,)\n",
    "\n",
    "    testing(predictors)\n",
    "    \n",
    "    # previous downloads addition\n",
    "    predictors = predictors.join(prev_downloads,[\"id\",\"device\"],how='left_outer')\n",
    "\n",
    "    # Days since release generation with imputation\n",
    "\n",
    "    testing(predictors)\n",
    "\n",
    "    release_date = downloads[[\"id\"]].dropDuplicates().join(release_date,[\"id\"],\"left\").fillna(\"no_date\",[\"release_date\"])\n",
    "    sqlCtx.registerDataFrameAsTable(release_date, \"release_date\")\n",
    "\n",
    "    temp_date = sqlCtx.sql(\"SELECT id\\\n",
    "    , get_days(release_date, id) AS days_since_release \\\n",
    "     FROM release_date\")\n",
    "\n",
    "    predictors = predictors.join(temp_date,[\"id\"],\"left\")\n",
    "\n",
    "    #ratings generation\n",
    "    sqlCtx.registerDataFrameAsTable(ratings, \"ratings\")\n",
    "    temp_ratings = sqlCtx.sql(\"SELECT id \\\n",
    "    , CAST(1.0*star1/(star1+star2+star3+star4+star5) AS float) AS star1 \\\n",
    "    , CAST(1.0*star2/(star1+star2+star3+star4+star5) AS float) AS star2 \\\n",
    "    , CAST(1.0*star3/(star1+star2+star3+star4+star5) AS float) AS star3 \\\n",
    "    , CAST(1.0*star4/(star1+star2+star3+star4+star5) AS float) AS star4 \\\n",
    "    , CAST(1.0*star5/(star1+star2+star3+star4+star5) AS float) AS star5 \\\n",
    "    , (star1+star2+star3+star4+star5) AS num_ratings \\\n",
    "     FROM ratings\")\n",
    "\n",
    "    predictors = predictors.join(temp_ratings,[\"id\"],\"left\")\n",
    "\n",
    "    testing(predictors)\n",
    "    \n",
    "    # Categories\n",
    "    list_categories = [ x.category.replace(\" \",\"_\") for x in sqlCtx.sql(\"SELECT category \\\n",
    "     FROM downloads\\\n",
    "     group by category \\\n",
    "     \").collect()]\n",
    "    for cat in list_categories:\n",
    "        sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "        predictors=sqlCtx.sql('''SELECT *, CASE WHEN (category = \"'''+cat+'''\") THEN 1 ELSE 0 END AS '''+cat+''' FROM predictors''')\n",
    "\n",
    "    # Device\n",
    "    sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "    predictors=sqlCtx.sql('''SELECT *, CASE WHEN (device = \"iphone\") THEN 1 ELSE 0 END AS iphone FROM predictors''')\n",
    "    sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "    #predictors=sqlCtx.sql('''SELECT *, CASE WHEN (device = \"ipad\") THEN 1 ELSE 0 END AS ipad FROM predictors''')\n",
    "\n",
    "\n",
    "\n",
    "    #Language of the title\n",
    "    for l in lang:\n",
    "        sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "        predictors=sqlCtx.sql('''SELECT *, CASE WHEN (get_language(name) = \"'''+l+'''\") THEN 1 \\\n",
    "        ELSE 0 END AS '''+l.replace(\"-\",\"_\")+''' FROM predictors''')\n",
    "\n",
    "\n",
    "    #Reviews \n",
    "    #First step\n",
    "    list_countries =['United_States', 'France', 'Japan', 'Spain', 'United_Kingdom','Saudi_Arabia', 'Germany'\\\n",
    "         , 'Hong_Kong', 'Switzerland', 'Turkey','Netherlands', 'Australia', 'Norway', 'Sweden', 'China', 'Canada'\\\n",
    "         ,'Tanzania', 'Denmark', 'South_Korea', 'Italy', 'Finland', 'Taiwan','Russia', 'Philippines', 'Slovenia'\\\n",
    "         , 'Ireland', 'Belgium', 'Mexico','Austria', 'India', 'Brazil', 'Benin', 'New_Zealand','United_Arab_Emirates'\\\n",
    "         , 'Ukraine', 'Poland', 'Israel', 'Portugal','Tunisia', 'Mali', 'Slovakia', 'Zimbabwe', 'Thailand', 'Panama'\\\n",
    "         ,'Indonesia', 'Singapore', 'Greece', 'Senegal', 'Nicaragua','Hungary', 'Czech_Republic', 'Macedonia', 'Chile'\\\n",
    "         , 'Uruguay','Malaysia', 'Algeria', 'Nepal', 'Mauritania', 'Croatia']\n",
    "\n",
    "    cmd = '''review_rdd = reviews\\\n",
    "    .map(lambda x : (x.id , Row(id = x.id , avg_review = x.rating \\\n",
    "    , recent_review = get_recentReviews(x.date), nb_review = 1\\\n",
    "    ,version = set([x.version])'''\n",
    "    cmd+=\",country = set([x.country])\"\n",
    "    #for c in list_countries:\n",
    "    #    cmd+=\",\"+c+''' = int( escape(x.country) == \"'''+c+'''\")'''\n",
    "    for i in range(1,6):\n",
    "        cmd+=\",review_rating_\"+str(i)+\" = int(x.rating == \"+str(i)+\")  \"\n",
    "    cmd+=\")))\"\n",
    "    exec cmd in globals(), locals()\n",
    "\n",
    "    #Group step\n",
    "    cmd = '''review_rdd = review_rdd.reduceByKey(lambda x1 ,x2 : Row(\\\n",
    "     avg_review = x1.avg_review + x2.avg_review\\\n",
    "       ,recent_review = x1.recent_review + x2.recent_review, nb_review = x1.nb_review + x2.nb_review'''\n",
    "    #for c in list_countries:\n",
    "    #    cmd+=\" , \"+c+\" = x1.\"+c+\" + x2.\"+c\n",
    "    cmd+=\", country = x1.country.union(x2.country)\"\n",
    "    cmd+=\", version = x1.version.union(x2.version)\"\n",
    "    for i in range(1,6):\n",
    "        cmd+=\", review_rating_\"+str(i)+\" = x1.review_rating_\"+str(i)+\" + x2.review_rating_\"+str(i)+\" \"\n",
    "    cmd+=\"))\"\n",
    "    exec cmd in globals(), locals()\n",
    "\n",
    "    # Clean the grouped rdd\n",
    "    cmd = '''review_rdd = review_rdd.map(lambda (id , x) : [ id \\\n",
    "    ,  1.0*x.avg_review /  x.nb_review\\\n",
    "       , x.recent_review,  x.nb_review'''\n",
    "    #for c in list_countries:\n",
    "    #    cmd+=\" , 1.0* x.\"+c+\"/ x.nb_review\"\n",
    "    cmd+=\",  escape(x.country.pop())\"\n",
    "    cmd+=\",  len(x.version) -1 \" # -1 if want number of updates\n",
    "    for i in range(1,6):\n",
    "        cmd+=\", 1.0*x.review_rating_\"+str(i)+\" / x.nb_review\"\n",
    "    cmd+=\"])\"\n",
    "    exec cmd in globals(), locals()\n",
    "\n",
    "    #Put back into dataframe\n",
    "    grp_reviews = sqlCtx.createDataFrame(review_rdd, [\"id\",\"avg_review\"\\\n",
    "          ,\"recent_reviews\",\"nb_reviews\",\"country\",\"versions\"]+[\"review_rating_\"+str(i) for i in range(1,6)])\n",
    "        #,\"recent_reviews\",\"nb_reviews\"] + list_countries + [\"versions\"]+[\"review_rating_\"+str(i) for i in range(1,6)])\n",
    "\n",
    "    #Join with predictors \n",
    "    predictors = predictors.join(grp_reviews,[\"id\"],\"left\").fillna(\"no_country\",[\"country\"])\n",
    "\n",
    "    testing(predictors)\n",
    "\n",
    "    # Generate DL Projection\n",
    "    sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "    derived_feats = sqlCtx.sql(\"SELECT id, device\\\n",
    "        ,(7*download_sum+cumulative_downloads_2015_02) AS dl_projection \\\n",
    "        ,CAST((1000000.0*num_ratings/(cumulative_downloads_2015_02 + download_sum))AS float )  AS ratings_per_downloads \\\n",
    "        ,CAST((1.0*num_ratings/(days_since_release+56))AS float )  AS ratings_per_day \\\n",
    "        ,CAST((1000000.0*nb_reviews/download_sum)AS float )  AS review_per_downloads \\\n",
    "        ,CAST((1.0*recent_reviews/nb_reviews)AS float )  AS review_recent_over_old \\\n",
    "        ,CAST((1.0*nb_reviews / days_since_release)AS float )  AS review_per_day \\\n",
    "        ,CAST((1.0*cumulative_downloads_2015_02/(days_since_release))AS float )  AS downloads_per_day_before \\\n",
    "        ,CAST(1.0*download_sum/cumulative_downloads_2015_02 AS float ) AS relative_download_increase\\\n",
    "        ,CAST(1.0*nb_reviews / (CASE WHEN (days_since_release<56) THEN days_since_release ELSE 56 END) AS float) AS reviews_per_days\\\n",
    "        ,CAST(7*download_sum+cumulative_downloads_2015_02 / days_since_release / days_since_release AS float) AS down_over_days2\\\n",
    "        FROM predictors\")\n",
    "    predictors = predictors.join(derived_feats,[\"id\",\"device\"],\"left\")\n",
    "\n",
    "\n",
    "    #we could group by continent or use the market potential\n",
    "    list_countries+=[\"no_country\"]\n",
    "    for co in list_countries:\n",
    "        sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "        predictors=sqlCtx.sql('''SELECT *, CASE WHEN (country = \"'''+co+'''\") THEN 1 ELSE 0 END AS '''+co+''' FROM predictors''')\n",
    "\n",
    "\n",
    "    sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "    predictors=sqlCtx.sql('''SELECT *, get_market(country,device) AS market_size FROM predictors''')\n",
    "\n",
    "    sqlCtx.registerDataFrameAsTable(rating_country, \"rating_country\")\n",
    "    rating_country = sqlCtx.sql('SELECT id\\\n",
    "    , CAST(SUM(star1)*1.0/(SUM(star1)+SUM(star2)+SUM(star3)+SUM(star4)+SUM(star5)) AS float) AS total_star1\\\n",
    "    , CAST(SUM(star2)*1.0/(SUM(star1)+SUM(star2)+SUM(star3)+SUM(star4)+SUM(star5)) AS float)AS total_star2\\\n",
    "    , CAST(SUM(star3)*1.0/(SUM(star1)+SUM(star2)+SUM(star3)+SUM(star4)+SUM(star5))AS float) AS total_star3\\\n",
    "    , CAST(SUM(star4)*1.0/(SUM(star1)+SUM(star2)+SUM(star3)+SUM(star4)+SUM(star5))AS float) AS total_star4\\\n",
    "    , CAST(SUM(star5)*1.0/(SUM(star1)+SUM(star2)+SUM(star3)+SUM(star4)+SUM(star5)) AS float)AS total_star5\\\n",
    "    , (SUM(star1)+SUM(star2)+SUM(star3)+SUM(star4)+SUM(star5)) AS total_star\\\n",
    "    , COUNT(1) AS nb_countries FROM rating_country GROUP BY id')\n",
    "\n",
    "    sqlCtx.registerDataFrameAsTable(rating_country, \"rating_country\")\n",
    "\n",
    "    predictors = predictors.join(rating_country,[\"id\"],how='left_outer')\n",
    "\n",
    "    testing(predictors)\n",
    "\n",
    "    #More on the ratings\n",
    "    sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "    cmd = 'SELECT * '\n",
    "    for i in range(1,6):\n",
    "        cmd+=',(total_star'+str(i)+' * total_star / cumulative_downloads_2015_02) AS country_star'+str(i)+'_per_downloads '\n",
    "        cmd+=',(review_rating_'+str(i)+' * nb_reviews / download_sum) AS review_star'+str(i)+'_per_downloads '\n",
    "        cmd+=',(star'+str(i)+' * num_ratings / cumulative_downloads_2015_02) AS rating_star'+str(i)+'_per_downloads '\n",
    "    cmd+= ' FROM predictors'\n",
    "\n",
    "    predictors = sqlCtx.sql(cmd)\n",
    "\n",
    "\n",
    "    sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "    predictors = sqlCtx.sql('SELECT *, CAST((max_step - min_step)*1.0/download_sum AS float) AS relative_step\\\n",
    "    , (max_step - min_step)  AS download_step \\\n",
    "    FROM predictors')\n",
    "\n",
    "    testing(predictors)\n",
    "    \n",
    "    sqlCtx.registerDataFrameAsTable(predictors, \"predictors\")\n",
    "\n",
    "    predictors = sqlCtx.sql('SELECT *, get_gdp(country)  AS gdp_country \\\n",
    "    FROM predictors')\n",
    "\n",
    "    predictors = predictors.fillna(0)\n",
    "    predictors.cache()\n",
    "\n",
    "    testing(predictors)\n",
    "    return predictors.toPandas().sort_values(by=\"id\").fillna(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors_train = generate_predictors(  'train_app_downloads.csv'\\\n",
    "                                      , 'train_app_rating.csv'\\\n",
    "                                      , 'train_usage.csv'\\\n",
    "                                      , 'train_revenue.csv'\\\n",
    "                                      , 'train_cumulative_downloads_2015-02.csv'\\\n",
    "                                      , 'train_release_date.csv'\\\n",
    "                                      , 'train_rating_by_country.csv'\\\n",
    "                                      , 'train_app_review.csv')\n",
    "predictors_train.to_csv('/mnt/final_train_predictors.csv', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors_test = generate_predictors(   'test_set/test_app_downloads.csv'\\\n",
    "                                      , 'test_set/test_app_rating.csv'\\\n",
    "                                      , 'test_set/test_usage.csv'\\\n",
    "                                      , 'test_set/test_revenue.csv'\\\n",
    "                                      , 'test_set/test_cumulative_downloads_2015-02.csv'\\\n",
    "                                      , 'test_set/test_release_date.csv'\\\n",
    "                                      , 'test_set/test_rating_by_country.csv'\\\n",
    "                                      , 'test_set/test_app_review.csv')\n",
    "predictors_test.to_csv('/mnt/final_test_predictors.csv', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot load files on s3\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    predictors_train = pd.read_csv('/mnt/final_train_predictors.csv').drop('Unnamed: 0',1)\n",
    "    predictors_test = pd.read_csv('/mnt/final_test_predictors.csv').drop('Unnamed: 0',1)\n",
    "except:\n",
    "    print \"Cannot load files on s3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictors_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e4323356cb07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredictors_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictors_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"device\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpredictors_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictors_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"device\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0moutput_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictors_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"device\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"left\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"device\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"cumulative_downloads_2016_02\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnumerical_predictors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictors_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictors_train' is not defined"
     ]
    }
   ],
   "source": [
    "predictors_train = predictors_train.sort_values([\"id\",\"device\"])\n",
    "predictors_test = predictors_test.sort_values([\"id\",\"device\"])\n",
    "output_train = pd.merge(predictors_train,output.toPandas(),on = [\"id\",\"device\"],how=\"left\")\\\n",
    "[[\"id\",\"device\", \"cumulative_downloads_2016_02\"]]\n",
    "\n",
    "numerical_predictors = list(predictors_train.columns.values)\n",
    "#numerical_predictors.remove(\"id\");\n",
    "numerical_predictors.remove(\"device\");\n",
    "numerical_predictors.remove(\"category\");\n",
    "numerical_predictors.remove(\"name\");\n",
    "numerical_predictors.remove(\"country\");\n",
    "\n",
    "\n",
    "predictors_train_id = predictors_train[[\"id\",\"device\"]].as_matrix()\n",
    "predictors_test_id = predictors_test[[\"id\",\"device\"]].as_matrix()\n",
    "\n",
    "\n",
    "\n",
    "#This is the metric we use to determine our performance\n",
    "def metric(y_pred,y_test,percent=1.0):\n",
    "    top = int(len(y_pred)/100.0*percent)\n",
    "    return (len(set([i[0] for i in sorted(enumerate(y_pred), key=lambda x:x[1],reverse=True)][0:top])\n",
    "       .intersection([i[0] for i in sorted(enumerate(y_test), key=lambda x:x[1],reverse=True)][0:top])\n",
    "               ))*(100.0)/top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def final_model(predictors_train,output_train,predictors_test):\n",
    "    print '.',\n",
    "    with_rel = False\n",
    "    top_percent_classif = 12\n",
    "    split_id = 771335033\n",
    "    \n",
    "    train_features_1 = predictors_train[numerical_predictors_1].as_matrix()\n",
    "    train_features_2 = predictors_train[predictors_train.id >= split_id][numerical_predictors_2].as_matrix()\n",
    "    train_output_1 = output_train[\"cumulative_downloads_2016_02\"].as_matrix()\n",
    "    train_output_2 = output_train[output_train.id >= split_id][\"cumulative_downloads_2016_02\"].as_matrix()\n",
    "\n",
    "\n",
    "\n",
    "    test_features_1 = predictors_test[predictors_test.id < split_id][numerical_predictors_1].as_matrix()\n",
    "    test_features_2 = predictors_test[predictors_test.id >= split_id][numerical_predictors_2].as_matrix()\n",
    "    \n",
    "     ###Classification\n",
    "    #mod_class_1= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100)   \n",
    "    #mod_class_1.fit(train_features_1, train_output_1)\n",
    "    #y_pred_1 =  mod_class_1.predict(test_features_1)\n",
    "    #mask_1 = y_pred_1 > (sorted(y_pred_1,reverse = True))[int(top_percent_classif*1.0/100*len(y_pred_1))]\n",
    "\n",
    "    #mod_class_2= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100)   \n",
    "    #mod_class_2.fit(train_features_2, train_output_2)\n",
    "    #y_pred_2 =  mod_class_2.predict(test_features_2)\n",
    "    #mask_2 = y_pred_2 > (sorted(y_pred_2,reverse = True))[int(top_percent_classif*1.0/100*len(y_pred_2))]\n",
    "\n",
    "    mod_class= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100)   \n",
    "    mod_class.fit(np.concatenate((train_features_1,train_features_2)), np.concatenate((train_output_1,train_output_2)))\n",
    "    y_pred_1 =  mod_class.predict(test_features_1)\n",
    "    mask_1 = y_pred_1 > (sorted(y_pred_1,reverse = True))[int(top_percent_classif*1.0/100*len(y_pred_1))]\n",
    "\n",
    "    y_pred_2 =  mod_class.predict(test_features_2)\n",
    "    mask_2 = y_pred_2 > (sorted(y_pred_2,reverse = True))[int(top_percent_classif*1.0/100*len(y_pred_2))]\n",
    "    print '.',\n",
    "\n",
    "    ###Weights\n",
    "    threshold1_20 = sorted(train_output_1,reverse = True)[int(len(train_output_1)*20.0/100)]\n",
    "    threshold1_12 = sorted(train_output_1,reverse = True)[int(len(train_output_1)*12.0/100)]\n",
    "    threshold1_5 = sorted(train_output_1,reverse = True)[int(len(train_output_1)*5.0/100)]\n",
    "    threshold1_1 = sorted(train_output_1,reverse = True)[int(len(train_output_1)*1.0/100)]\n",
    "    threshold1_0 = 0.0\n",
    "    if with_rel == True :\n",
    "        relevance_1 =(range(len(train_features_1)))\n",
    "        relevance_1.sort(key=lambda x: (train_output_1[x]))\n",
    "        relevance_1 = (np.asarray(relevance_1)-int(99.0*len(train_features_1)/100))\n",
    "        relevance_1[relevance_1<0] = 0\n",
    "\n",
    "        mod_train_1 = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=2, random_state=0, loss='ls')\\\n",
    "        .fit(train_features_1,np.log(train_output_1))\n",
    "        train_pred_1 =  mod_train_1.predict(train_features_1)\n",
    "\n",
    "        pred_train_mask_1 = train_pred_1 > sorted(train_pred_1,reverse = True)[int(1.0/100*len(train_features_1))]\n",
    "        true_train_mask_1 = train_output_1 > sorted(train_output_1,reverse = True)[int(1.0/100*len(train_features_1))]\n",
    "        hard_class_1 = np.logical_xor(pred_train_mask_1,true_train_mask_1)\n",
    "    else: \n",
    "        relevance_1 =(np.zeros(len(train_features_1)))\n",
    "        hard_class_1 =(np.zeros(len(train_features_1)))\n",
    "\n",
    "    print '.',\n",
    "    weights_1\\\n",
    "    = 0.05*(train_output_1 >  threshold1_0).astype(int)\\\n",
    "    + 0.05*(train_output_1 >  threshold1_20).astype(int)\\\n",
    "    + 0.5*(train_output_1 >  threshold1_12).astype(int)\\\n",
    "    + 0*(train_output_1 >  threshold1_5).astype(int)\\\n",
    "    + 0*(train_output_1 >  threshold1_1).astype(int)\\\n",
    "    + 0*relevance_1\\\n",
    "    + 0*hard_class_1\\\n",
    "    + 0*(np.asarray([0]*(len(train_features_1) -len(train_features_2)) + [1]*len(train_features_2) ))\\\n",
    "       * (train_output_1 >  threshold1_12).astype(int)\n",
    "    + 1.5*(np.asarray([0]*(len(train_features_1) -len(train_features_2)) + [1]*len(train_features_2) ))\n",
    "\n",
    "\n",
    "    mod_1 = GradientBoostingRegressor(max_features =1.0,n_estimators=100, learning_rate=0.1,max_depth=2, random_state=0, loss='ls')\n",
    "    #mod_1 = linear_model.Lasso(alpha=100)\n",
    "    mod_1.fit(train_features_1, train_output_1 \\\n",
    "              ,sample_weight \\\n",
    "    =(weights_1)*1.0/sum(weights_1))\n",
    "    print '.',\n",
    "\n",
    "\n",
    "    threshold2_20 = sorted(train_output_2,reverse = True)[int(len(train_output_2)*20.0/100)]\n",
    "    threshold2_12 = sorted(train_output_2,reverse = True)[int(len(train_output_2)*12.0/100)]\n",
    "    threshold2_5 = sorted(train_output_2,reverse = True)[int(len(train_output_2)*5.0/100)]\n",
    "    threshold2_1 = sorted(train_output_2,reverse = True)[int(len(train_output_2)*1.0/100)]\n",
    "    threshold2_0 = 0.0\n",
    "\n",
    "    if with_rel == True :\n",
    "        relevance_2 =(range(len(train_features_2)))\n",
    "        relevance_2.sort(key=lambda x: (train_output_2[x]))\n",
    "        relevance_2 = (np.asarray(relevance_2)-int(99.0*len(train_features_2)/100))\n",
    "        relevance_2[relevance_2<0] = 0\n",
    "\n",
    "        mod_train_2 = GradientBoostingRegressor(n_estimators=50, learning_rate=0.1,max_depth=2, random_state=0, loss='ls')\\\n",
    "        .fit(train_features_2,np.log(train_output_2))\n",
    "        train_pred_2 =  mod_train_2.predict(train_features_2)\n",
    "\n",
    "        pred_train_mask_2 = train_pred_2 > sorted(train_pred_2,reverse = True)[int(1.0/100*len(train_features_2))]\n",
    "        true_train_mask_2 = train_output_2 > sorted(train_output_2,reverse = True)[int(1.0/100*len(train_features_2))]\n",
    "        hard_class_2 = np.logical_xor(pred_train_mask_2,true_train_mask_2)\n",
    "    else: \n",
    "        relevance_2 =(np.zeros(len(train_features_2)))\n",
    "        hard_class_2 =(np.zeros(len(train_features_2)))\n",
    "    print '.',\n",
    "\n",
    "    weights_2 \\\n",
    "    = 1*(train_output_2 >  threshold2_0).astype(int)\\\n",
    "    + 0*(train_output_2 >  threshold2_20).astype(int)\\\n",
    "    + 0.5*(train_output_2 >  threshold2_12).astype(int)\\\n",
    "    + 0*(train_output_2 >  threshold2_5).astype(int)\\\n",
    "    + 0*(train_output_2 >  threshold2_1).astype(int)\\\n",
    "    + 0*relevance_2*relevance_2\\\n",
    "    + 0*hard_class_2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    mod_2 = GradientBoostingRegressor(n_estimators=500, learning_rate=0.01,max_depth=2, random_state=0, loss='ls')\n",
    "    #mod_2 = linear_model.Lasso(alpha=100)\n",
    "    #mod_2.fit(train_features_2, train_output_2)\n",
    "    mod_2.fit(train_features_2, train_output_2 ,sample_weight =(weights_2)*1.0/sum(weights_2))\n",
    "    print '.',\n",
    "\n",
    "    y_pred_1 = mod_1.predict(test_features_1)\n",
    "    print '.',\n",
    "\n",
    "    y_pred_2 = mod_2.predict(test_features_2)\n",
    "\n",
    "    ###CHECK ON THE IDs\n",
    "    #print min(train_features_2[:,0])\n",
    "    #print min(test_features_2[:,0])\n",
    "\n",
    "    ###Taking only the top 1 and not the values (not working)\n",
    "    #y_pred_1 = (y_pred_1 > sorted(y_pred_1,reverse = True)[int(len(y_pred_1)*0.6/100)])\n",
    "    #y_pred_2 = (y_pred_2 > sorted(y_pred_2,reverse = True)[int(len(y_pred_2)*2.4/100)])\n",
    "\n",
    "    #print \"lenghts of train and test : \" + str(len(train_features_1) ) + \" ; \"+ str(len(test_output_1))\n",
    "    #print \"Fraction in test_2 : \"+str(1.0*sum(test_output_2 > test_threshold)/sum(np.concatenate((test_output_1,test_output_2)) > test_threshold))\n",
    "\n",
    "    print '.',\n",
    "    mod = GradientBoostingRegressor(max_features =1.0,n_estimators=100, learning_rate=0.15,max_depth=3, random_state=0, loss='ls')\n",
    "    mod.fit(train_features_1, train_output_1)\n",
    "    pred_boost = mod.predict(np.concatenate((test_features_1,test_features_2)))\n",
    "    print '.',\n",
    "\n",
    "\n",
    "\n",
    "    #print \"perf_1 = \"+str(metric(test_output_1,y_pred_1*mask_1,0.6))\n",
    "    #print \"perf_2 = \"+str(metric(test_output_2,y_pred_2*mask_2,2.4))\n",
    "    y_pred = np.concatenate((y_pred_1*mask_1,y_pred_2*mask_2))+pred_boost\n",
    "\n",
    "\n",
    "    return y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The predictor pandas needs to be sorted by ascending ID\n",
    "\n",
    "### Obtained with boosting feature selection\n",
    "#numerical_predictors_2 = ['dl_projection','week_8','coef_2','week_5','downloads_per_day_before','cumulative_downloads_2015_02','week_7','downloads_std','coef_0','versions','download_sum','week_4','relative_download_increase','total_star3','days_since_release','market_size','week_3','review_rating_5','star3','review_rating_4','week_1','coef_1','m4_mean','review_rating_2','week_2','Uruguay','avg_review','nb_reviews']\n",
    "#numerical_predictors_1 = ['dl_projection','week_8','coef_2','week_5','downloads_per_day_before','cumulative_downloads_2015_02','coef_3','versions','downloads_std','download_sum','days_since_release','m2_mean','coef_1','m1_mean','week_7','m2_coef_1','review_rating_5','week_6','review_recent_over_old','week_2','star4','coef_0','total_star3','m1_max','m3_max','star3','review_per_downloads','week_4','market_size','review_rating_4','week_3','nb_reviews','nb_missing','daily_avg','ratings_per_day']\n",
    "\n",
    "###Custom\n",
    "#numerical_predictors_2 = ['download_sum', 'nb_reviews', 'iphone', 'Education']\n",
    "#numerical_predictors_1 = ['dl_projection', 'm3_max', 'week_8', 'coef_2', 'review_star5_per_downloads']\n",
    "\n",
    "### Use all predictors\n",
    "numerical_predictors_2 = list(numerical_predictors)\n",
    "numerical_predictors_1 = list(numerical_predictors)\n",
    "\n",
    "#numerical_predictors_1 = [\"coef_0\",\"coef_1\",\"coef_2\"]\n",
    "#numerical_predictors_2 = [\"coef_0\",\"coef_1\",\"coef_2\"]\n",
    "\n",
    "#numerical_predictors_1 = lst\n",
    "#numerical_predictors_2 = lst\n",
    "\n",
    "#Analysis on train performance if train > test this means there is overfit\n",
    "\n",
    "\n",
    "K = 5\n",
    "for mu in range(200000):\n",
    "    #np.random.seed(mu)\n",
    "\n",
    "    kf = KFold(len(predictors_train), n_folds=K,shuffle=True)\n",
    "    new_top = []\n",
    "    err_1 = []\n",
    "    err_2 = []\n",
    "    \n",
    "    for train, test in kf:\n",
    "        \n",
    "\n",
    "        ### The model_1 is trainned on everything\n",
    "\n",
    "\n",
    "        #test_output_1 = output_train.iloc[test][output_train.iloc[test].id < split_id][\"cumulative_downloads_2016_02\"].as_matrix()\n",
    "        #test_output_2 = output_train.iloc[test][output_train.iloc[test].id >= split_id][\"cumulative_downloads_2016_02\"].as_matrix()\n",
    "        test_output = output_train.iloc[test][\"cumulative_downloads_2016_02\"].as_matrix()\n",
    "        \n",
    "        #test_threshold = sorted(np.concatenate((test_output_1,test_output_2)),reverse = True)[int(len(test)*1.0/100)]\n",
    "        \n",
    "\n",
    "        ########\n",
    "        y_pred = final_model(predictors_train.iloc[train],output_train.iloc[train],predictors_train.iloc[test])\n",
    "        ########\n",
    "\n",
    "        #print \"perf_1 = \"+str(metric(test_output_1,y_pred_1,0.6))\n",
    "        #print \"perf_2 = \"+str(metric(test_output_2,y_pred_2,2.4))\n",
    "        #y_pred = np.concatenate((y_pred_1,y_pred_2))\n",
    "\n",
    "        #err_1.append(metric(test_output_1,y_pred_1,0.6))\n",
    "        #err_2.append(metric(test_output_2,y_pred_2,2.4))\n",
    "        err = (metric(test_output,y_pred))\n",
    "        new_top.append(err)\n",
    "        print \"perf concat : \"+str(err)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    print \"  =========== \"+str(mu)\n",
    "    #print \"Total CV_1 : \"+str(1.0*sum(err_1)/len(err_1))\n",
    "    #print \"Total CV_2 : \"+str(1.0*sum(err_2)/len(err_2))\n",
    "    print \"Total CV      ---------+>   : \"+str(1.0*sum(new_top)/len(new_top))\n",
    "    print \" --- \"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The predictor pandas needs to be sorted by ascending ID\n",
    "\n",
    "### Obtained with boosting feature selection\n",
    "#numerical_predictors_2 = ['dl_projection','week_8','coef_2','week_5','downloads_per_day_before','cumulative_downloads_2015_02','week_7','downloads_std','coef_0','versions','download_sum','week_4','relative_download_increase','total_star3','days_since_release','market_size','week_3','review_rating_5','star3','review_rating_4','week_1','coef_1','m4_mean','review_rating_2','week_2','Uruguay','avg_review','nb_reviews']\n",
    "#numerical_predictors_1 = ['dl_projection','week_8','coef_2','week_5','downloads_per_day_before','cumulative_downloads_2015_02','coef_3','versions','downloads_std','download_sum','days_since_release','m2_mean','coef_1','m1_mean','week_7','m2_coef_1','review_rating_5','week_6','review_recent_over_old','week_2','star4','coef_0','total_star3','m1_max','m3_max','star3','review_per_downloads','week_4','market_size','review_rating_4','week_3','nb_reviews','nb_missing','daily_avg','ratings_per_day']\n",
    "\n",
    "###Custom\n",
    "#numerical_predictors_2 = ['download_sum', 'nb_reviews', 'iphone', 'Education']\n",
    "#numerical_predictors_1 = ['dl_projection', 'm3_max', 'week_8', 'coef_2', 'review_star5_per_downloads']\n",
    "\n",
    "### Use all predictors\n",
    "numerical_predictors_2 = list(numerical_predictors)\n",
    "numerical_predictors_1 = list(numerical_predictors)\n",
    "\n",
    "#numerical_predictors_1 = [\"coef_0\",\"coef_1\",\"coef_2\"]\n",
    "#numerical_predictors_2 = [\"coef_0\",\"coef_1\",\"coef_2\"]\n",
    "\n",
    "#Analysis on train performance if train > test this means there is overfit\n",
    "\n",
    "np.random.seed(1600)\n",
    "\n",
    "\n",
    "########\n",
    "y_pred = final_model(predictors_train,output_train,predictors_test)\n",
    "########\n",
    "\n",
    "print \"   \"\n",
    "print \"   \"\n",
    "print \"   \"\n",
    "\n",
    "\n",
    "thres = sorted(y_pred,reverse=True)[int(len(y_pred)*1.0/100)]\n",
    "lst= []\n",
    "name_lst = []\n",
    "for x in enumerate(y_pred):\n",
    "    if x[1]>thres:\n",
    "        lst.append( predictors_test_id[x[0]])\n",
    "        name_lst.append((x[1],predictors_test[predictors_test.id == predictors_test_id[x[0]][0]]\\\n",
    "                     [predictors_test[predictors_test.id == predictors_test_id[x[0]][0]]\\\n",
    "                      .device == predictors_test_id[x[0]][1]]['name'].iloc[0]))\n",
    "\n",
    "for x in lst:\n",
    "    print str(x[0]) + \", \"+x[1]\n",
    "\n",
    "print \"   \"\n",
    "print \"   \"\n",
    "print \"   \"\n",
    "\n",
    "for x in sorted(name_lst,reverse=True):\n",
    "    print str(x[0]) + \" \\t ---> \"+x[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "thres = sorted(y_pred,reverse=True)[int(len(y_pred)*1.0/100)]\n",
    "lst= []\n",
    "name_lst = []\n",
    "for x in enumerate(y_pred):\n",
    "    if x[1]>thres:\n",
    "        lst.append( predictors_test_id[x[0]])\n",
    "        name_lst.append((x[1],predictors_test[predictors_test.id == predictors_test_id[x[0]][0]]\\\n",
    "                     [predictors_test[predictors_test.id == predictors_test_id[x[0]][0]]\\\n",
    "                      .device == predictors_test_id[x[0]][1]]['name'].iloc[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in lst:\n",
    "    print str(x[0]) + \", \"+x[1]\n",
    "\n",
    "print \"   \"\n",
    "print \"   \"\n",
    "print \"   \"\n",
    "\n",
    "for x in sorted(name_lst,reverse=True):\n",
    "    print str(x[0]) + \" \\t ---> \"+x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def final_model(train_features_1,train_features_2,test_features_1,test_features_2,train_output_1,train_output_2):\n",
    "    ###Classification\n",
    "    #mod_class_1= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100)   \n",
    "    #mod_class_1.fit(train_features_1, train_output_1)\n",
    "    #y_pred_1 =  mod_class_1.predict(test_features_1)\n",
    "    #mask_1 = y_pred_1 > (sorted(y_pred_1,reverse = True))[int(top_percent_classif*1.0/100*len(y_pred_1))]\n",
    "\n",
    "    #mod_class_2= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100)   \n",
    "    #mod_class_2.fit(train_features_2, train_output_2)\n",
    "    #y_pred_2 =  mod_class_2.predict(test_features_2)\n",
    "    #mask_2 = y_pred_2 > (sorted(y_pred_2,reverse = True))[int(top_percent_classif*1.0/100*len(y_pred_2))]\n",
    "\n",
    "    mod_class= RandomForestRegressor(max_features = 'sqrt',n_estimators = 100)   \n",
    "    mod_class.fit(np.concatenate((train_features_1,train_features_2)), np.concatenate((train_output_1,train_output_2)))\n",
    "    y_pred_1 =  mod_class.predict(test_features_1)\n",
    "    mask_1 = y_pred_1 > (sorted(y_pred_1,reverse = True))[int(top_percent_classif*1.0/100*len(y_pred_1))]\n",
    "\n",
    "    y_pred_2 =  mod_class.predict(test_features_2)\n",
    "    mask_2 = y_pred_2 > (sorted(y_pred_2,reverse = True))[int(top_percent_classif*1.0/100*len(y_pred_2))]\n",
    "\n",
    "\n",
    "    ###Weights\n",
    "    threshold1_20 = sorted(train_output_1,reverse = True)[int(len(train_output_1)*20.0/100)]\n",
    "    threshold1_12 = sorted(train_output_1,reverse = True)[int(len(train_output_1)*12.0/100)]\n",
    "    threshold1_5 = sorted(train_output_1,reverse = True)[int(len(train_output_1)*5.0/100)]\n",
    "    threshold1_1 = sorted(train_output_1,reverse = True)[int(len(train_output_1)*1.0/100)]\n",
    "    threshold1_0 = 0.0\n",
    "    if with_rel == True :\n",
    "        relevance_1 =(range(len(train_features_1)))\n",
    "        relevance_1.sort(key=lambda x: (train_output_1[x]))\n",
    "        relevance_1 = (np.asarray(relevance_1)-int(99.0*len(train_features_1)/100))\n",
    "        relevance_1[relevance_1<0] = 0\n",
    "\n",
    "        mod_train_1 = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01,max_depth=2, random_state=0, loss='ls')\\\n",
    "        .fit(train_features_1,np.log(train_output_1))\n",
    "        train_pred_1 =  mod_train_1.predict(train_features_1)\n",
    "\n",
    "        pred_train_mask_1 = train_pred_1 > sorted(train_pred_1,reverse = True)[int(1.0/100*len(train_features_1))]\n",
    "        true_train_mask_1 = train_output_1 > sorted(train_output_1,reverse = True)[int(1.0/100*len(train_features_1))]\n",
    "        hard_class_1 = np.logical_xor(pred_train_mask_1,true_train_mask_1)\n",
    "    else: \n",
    "        relevance_1 =(np.zeros(len(train_features_1)))\n",
    "        hard_class_1 =(np.zeros(len(train_features_1)))\n",
    "\n",
    "\n",
    "    weights_1\\\n",
    "    = 0*(train_output_1 >  threshold1_0).astype(int)\\\n",
    "    + 0*(train_output_1 >  threshold1_20).astype(int)\\\n",
    "    + 0.5*(train_output_1 >  threshold1_12).astype(int)\\\n",
    "    + 0*(train_output_1 >  threshold1_5).astype(int)\\\n",
    "    + 0*(train_output_1 >  threshold1_1).astype(int)\\\n",
    "    + 0*relevance_1\\\n",
    "    + 0*hard_class_1\\\n",
    "    + 1.5*np.asarray([0]*(len(train_features_1) -len(train_features_2)) + [1]*len(train_features_2) )#* (train_output_1 >  threshold1_12).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    mod_1 = GradientBoostingRegressor(max_features =1.0,n_estimators=100, learning_rate=0.1,max_depth=2, random_state=0, loss='ls')\n",
    "    #mod_1 = linear_model.Lasso(alpha=100)\n",
    "    mod_1.fit(train_features_1, train_output_1 \\\n",
    "              ,sample_weight \\\n",
    "    =(weights_1)*1.0/sum(weights_1))\n",
    "\n",
    "    y_pred_1 = mod_1.predict(test_features_1)\n",
    "\n",
    "    threshold2_20 = sorted(train_output_2,reverse = True)[int(len(train_output_2)*20.0/100)]\n",
    "    threshold2_12 = sorted(train_output_2,reverse = True)[int(len(train_output_2)*12.0/100)]\n",
    "    threshold2_5 = sorted(train_output_2,reverse = True)[int(len(train_output_2)*5.0/100)]\n",
    "    threshold2_1 = sorted(train_output_2,reverse = True)[int(len(train_output_2)*1.0/100)]\n",
    "    threshold2_0 = 0.0\n",
    "\n",
    "    if with_rel == True :\n",
    "        relevance_2 =(range(len(train_features_2)))\n",
    "        relevance_2.sort(key=lambda x: (train_output_2[x]))\n",
    "        relevance_2 = (np.asarray(relevance_2)-int(99.0*len(train_features_2)/100))\n",
    "        relevance_2[relevance_2<0] = 0\n",
    "\n",
    "        mod_train_2 = GradientBoostingRegressor(n_estimators=500, learning_rate=0.01,max_depth=2, random_state=0, loss='ls')\\\n",
    "        .fit(train_features_2,np.log(train_output_2))\n",
    "        train_pred_2 =  mod_train_2.predict(train_features_2)\n",
    "\n",
    "        pred_train_mask_2 = train_pred_2 > sorted(train_pred_2,reverse = True)[int(1.0/100*len(train_features_2))]\n",
    "        true_train_mask_2 = train_output_2 > sorted(train_output_2,reverse = True)[int(1.0/100*len(train_features_2))]\n",
    "        hard_class_2 = np.logical_xor(pred_train_mask_2,true_train_mask_2)\n",
    "    else: \n",
    "        relevance_2 =(np.zeros(len(train_features_2)))\n",
    "        hard_class_2 =(np.zeros(len(train_features_2)))\n",
    "\n",
    "\n",
    "    weights_2 \\\n",
    "    = 1*(train_output_2 >  threshold2_0).astype(int)\\\n",
    "    + 0*(train_output_2 >  threshold2_20).astype(int)\\\n",
    "    + 0*(train_output_2 >  threshold2_12).astype(int)\\\n",
    "    + 0*(train_output_2 >  threshold2_5).astype(int)\\\n",
    "    + 0*(train_output_2 >  threshold2_1).astype(int)\\\n",
    "    + 0*relevance_2*relevance_2\\\n",
    "    + 0*hard_class_2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    mod_2 = GradientBoostingRegressor(n_estimators=50, learning_rate=0.1,max_depth=3, random_state=0, loss='ls')\n",
    "    #mod_2 = linear_model.Lasso(alpha=100)\n",
    "    #mod_2.fit(train_features_2, train_output_2)\n",
    "    mod_2.fit(train_features_2, train_output_2 ,sample_weight =(weights_2)*1.0/sum(weights_2))\n",
    "\n",
    "    y_pred_2 = mod_2.predict(test_features_2)\n",
    "\n",
    "    ###CHECK ON THE IDs\n",
    "    #print min(train_features_2[:,0])\n",
    "    #print min(test_features_2[:,0])\n",
    "\n",
    "    ###Taking only the top 1 and not the values (not working)\n",
    "    #y_pred_1 = (y_pred_1 > sorted(y_pred_1,reverse = True)[int(len(y_pred_1)*0.6/100)])\n",
    "    #y_pred_2 = (y_pred_2 > sorted(y_pred_2,reverse = True)[int(len(y_pred_2)*2.4/100)])\n",
    "\n",
    "    #print \"lenghts of train and test : \" + str(len(train_features_1) ) + \" ; \"+ str(len(test_output_1))\n",
    "    #print \"Fraction in test_2 : \"+str(1.0*sum(test_output_2 > test_threshold)/sum(np.concatenate((test_output_1,test_output_2)) > test_threshold))\n",
    "\n",
    "\n",
    "    return y_pred_1*mask_1, y_pred_2*mask_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The predictor pandas needs to be sorted by ascending ID\n",
    "\n",
    "### Obtained with boosting feature selection\n",
    "#numerical_predictors_2 = ['dl_projection','week_8','coef_2','week_5','downloads_per_day_before','cumulative_downloads_2015_02','week_7','downloads_std','coef_0','versions','download_sum','week_4','relative_download_increase','total_star3','days_since_release','market_size','week_3','review_rating_5','star3','review_rating_4','week_1','coef_1','m4_mean','review_rating_2','week_2','Uruguay','avg_review','nb_reviews']\n",
    "#numerical_predictors_1 = ['dl_projection','week_8','coef_2','week_5','downloads_per_day_before','cumulative_downloads_2015_02','coef_3','versions','downloads_std','download_sum','days_since_release','m2_mean','coef_1','m1_mean','week_7','m2_coef_1','review_rating_5','week_6','review_recent_over_old','week_2','star4','coef_0','total_star3','m1_max','m3_max','star3','review_per_downloads','week_4','market_size','review_rating_4','week_3','nb_reviews','nb_missing','daily_avg','ratings_per_day']\n",
    "\n",
    "###Custom\n",
    "#numerical_predictors_2 = ['download_sum', 'nb_reviews', 'iphone', 'Education']\n",
    "#numerical_predictors_1 = ['dl_projection', 'm3_max', 'week_8', 'coef_2', 'review_star5_per_downloads']\n",
    "\n",
    "### Use all predictors\n",
    "numerical_predictors_2 = list(numerical_predictors)\n",
    "numerical_predictors_1 = list(numerical_predictors)\n",
    "\n",
    "#numerical_predictors_1 = [\"coef_0\",\"coef_1\",\"coef_2\"]\n",
    "#numerical_predictors_2 = [\"coef_0\",\"coef_1\",\"coef_2\"]\n",
    "\n",
    "#Analysis on train performance if train > test this means there is overfit\n",
    "\n",
    "with_rel = False\n",
    "K = 5\n",
    "for mu in ['nope']:\n",
    "    np.random.seed(1)\n",
    "    top_percent_classif = 12\n",
    "    split_id = 771335033\n",
    "    kf = KFold(len(predictors_train), n_folds=K,shuffle=True)\n",
    "    new_top = []\n",
    "    err_1 = []\n",
    "    err_2 = []\n",
    "    for train, test in kf:\n",
    "        \n",
    "\n",
    "        ### The model_1 is trainned on everything\n",
    "\n",
    "        train_features_1 = predictors_train.iloc[train][numerical_predictors_1].as_matrix()\n",
    "        train_features_2 = predictors_train.iloc[train][predictors_train.iloc[train].id >= split_id][numerical_predictors_2].as_matrix()\n",
    "        train_output_1 = output_train.iloc[train][\"cumulative_downloads_2016_02\"].as_matrix()\n",
    "        train_output_2 = output_train.iloc[train][output_train.iloc[train].id >= split_id][\"cumulative_downloads_2016_02\"].as_matrix()\n",
    "        \n",
    "\n",
    "\n",
    "        test_features_1 = predictors_train.iloc[test][predictors_train.iloc[test].id < split_id][numerical_predictors_1].as_matrix()\n",
    "        test_features_2 = predictors_train.iloc[test][predictors_train.iloc[test].id >= split_id][numerical_predictors_2].as_matrix()\n",
    "        test_output_1 = output_train.iloc[test][output_train.iloc[test].id < split_id][\"cumulative_downloads_2016_02\"].as_matrix()\n",
    "        test_output_2 = output_train.iloc[test][output_train.iloc[test].id >= split_id][\"cumulative_downloads_2016_02\"].as_matrix()\n",
    "        \n",
    "        test_threshold = sorted(np.concatenate((test_output_1,test_output_2)),reverse = True)[int(len(test)*1.0/100)]\n",
    "        \n",
    "\n",
    "        ########\n",
    "        y_pred_1 , y_pred_2 = final_model(train_features_1,train_features_2,test_features_1,test_features_2,train_output_1,train_output_2)\n",
    "        ########\n",
    "\n",
    "        print \"perf_1 = \"+str(metric(test_output_1,y_pred_1,0.6))\n",
    "        print \"perf_2 = \"+str(metric(test_output_2,y_pred_2,2.4))\n",
    "        y_pred = np.concatenate((y_pred_1,y_pred_2))\n",
    "\n",
    "        err_1.append(metric(test_output_1,y_pred_1,0.6))\n",
    "        err_2.append(metric(test_output_2,y_pred_2,2.4))\n",
    "        err = (metric(np.concatenate((test_output_1,test_output_2)),y_pred))\n",
    "        new_top.append(err)\n",
    "        print \"perf concat : \"+str(err)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    print \"  =========== \"+str(mu)\n",
    "    print \"Total CV_1 : \"+str(1.0*sum(err_1)/len(err_1))\n",
    "    print \"Total CV_2 : \"+str(1.0*sum(err_2)/len(err_2))\n",
    "    print \"Total CV : \"+str(1.0*sum(new_top)/len(new_top))\n",
    "    print \" --- \"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The predictor pandas needs to be sorted by ascending ID\n",
    "\n",
    "### Obtained with boosting feature selection\n",
    "#numerical_predictors_2 = ['dl_projection','week_8','coef_2','week_5','downloads_per_day_before','cumulative_downloads_2015_02','week_7','downloads_std','coef_0','versions','download_sum','week_4','relative_download_increase','total_star3','days_since_release','market_size','week_3','review_rating_5','star3','review_rating_4','week_1','coef_1','m4_mean','review_rating_2','week_2','Uruguay','avg_review','nb_reviews']\n",
    "#numerical_predictors_1 = ['dl_projection','week_8','coef_2','week_5','downloads_per_day_before','cumulative_downloads_2015_02','coef_3','versions','downloads_std','download_sum','days_since_release','m2_mean','coef_1','m1_mean','week_7','m2_coef_1','review_rating_5','week_6','review_recent_over_old','week_2','star4','coef_0','total_star3','m1_max','m3_max','star3','review_per_downloads','week_4','market_size','review_rating_4','week_3','nb_reviews','nb_missing','daily_avg','ratings_per_day']\n",
    "\n",
    "###Custom\n",
    "#numerical_predictors_2 = ['download_sum', 'nb_reviews', 'iphone', 'Education']\n",
    "#numerical_predictors_1 = ['dl_projection', 'm3_max', 'week_8', 'coef_2', 'review_star5_per_downloads']\n",
    "\n",
    "### Use all predictors\n",
    "numerical_predictors_2 = list(numerical_predictors)\n",
    "numerical_predictors_1 = list(numerical_predictors)\n",
    "\n",
    "#numerical_predictors_1 = [\"coef_0\",\"coef_1\",\"coef_2\"]\n",
    "#numerical_predictors_2 = [\"coef_0\",\"coef_1\",\"coef_2\"]\n",
    "\n",
    "#Analysis on train performance if train > test this means there is overfit\n",
    "\n",
    "with_rel = False\n",
    "\n",
    "np.random.seed(0)\n",
    "top_percent_classif = 12\n",
    "split_id = 771335033\n",
    "\n",
    "train_features_1 = predictors_train[predictors_train.id < split_id][numerical_predictors_1].as_matrix()\n",
    "train_features_2 = predictors_train[predictors_train.id >= split_id][numerical_predictors_2].as_matrix()\n",
    "train_output_1 = output_train[output_train.id < split_id][\"cumulative_downloads_2016_02\"].as_matrix()\n",
    "train_output_2 = output_train[output_train.id >= split_id][\"cumulative_downloads_2016_02\"].as_matrix()\n",
    "\n",
    "\n",
    "\n",
    "test_features_1 = predictors_test[predictors_test.id < split_id][numerical_predictors_1].as_matrix()\n",
    "test_features_2 = predictors_test[predictors_test.id >= split_id][numerical_predictors_2].as_matrix()\n",
    "\n",
    "########\n",
    "y_pred_1 , y_pred_2 = final_model(train_features_1,train_features_2,test_features_1,test_features_2,train_output_1,train_output_2)\n",
    "########\n",
    "\n",
    "y_pred = np.concatenate((y_pred_1,y_pred_2))\n",
    "\n",
    "thres = sorted(y_pred,reverse=True)[int(len(y_pred)*1.0/100)]\n",
    "lst= []\n",
    "for x in enumerate(y_pred):\n",
    "    if x[1]>thres:\n",
    "        lst.append( predictors_test_id[x[0]])\n",
    "\n",
    "for x in lst:\n",
    "    print str(x[0]) + \", \"+x[1]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in lst:\n",
    "    print predictors_test[predictors_test.id == x[0] ]['name'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "down = 'test_set/test_app_downloads.csv'\n",
    "rati = 'test_set/test_app_rating.csv'\n",
    "usag='test_set/test_usage.csv'\n",
    "reve = 'test_set/test_revenue.csv'\n",
    "prev ='test_set/test_cumulative_downloads_2015-02.csv'\n",
    "rele = 'test_set/test_release_date.csv'\n",
    "raco = 'test_set/test_rating_by_country.csv'\n",
    "revi ='test_set/test_app_review.csv'\n",
    "\n",
    "\n",
    "'''down = 'train_app_downloads.csv'\n",
    "rati = 'train_app_rating.csv'\n",
    "usag='train_usage.csv'\n",
    "reve = 'train_revenue.csv'\n",
    "prev ='train_cumulative_downloads_2015-02.csv'\n",
    "rele = 'train_release_date.csv'\n",
    "raco = 'train_rating_by_country.csv'\n",
    "revi ='train_app_review.csv'\n",
    "'''\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", os.environ['AWS_ACCESS_KEY_ID'])\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", os.environ['AWS_SECRET_ACCESS_KEY'])\n",
    "\n",
    "usages = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true',inferSchema='true') \\\n",
    "    .load('s3n://cs341bucket1/Data/'+usag).drop('')\n",
    "revenues = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true',inferSchema='true') \\\n",
    "    .load('s3n://cs341bucket1/Data/'+reve).drop('')\n",
    "\n",
    "###imputations\n",
    "#usage imputation\n",
    "imputer = Imputer(missing_values=-1, strategy='median', axis=0)\n",
    "pd_usages = usages.toPandas()\n",
    "category = list(set(pd_usages[\"category\"].values))\n",
    "imp = pd.DataFrame(columns = pd_usages.columns)\n",
    "for cat in category:\n",
    "    #for dev in [\"iphone\",\"ipad\"]:\n",
    "        for metric in range(1,5):\n",
    "            curr_df = pd_usages.ix[pd_usages[\"category\"]==cat,:]\n",
    "            #curr_df = curr_df.ix[curr_df[\"device\"]==dev,:]\n",
    "            curr_df = curr_df.ix[curr_df[\"metric\"]==metric,:]\n",
    "            \n",
    "            try:\n",
    "                name = curr_df.columns\n",
    "                df1 = curr_df.ix[:,0:6]\n",
    "                df2 = pd.DataFrame(imputer.fit_transform(curr_df.ix[:,6:]))\n",
    "                df2.index = df1.index\n",
    "                curr_df = pd.concat([df1,df2],axis = 1)\n",
    "                curr_df.columns = name \n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            \n",
    "            imp = pd.concat([imp,curr_df],axis = 0)\n",
    "usages = sqlCtx.createDataFrame(imp)\n",
    "\n",
    "#revenue imputation\n",
    "pd_revenues = revenues.toPandas()\n",
    "imp = pd.DataFrame(columns = revenues.columns)\n",
    "for cat in category:\n",
    "    for dev in [\"iphone\",\"ipad\"]:\n",
    "        #for metric in range(1,5):\n",
    "            curr_df = pd_revenues.ix[pd_revenues[\"category\"]==cat,:]\n",
    "            curr_df = curr_df.ix[curr_df[\"device\"]==dev,:]\n",
    "            #curr_df = curr_df.ix[curr_df[\"metric\"]==metric,:]\n",
    "            name = curr_df.columns\n",
    "            df1 = curr_df.ix[:,0:5]\n",
    "            df2 = pd.DataFrame(imputer.fit_transform(curr_df.ix[:,5:]))\n",
    "            df2.index = df1.index\n",
    "            curr_df = pd.concat([df1,df2],axis = 1)\n",
    "            curr_df.columns = name \n",
    "            imp = pd.concat([imp,curr_df],axis = 0)\n",
    "revenues = sqlCtx.createDataFrame(imp)\n",
    "\n",
    "\n",
    "temp_revenues = sqlCtx.sql(\"SELECT id, name, category, device, \\\n",
    "    get_revenue_coefficients(0,\"+\",\".join(revenues.columns[4:])+\") AS rev_coef_0, \\\n",
    "    get_revenue_coefficients(1,\"+\",\".join(revenues.columns[4:])+\") AS rev_coef_1, \\\n",
    "    get_revenue_coefficients(2,\"+\",\".join(revenues.columns[4:])+\") AS rev_coef_2, \\\n",
    "    get_revenue_max(0,\"+\",\".join(revenues.columns[4:])+\") AS rev_max, \\\n",
    "    get_revenue_mean(0,\"+\",\".join(revenues.columns[4:])+\") AS rev_mean FROM revenues\")\n",
    "\n",
    "temp_revenues.toPandas().head(1)\n",
    "print \",\".join(revenues.columns[4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlCtx.registerDataFrameAsTable(usages, \"usages\")\n",
    "sqlCtx.sql('select * from usages where id =411796932').fillna(-1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "usag='test_set/test_usage.csv'\n",
    "#usag='train_usage.csv'\n",
    "\n",
    "usages = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true',inferSchema='true') \\\n",
    "    .load('s3n://cs341bucket1/Data/'+usag).drop('')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "###imputations\n",
    "#usage imputation\n",
    "imputer = Imputer(missing_values=-1, strategy='median', axis=0)\n",
    "pd_usages = usages.toPandas()\n",
    "category = list(set(pd_usages[\"category\"].values))\n",
    "imp = pd.DataFrame(columns = pd_usages.columns)\n",
    "for cat in category:\n",
    "    #for dev in [\"iphone\",\"ipad\"]:\n",
    "        for metric in range(1,5):\n",
    "            curr_df = pd_usages.ix[pd_usages[\"category\"]==cat,:]\n",
    "            #curr_df = curr_df.ix[curr_df[\"device\"]==dev,:]\n",
    "            curr_df = curr_df.ix[curr_df[\"metric\"]==metric,:]\n",
    "            try:\n",
    "                name = curr_df.columns\n",
    "                df1 = curr_df.ix[:,0:6]\n",
    "                df2 = pd.DataFrame(imputer.fit_transform(curr_df.ix[:,6:]))\n",
    "                df2.index = df1.index\n",
    "                curr_df = pd.concat([df1,df2],axis = 1)\n",
    "                curr_df.columns = name \n",
    "            except:\n",
    "                pass\n",
    "            imp = pd.concat([imp,curr_df],axis = 0)\n",
    "usages = sqlCtx.createDataFrame(imp).fillna(-1)\n",
    "\n",
    "m_col = '03_01_2015,03_08_2015,03_15_2015,03_22_2015,03_29_2015,04_05_2015,04_12_2015,04_19_2015'\n",
    "\n",
    "m1 = sqlCtx.sql(\"SELECT * FROM usages WHERE metric = 1 AND id != 411796932\")\n",
    "sqlCtx.registerDataFrameAsTable(m1,\"m1\")\n",
    "temp_m1 = sqlCtx.sql(\"SELECT id, name, category, device, \\\n",
    "get_usage_coefficients(0,\"+m_col+\") AS m1_coef_0, \\\n",
    "get_usage_coefficients(1,\"+m_col+\") AS m1_coef_1, \\\n",
    "get_usage_coefficients(2,\"+m_col+\") AS m1_coef_2, \\\n",
    "get_usage_max(0,\"+m_col+\") AS m1_max, \\\n",
    "get_usage_mean(0,\"+m_col+\") AS m1_mean FROM m1\")\n",
    "\n",
    "temp_m1.toPandas().head(1)\n",
    "print \"At last\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\",\".join(m1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "down = 'test_set/test_app_downloads.csv'\n",
    "rati = 'test_set/test_app_rating.csv'\n",
    "usag='test_set/test_usage.csv'\n",
    "reve = 'test_set/test_revenue.csv'\n",
    "prev ='test_set/test_cumulative_downloads_2015-02.csv'\n",
    "rele = 'test_set/test_release_date.csv'\n",
    "raco = 'test_set/test_rating_by_country.csv'\n",
    "revi ='test_set/test_app_review.csv'\n",
    "\n",
    "\n",
    "'''down = 'train_app_downloads.csv'\n",
    "rati = 'train_app_rating.csv'\n",
    "usag='train_usage.csv'\n",
    "reve = 'train_revenue.csv'\n",
    "prev ='train_cumulative_downloads_2015-02.csv'\n",
    "rele = 'train_release_date.csv'\n",
    "raco = 'train_rating_by_country.csv'\n",
    "revi ='train_app_review.csv'\n",
    "'''\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", os.environ['AWS_ACCESS_KEY_ID'])\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", os.environ['AWS_SECRET_ACCESS_KEY'])\n",
    "\n",
    "usages = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true',inferSchema='true') \\\n",
    "    .load('s3n://cs341bucket1/Data/'+usag).drop('')\n",
    "revenues = sqlCtx.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true',inferSchema='true') \\\n",
    "    .load('s3n://cs341bucket1/Data/'+reve).drop('')\n",
    "\n",
    "sqlCtx.registerDataFrameAsTable(revenues, \"revenues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mod_0 = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01,max_depth=2, random_state=0, loss='ls')\\\n",
    "        .fit(predictors_train[numerical_predictors].as_matrix(),output_train[\"cumulative_downloads_2016_02\"].as_matrix())\n",
    "ranking = mod_0.feature_importances_\n",
    "lst = []\n",
    "for e in [(i[0],i[1]) for i in sorted(zip(numerical_predictors,ranking),key=lambda x: -x[1])]:\n",
    "    print e\n",
    "    if e[1]>0:\n",
    "        lst.append(e[0])\n",
    "print lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('error')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
